{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_path = 'C:/ASM/Dropbox/Developments/Jupyter/Eating/myutils' if 'C:' in os.getcwd() else './myutils'\n",
    "sys.path.append(util_path)\n",
    "import my_file_utils as mfileu\n",
    "import my_classification_utils as mclfu\n",
    "import my_steven_lab_utils as mslabu\n",
    "#importlib.reload(biteu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win size: 80 , axis count:  6\n"
     ]
    }
   ],
   "source": [
    "win_size, axis_count = 5*16, 6\n",
    "print(\"Win size:\", win_size, \", axis count: \", axis_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(ds):\n",
    "    new_ds= []    \n",
    "    for subj in range(len(ds)):\n",
    "        subj_ds=[]\n",
    "        for sess in range(len(ds[subj])):\n",
    "            d = ds[subj][sess]\n",
    "            accel = d[:, 1:4]\n",
    "            accel = (accel+3*9.8)/(2*3*9.8)\n",
    "            accel[accel>1] = 1\n",
    "            accel[accel<1] = 0\n",
    "            \n",
    "            gyro = d[:, 4:7]\n",
    "            gyro = (gyro+10)/(2*10)\n",
    "            gyro[gyro>1] = 1\n",
    "            gyro[gyro<0] = 0\n",
    "            \n",
    "            t = d[:, 0].reshape((-1, 1))\n",
    "            d = np.concatenate((t, accel, gyro), axis=1)\n",
    "            subj_ds.append(d)\n",
    "            \n",
    "            assert np.sum(d[:, 1:]>1) == 0\n",
    "            assert np.sum(d[:, 1:]<0) == 0\n",
    "            assert d.shape[1] == 7\n",
    "            \n",
    "        new_ds.append(subj_ds)\n",
    "        \n",
    "    return new_ds            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_data(ds, indices):\n",
    "    count = len(indices)\n",
    "    w = np.zeros((count, win_size, 6))        \n",
    "    \n",
    "    for i in range(count):\n",
    "        subj, sess, ix = indices[i, 0], indices[i, 1], indices[i, 2]\n",
    "        w[i, :, :] = ds[subj][sess][ix:ix+win_size, 1:7]                \n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj, num_epochs, train_test = 0, 1, 'train'\n",
    "if 'C:' not in mfileu.get_path():    \n",
    "    subj, num_epochs, train_test = int(sys.argv[1]), int(sys.argv[2]), sys.argv[3]\n",
    "    \n",
    "assert train_test in ['train', 'test_lab', 'test_free']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_SIZE = 5\n",
    "NUM_FILTERS = 64\n",
    "BATCH_SIZE = 100\n",
    "NUM_SENSOR_CHANNELS = 6\n",
    "NUM_UNITS_LSTM = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(x, size_in, size_out, ksize, strides, padding, name):    \n",
    "    strides = [1, strides[0], strides[1], 1]   \n",
    "    with tf.name_scope(name):\n",
    "        W = tf.Variable(tf.truncated_normal([ksize[0], ksize[1], size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.0, shape=[size_out]), name=\"b\")\n",
    "        conv = tf.nn.conv2d(x, W, strides=strides, padding=padding)\n",
    "        output = tf.nn.relu(conv + b)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_layer(lstm_input, name):\n",
    "    cell_fw = tf.nn.rnn_cell.LSTMCell(NUM_UNITS_LSTM, state_is_tuple=True)        \n",
    "    initial_state = cell_fw.zero_state(BATCH_SIZE, tf.float32)\n",
    "\n",
    "    output, last_state = tf.nn.dynamic_rnn(cell_fw, lstm_input,\n",
    "                                           initial_state=initial_state,                                               \n",
    "                                           scope = name,\n",
    "                                           dtype=tf.float32)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(x, size_out, name=\"FC_Layer\" ):\n",
    "    dims = x.get_shape().as_list()\n",
    "    size_in = dims[-1]\n",
    "    with tf.name_scope(name):\n",
    "        W = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.0, shape=[size_out]), name=\"b\")\n",
    "        output = tf.matmul(x, W) + b             \n",
    "        \n",
    "    return output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(x):\n",
    "    x_shape =x.get_shape().as_list()\n",
    "    print('Inside get_net,  x_shape: ', x_shape)\n",
    "    \n",
    "    x = tf.reshape(x, shape=[-1, x.shape[1], x.shape[2], 1], name=\"reshape\")\n",
    "    conv_1 = conv_layer(x, size_in=1, size_out=64, ksize=[5,1], strides=[1,1], padding=\"VALID\", name='conv_1')                \n",
    "    conv_2 = conv_layer(conv_1, size_in=64, size_out=64, ksize=[5,1], strides=[1,1], padding=\"VALID\", name='conv_2')\n",
    "    conv_3 = conv_layer(conv_2, size_in=64, size_out=64, ksize=[5,1], strides=[1,1], padding=\"VALID\", name='conv_3')\n",
    "    conv_4 = conv_layer(conv_3, size_in=64, size_out=64, ksize=[5,1], strides=[1,1], padding=\"VALID\", name='conv_4')\n",
    "\n",
    "    print(\"Conv Layer Shapes: \")\n",
    "    print(\"Conv Layer 1: \", conv_1.get_shape().as_list())\n",
    "    print(\"Conv Layer 2: \", conv_2.get_shape().as_list())\n",
    "    print(\"Conv Layer 3: \", conv_3.get_shape().as_list())\n",
    "    print(\"Conv Layer 4: \", conv_4.get_shape().as_list())\n",
    "    \n",
    "    sz = conv_4.get_shape().as_list()\n",
    "    lstm_input = tf.reshape(conv_4, shape=[-1, sz[1], sz[2]*sz[3]], name=\"Flattened\")\n",
    "    \n",
    "    lstm_1 = lstm_layer(lstm_input, name=\"lstm_1\")    \n",
    "    lstm_2 = lstm_layer(lstm_1, name=\"lstm_2\")        \n",
    "    \n",
    "    lstm_out = lstm_2[:, -1, :]    \n",
    "    print(\"Lstm out shape: \", lstm_out.get_shape().as_list())\n",
    "    \n",
    "    logits = fc_layer(lstm_out, 1, name=\"Logits\")  \n",
    "    print(\"Logit shape: \",logits.get_shape().as_list())\n",
    "    \n",
    "    return logits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(ds, train_indices, test_indices, model_path_dest=None, model_path_src=None):       \n",
    "    #print_out, sys.stdout = sys.stdout, open(os.devnull, 'w')    \n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, win_size, axis_count], name=\"x\")        \n",
    "    y = tf.placeholder(tf.float32, [None, 1], name=\"y\")    \n",
    "    \n",
    "    logits = get_net(x)\n",
    "    prediction = tf.nn.sigmoid(logits, name=\"prediction\")\n",
    "    correct_prediction = tf.equal(tf.greater(prediction, 0.5), tf.equal(y,1), name=\"correct_prediction\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y), name=\"loss_op\")        \n",
    "    train_step = tf.train.AdamOptimizer().minimize(loss_op, name=\"train_step\")\n",
    "\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    #sys.stdout = print_out\n",
    "    ########## Train and then save the model ########################\n",
    "    if len(train_indices)>0:                \n",
    "        sess.run(tf.global_variables_initializer())    \n",
    "        \n",
    "        train_indices, _ = mclfu.adjust_for_batch_size(train_indices, train_indices, BATCH_SIZE)\n",
    "        train_count = len(train_indices)\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch:\", epoch)\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for ix in range(0, train_count, BATCH_SIZE):                                            \n",
    "                batch_x = get_window_data(ds, train_indices[ix:ix+BATCH_SIZE])                 \n",
    "                batch_y = train_indices[ix:ix+BATCH_SIZE, -1].reshape((-1,1))                 \n",
    "                sess.run(train_step, feed_dict={x:batch_x, y:batch_y})                \n",
    "\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={x:batch_x, y:batch_y})        \n",
    "                total_loss+= loss*BATCH_SIZE\n",
    "                total_acc += acc*BATCH_SIZE                \n",
    "            print('  Train loss: {:.4f}, acc: {:.4f}'.format(total_loss/train_count, total_acc/train_count))\n",
    "\n",
    "            test_count = len(test_indices)\n",
    "            if test_count>0:            \n",
    "                test_indices, _ = mclfu.adjust_for_batch_size(test_indices, test_indices, BATCH_SIZE)\n",
    "                total_loss, total_acc = 0, 0\n",
    "                for ix in range(0, test_count, BATCH_SIZE):                \n",
    "                    batch_x, batch_features = get_window_data(ds, test_indices[ix:ix+BATCH_SIZE])\n",
    "                    batch_y = test_indices[ix:ix+BATCH_SIZE, -1].reshape((-1,1))                      \n",
    "                    loss, acc = sess.run([loss_op, accuracy], feed_dict={x:batch_x, y:batch_y})                \n",
    "                    total_loss+= loss*BATCH_SIZE\n",
    "                    total_acc += acc*BATCH_SIZE                \n",
    "                print('  Test loss: {:.4f}, acc: {:.4f}'.format(total_loss/test_count, total_acc/test_count))\n",
    "\n",
    "        print('!!!!!!!!!!!!!!! Optimization Finished !!!!!!!!!!!!!!!!!')\n",
    "\n",
    "        if model_path_dest:\n",
    "            saver = tf.train.Saver()            \n",
    "            mfileu.create_directory(model_path_dest)\n",
    "            saver.save(sess, model_path_dest+'/model')    \n",
    "            print(\"Model Saved!\")\n",
    "        sess.close()\n",
    "        \n",
    "    ########## Restore the model and then Test  ########################\n",
    "    else:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path_src+'/model')\n",
    "        print(\"Model Loaded for test!\")\n",
    "        \n",
    "        test_count_original = len(test_indices)        \n",
    "        test_indices, _ = mclfu.adjust_for_batch_size(test_indices, test_indices, BATCH_SIZE)\n",
    "        test_count = len(test_indices)\n",
    "        res = np.zeros((test_count, 1))\n",
    "        \n",
    "        for ix in range(0, test_count, BATCH_SIZE):                \n",
    "            batch_x, batch_features = get_window_data(ds, test_indices[ix:ix+BATCH_SIZE], win_size)\n",
    "            batch_y = test_indices[ix:ix+BATCH_SIZE, -1].reshape((-1,1))  \n",
    "            pred = sess.run([prediction], feed_dict={x: batch_x, y:batch_y})            \n",
    "            res[ix:ix+BATCH_SIZE, 0] = np.array(pred).reshape((-1, ))\n",
    "        \n",
    "        res = res[:test_count_original, :]        \n",
    "        sess.close()\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Subject, Epochs, Win Size: 0, 1, 80 =============\n",
      "=========================== Train/Test: train =============================\n"
     ]
    }
   ],
   "source": [
    "print(\"==================== Subject, Epochs, Win Size: {}, {}, {} =============\".format(subj, num_epochs, win_size))\n",
    "print(\"=========================== Train/Test: {} =============================\".format(train_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = mfileu.get_path()\n",
    "model_folder_src = path+'/bite_models_DeepConvLSTM'\n",
    "model_folder_dest = model_folder_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.....\n",
      "Indices summary after subject filter total, neg, pos: 1914621 1895991 18630\n",
      "train, val shapes:  (1723158, 4) (191463, 4) 16767 1863\n",
      "Inside get_net,  x_shape:  [None, 80, 6]\n",
      "Conv Layer Shapes: \n",
      "Conv Layer 1:  [None, 76, 6, 64]\n",
      "Conv Layer 2:  [None, 72, 6, 64]\n",
      "Conv Layer 3:  [None, 68, 6, 64]\n",
      "Conv Layer 4:  [None, 64, 6, 64]\n",
      "Lstm out shape:  [100, 128]\n",
      "Logit shape:  [100, 1]\n",
      "Epoch: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-6e5c8555e74c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mmodel_path_dest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_folder_dest\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/subj_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mtrain_test_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path_dest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_path_dest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-66-59021482dcfd>\u001b[0m in \u001b[0;36mtrain_test_model\u001b[1;34m(ds, train_indices, test_indices, model_path_dest, model_path_src)\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[0mbatch_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_window_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if train_test == 'train':\n",
    "    print(\"Training.....\")\n",
    "    ds = mfileu.read_file('data', 'lab_data_steven.pkl')\n",
    "    ds, _, _ = mslabu.separate_right_left_annots(ds)    \n",
    "    ds = normalize_data(ds)    \n",
    "    indices = mfileu.read_file('features', 'lab_labels_steven_single_array.pkl')\n",
    "    \n",
    "    indices = indices[indices[:,0]!=subj, :]        \n",
    "    assert np.sum(indices[:, 0]==subj) == 0    \n",
    "    assert np.sum(indices[:, -1]>1) == 0\n",
    "    assert np.sum(indices[:, -1]<0) == 0\n",
    "    print(\"Indices summary after subject filter total, neg, pos:\", len(indices), np.sum(indices[:, -1]==0), np.sum(indices[:, -1]==1))\n",
    "        \n",
    "    indices = shuffle(indices)    \n",
    "    train_indices, val_indices = train_test_split(indices, test_size=0.1, stratify=indices[:, -1])\n",
    "    \n",
    "    print(\"train, val shapes: \", train_indices.shape, val_indices.shape, np.sum(train_indices[:, -1]), np.sum(val_indices[:, -1]))\n",
    "    \n",
    "    model_path_dest = model_folder_dest+\"/subj_\"+str(subj)\n",
    "    train_test_model(ds, train_indices=train_indices, test_indices=val_indices, model_path_dest=model_path_dest)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_test=='test_lab':\n",
    "    print(\"Bite Model Testing Lab .....\")\n",
    "    ds = mfileu.read_file('data', 'lab_data_steven_smoothed.pkl')\n",
    "    ds, _, _ = mslabu.separate_right_left_annots(ds)\n",
    "    ds = normalize_data(ds) \n",
    "    indices = mfileu.read_file('features', 'lab_labels_steven_dict.pkl')\n",
    "        \n",
    "    res = {}    \n",
    "    for subj in range(len(ds)):        \n",
    "        for sess in range(len(ds[subj])):\n",
    "            test_indices[:, 2] = indices[(subj, sess)]            \n",
    "            print(\"Subj, sess, indices shape: \", subj, sess, test_indices.shape)\n",
    "                \n",
    "            model_path_src = model_folder_src+\"/subj_\"+str(subj)            \n",
    "            pred = train_test_model(ds, train_indices=[], test_indices=test_indices, model_path_src=model_path_src)            \n",
    "            print(\"Prediction shape: \", pred.shape, np.sum(pred), np.sum(pred>=0.5))            \n",
    "            assert len(pred)==len(test_indices)\n",
    "            print(\"Sample Predictions :\", pred[:20])\n",
    "            \n",
    "            ixpred = np.zeros((len(pred), 2))\n",
    "            ixpred[:, 0] = ix\n",
    "            ixpred[:, 1] = pred            \n",
    "            res[(subj, sess)] = ixpred\n",
    "            print(\"Result shape, total_prob, pos_count: \", res[(subj, sess)].shape, np.sum(res[(subj, sess)][:, 1]), np.sum(res[(subj, sess)][:, 1]>=0.5))            \n",
    "            \n",
    "    mfileu.write_file('all_proba', 'all_proba_bite_lab_DeepConvLSTM.pkl', res)\n",
    "    print(\"Done Lab bite Testing\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_test=='test_free':\n",
    "    print(\"Bite Model Testing Free .....\")\n",
    "    ds = mfileu.read_file('data', 'free_data_steven_right_smoothed.pkl')    \n",
    "    ds = normalize_data(ds)\n",
    "    fs = mfileu.read_file('features', 'free_features_steven_right.pkl')\n",
    "    \n",
    "    res = {}    \n",
    "    for subj in range(len(ds)):        \n",
    "        for sess in range(len(ds[subj])):\n",
    "            \n",
    "            ix = fs[subj][sess][:, 0]\n",
    "            test_indices = np.zeros((len(ix), 4)).astype(int)\n",
    "            test_indices[:, 0] = subj\n",
    "            test_indices[:, 1] = sess\n",
    "            test_indices[:, 2] = ix            \n",
    "            print(\"Subj, sess, indices shape: \", subj, sess, ix.shape, test_indices.shape)\n",
    "                \n",
    "            lab_subj = subj+2 if subj<5 else 100            \n",
    "            model_path_src = model_folder_src+\"/subj_\"+str(lab_subj)            \n",
    "            pred = train_test_model(ds, mu=mu, sigma=sigma, train_indices=[], test_indices=test_indices, model_path_src=model_path_src)            \n",
    "            print(\"Prediction shape: \", pred.shape, np.sum(pred), np.sum(pred>=0.5))            \n",
    "            assert len(pred)==len(test_indices)\n",
    "            print(pred[:20])\n",
    "            \n",
    "            ixpred = np.zeros((len(pred), 2))\n",
    "            ixpred[:, 0] = ix\n",
    "            ixpred[:, 1] = pred            \n",
    "            res[(subj, sess)] = ixpred\n",
    "            print(\"Result shape, total_prob, pos_count: \", res[(subj, sess)].shape, np.sum(res[(subj, sess)][:, 1]), np.sum(res[(subj, sess)][:, 1]>=0.5))            \n",
    "            \n",
    "    mfileu.write_file('all_proba', 'all_proba_bite_free_DeepConvLSTM.pkl', res)\n",
    "    print(\"Done Free Bite Testing\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
