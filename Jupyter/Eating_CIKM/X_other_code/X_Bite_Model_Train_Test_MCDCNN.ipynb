{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import deep_train_test_utils as dttu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_path = 'C:/ASM/Dropbox/Developments/Jupyter/Eating_CIKM/my_utils' if 'C:' in os.getcwd() else './my_utils'\n",
    "sys.path.append(util_path)\n",
    "import my_file_utils as mfileu\n",
    "import my_steven_lab_utils as mslabu\n",
    "import my_classification_utils as mclfu\n",
    "import my_data_process_utils as mdpu\n",
    "import my_tensorflow_dense_utils as mdenseu\n",
    "#importlib.reload(dttu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj, num_epochs, train_test = 0, 1, 'test_lab'\n",
    "if 'C:' not in mfileu.get_path():    \n",
    "    subj, num_epochs, train_test = int(sys.argv[1]), int(sys.argv[2]), sys.argv[3]    \n",
    "assert train_test in ['train', 'test_lab', 'test_free']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_count, win_size, step_size = 6, 5*16, 2\n",
    "print(\"axis_count: {}, win_size: {}, step_size: {}\".format(axis_count, win_size, step_size) )\n",
    "\n",
    "learning_rate, num_epochs, batch_size, keep_prob_val = 0.01, num_epochs, 128, 0.25\n",
    "print(\"learning_rate: {}, num_epochs: {}, batch_size: {}, keep_prob_val: {}\".format(learning_rate, num_epochs, batch_size, keep_prob_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(x, size_in, size_out, ksize, strides, padding, name):    \n",
    "    strides = [1, strides[0], strides[1], 1]   \n",
    "    with tf.name_scope(name):\n",
    "        W = tf.Variable(tf.truncated_normal([ksize[0], ksize[1], size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.0, shape=[size_out]), name=\"b\")\n",
    "        conv = tf.nn.conv2d(x, W, strides=strides, padding=padding)\n",
    "        output = tf.nn.sigmoid(conv + b)\n",
    "        tf.summary.histogram(\"weights\", W)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"outputs\", output)\n",
    "        return output\n",
    "    \n",
    "def avgpool_layer(x, ksize, strides, padding, name):\n",
    "    ksize = [1, ksize[0], ksize[1], 1]\n",
    "    strides = [1, strides[0], strides[1], 1]\n",
    "    with tf.name_scope(name):\n",
    "        output = tf.nn.avg_pool(x, ksize=ksize, strides=strides, padding=padding)\n",
    "        tf.summary.histogram(\"outputs\", output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_axis_conv_net(x, name):\n",
    "    x_shape =x.get_shape().as_list()\n",
    "    print('Inside one_axis_net: ',name,', x_shape', x_shape)\n",
    "        \n",
    "    with tf.name_scope(name):\n",
    "        x = tf.reshape(x, shape=[-1, x.shape[1], 1, 1], name=\"reshape\")\n",
    "        conv_1 = conv_layer(x, size_in=1, size_out=8, ksize=[5,1], strides=[1,1], padding=\"VALID\", name='conv_1')        \n",
    "        pool_1 = avgpool_layer(conv_1, ksize=[2,1], strides=[2,1], padding=\"VALID\", name=\"pool_1\")\n",
    "        print(\"  Conv_1, pool_1 shape: \", conv_1.get_shape().as_list(), pool_1.get_shape().as_list())\n",
    "\n",
    "        conv_2 = conv_layer(pool_1, size_in=8, size_out=4, ksize=[5,1], strides=[1,1], padding=\"VALID\", name='conv_2')\n",
    "        pool_2 = avgpool_layer(conv_2, ksize=[2,1], strides=[2,1], padding=\"VALID\", name=\"pool_2\")\n",
    "        print(\"  Conv_2, pool_2 shape: \", conv_2.get_shape().as_list(), pool_2.get_shape().as_list())\n",
    "        \n",
    "        sz = pool_2.get_shape().as_list()\n",
    "        flattened = tf.reshape(pool_2, shape=[-1, sz[1]*sz[2]*sz[3]], name=\"Flattened\")\n",
    "        return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_axes_net(x, name):\n",
    "    x_shape = x.get_shape().as_list()   \n",
    "    print('Inside all_axes_net: x_shape:', x_shape)    \n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "        all_axes = []        \n",
    "        for i in range(x_shape[-1]):\n",
    "            axis_data = x[:, :, i]            \n",
    "            a = one_axis_conv_net(axis_data, name=\"conv_net_axis_\"+str(i))\n",
    "            all_axes.append(a)\n",
    "            print(\"One axis, shape: \", i, all_axes[-1].get_shape().as_list())\n",
    "        \n",
    "        print(\"All axis list size:\", len(all_axes))        \n",
    "        combo_flattened = tf.concat(all_axes, axis=1, name='concat_axis')        \n",
    "        print(\"Combined flattened shape: \", combo_flattened.get_shape().as_list())\n",
    "\n",
    "        return combo_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(x, y):\n",
    "    y_shape = y.get_shape().as_list()\n",
    "    print(\"Inside get_net, y_shape: \", y_shape)\n",
    "    \n",
    "    combo_flattened = all_axes_net(x, 'all_axes_conv_net')\n",
    "    combo_shape = combo_flattened.get_shape().as_list()\n",
    "    \n",
    "    fc_1 = mdenseu.fc_layer(combo_flattened, size_out=732, activation=\"sigmoid\", name='FC_1')         \n",
    "    logits = mdenseu.fc_layer(fc_1, size_out=y_shape[-1], activation=\"sigmoid\", name='logits')\n",
    "    print(\"Shapes Fc_1, Logit:\", fc_1.get_shape().as_list(), logits.get_shape().as_list())\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(ds, mu, sigma, train_indices, test_indices, model_path_dest=None, model_path_src=None):\n",
    "    #print_out, sys.stdout = sys.stdout, open(os.devnull, 'w')\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, win_size, axis_count], name=\"x\")        \n",
    "    y = tf.placeholder(tf.float32, [None, 1], name=\"y\")\n",
    "    #keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    \n",
    "    logits = get_net(x, y)    \n",
    "    prediction = tf.nn.sigmoid(logits, name=\"prediction\")\n",
    "    correct_prediction = tf.equal(tf.greater(prediction, 0.5), tf.equal(y,1), name=\"correct_prediction\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y), name=\"loss_op\")        \n",
    "    train_step = tf.train.RMSPropOptimizer(learning_rate=0.01, decay=0.0005, momentum=0.9).minimize(loss_op, name=\"train_step\")\n",
    "\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    #sys.stdout = print_out\n",
    "    \n",
    "    ########## Train and then save the model ########################\n",
    "    if len(train_indices)>0:                \n",
    "        sess.run(tf.global_variables_initializer())                \n",
    "        train_indices, _ = mclfu.adjust_for_batch_size(train_indices, train_indices, batch_size)\n",
    "\n",
    "        train_count = len(train_indices)\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch:\", epoch)\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for ix in range(0, train_count, batch_size):                                            \n",
    "                batch_x = dttu.get_window_data_normalized(ds, train_indices[ix:ix+batch_size], win_size, mu, sigma)                 \n",
    "                batch_y = train_indices[ix:ix+batch_size, -1].reshape((-1,1))                 \n",
    "                sess.run(train_step, feed_dict={x:batch_x, y:batch_y})                \n",
    "\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={x:batch_x, y:batch_y})        \n",
    "                total_loss+= loss*batch_size\n",
    "                total_acc += acc*batch_size                \n",
    "            print('  Train loss: {:.8f}, acc: {:.8f}'.format(total_loss/train_count, total_acc/train_count))\n",
    "\n",
    "            test_count = len(test_indices)\n",
    "            if test_count>0:            \n",
    "                test_indices, _ = mclfu.adjust_for_batch_size(test_indices, test_indices, batch_size)\n",
    "                total_loss, total_acc = 0, 0\n",
    "                for ix in range(0, test_count, batch_size):                \n",
    "                    batch_x = dttu.get_window_data_normalized(ds, test_indices[ix:ix+batch_size], win_size, mu, sigma)\n",
    "                    batch_y = test_indices[ix:ix+batch_size, -1].reshape((-1,1))                      \n",
    "                    loss, acc = sess.run([loss_op, accuracy], feed_dict={x:batch_x, y:batch_y})                \n",
    "                    total_loss+= loss*batch_size\n",
    "                    total_acc += acc*batch_size                \n",
    "                print('  Test loss: {:.8f}, acc: {:.8f}'.format(total_loss/test_count, total_acc/test_count))\n",
    "\n",
    "        print('!!!!!!!!!!!!!!! Optimization Finished !!!!!!!!!!!!!!!!!')\n",
    "\n",
    "        if model_path_dest:\n",
    "            saver = tf.train.Saver()            \n",
    "            mfileu.create_directory(model_path_dest)\n",
    "            saver.save(sess, model_path_dest+'/model')    \n",
    "            print(\"Model Saved!\")\n",
    "        sess.close()\n",
    "        \n",
    "    ########## Restore the model and then Test  ########################\n",
    "    else:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path_src+'/model')\n",
    "        print(\"Model Loaded for test!\")\n",
    "        \n",
    "        test_count_original = len(test_indices)        \n",
    "        test_indices, _ = mclfu.adjust_for_batch_size(test_indices, test_indices, batch_size)\n",
    "        test_count = len(test_indices)\n",
    "        res = np.zeros((test_count, ))\n",
    "        \n",
    "        for ix in range(0, test_count, batch_size):                \n",
    "            batch_x = dttu.get_window_data_normalized(ds, test_indices[ix:ix+batch_size], win_size, mu, sigma)\n",
    "            batch_y = test_indices[ix:ix+batch_size, -1].reshape((-1,1))  \n",
    "            pred = sess.run([prediction], feed_dict={x: batch_x, y:batch_y})            \n",
    "            res[ix:ix+batch_size] = np.array(pred).reshape((-1, ))\n",
    "        \n",
    "        res = res[:test_count_original]        \n",
    "        sess.close()\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"====================== Subject: {}, Epochs: {}, Win Size: {} ==============\".format(subj, num_epochs, win_size))\n",
    "print(\"=============================  Train/Test: {} =============================\".format(train_test))\n",
    "\n",
    "path = mfileu.get_path()\n",
    "model_folder_src = path+'/bite_models_MCDCNN'\n",
    "model_folder_dest = model_folder_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if train_test == 'train':\n",
    "    print(\"Bite Model Training.....\")\n",
    "    ds = mfileu.read_file('data', 'lab_data_steven_smoothed.pkl')\n",
    "    ds, _, _ = mslabu.separate_right_left_annots(ds)\n",
    "    mu_sigma_labels = mfileu.read_file('features', 'mu_sigma_labels_lab_right.pkl')\n",
    "    mu_all, sigma_all, indices_labels = mu_sigma_labels['mu'], mu_sigma_labels['sigma'], mu_sigma_labels['labels']\n",
    "    #print(\"Mu \", mu)\n",
    "    #print(\"Sigma\", sigma)\n",
    "    \n",
    "    mu, sigma = (mu_all[subj, :], sigma_all[subj, :]) if subj<len(ds) else (mu_all[-1, :], sigma_all[-1, :])    \n",
    "    print(mu.shape, sigma.shape)\n",
    "    print(\"Mu \", mu)\n",
    "    print(\"Sigma\", sigma)\n",
    "    \n",
    "    \n",
    "    indices = dttu.get_all_indices_labels_exclude_subject(ds, indices_labels, exclude_subj=subj)    \n",
    "    assert np.sum(indices[:, 0]==subj) == 0        \n",
    "    print(\"Indices summary after subject filter total, neg, pos:\", len(indices), np.sum(indices[:, -1]==0), np.sum(indices[:, -1]==1))\n",
    "        \n",
    "    indices = shuffle(indices)    \n",
    "    train_indices, val_indices = train_test_split(indices, test_size=0.1, stratify=indices[:, -1])\n",
    "    \n",
    "    print(\"train, val shapes: \", train_indices.shape, val_indices.shape, np.sum(train_indices[:, -1]), np.sum(val_indices[:, -1]))\n",
    "    \n",
    "    model_path_dest = model_folder_dest + \"/subj_\" + str(subj)\n",
    "    train_test_model(ds, mu=mu, sigma=sigma, train_indices=train_indices, test_indices=val_indices, model_path_dest=model_path_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_test=='test_lab':\n",
    "    print(\"Bite Model Testing Lab .....\")\n",
    "    ds = mfileu.read_file('data', 'lab_data_steven_smoothed.pkl')\n",
    "    ds, _, _ = mslabu.separate_right_left_annots(ds)\n",
    "    fs = mfileu.read_file('features', 'lab_features_steven_right.pkl')\n",
    "    \n",
    "    mu_sigma_labels = mfileu.read_file('features', 'mu_sigma_labels_lab_right.pkl')\n",
    "    mu_all, sigma_all, _ = mu_sigma_labels['mu'], mu_sigma_labels['sigma'], mu_sigma_labels['labels']\n",
    "    \n",
    "    res = {}    \n",
    "    for subj in range(len(ds)):        \n",
    "        for sess in range(len(ds[subj])):\n",
    "            mu, sigma = mu_all[subj, :], sigma_all[subj, :]  \n",
    "            \n",
    "            ix = fs[subj][sess][:, 0]\n",
    "            test_indices = np.zeros((len(ix), 4)).astype(int)\n",
    "            test_indices[:, 0] = subj\n",
    "            test_indices[:, 1] = sess\n",
    "            test_indices[:, 2] = ix            \n",
    "            print(\"Subj, sess, indices shape: \", subj, sess, ix.shape, test_indices.shape)\n",
    "                \n",
    "            model_path_src = model_folder_src+\"/subj_\"+str(subj)            \n",
    "            pred = train_test_model(ds, mu=mu, sigma=sigma, train_indices=[], test_indices=test_indices, model_path_src=model_path_src)            \n",
    "            print(\"Prediction shape: \", pred.shape, np.sum(pred), np.sum(pred>=0.5))            \n",
    "            assert len(pred)==len(test_indices)\n",
    "            print(pred[:20])\n",
    "            \n",
    "            ixpred = np.zeros((len(pred), 2))\n",
    "            ixpred[:, 0] = ix\n",
    "            ixpred[:, 1] = pred            \n",
    "            res[(subj, sess)] = ixpred\n",
    "            print(\"Result shape, total_prob, pos_count: \", res[(subj, sess)].shape, np.sum(res[(subj, sess)][:, 1]), np.sum(res[(subj, sess)][:, 1]>=0.5))            \n",
    "            \n",
    "    mfileu.write_file('all_proba', 'all_proba_bite_lab_MCDCNN.pkl', res)\n",
    "    print(\"Done Lab bite Testing\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_test=='test_free':\n",
    "    print(\"Bite Model Testing Free .....\")\n",
    "    ds = mfileu.read_file('data', 'free_data_steven_right_smoothed.pkl')    \n",
    "    fs = mfileu.read_file('features', 'free_features_steven_right.pkl')\n",
    "    \n",
    "    mu_sigma_labels = mfileu.read_file('features', 'mu_sigma_labels_lab_right.pkl')\n",
    "    mu_all, sigma_all, _ = mu_sigma_labels['mu'], mu_sigma_labels['sigma'], mu_sigma_labels['labels']\n",
    "    \n",
    "    res = {}    \n",
    "    for subj in range(len(ds)):        \n",
    "        for sess in range(len(ds[subj])):\n",
    "            \n",
    "            lab_subj = subj+2 if subj<5 else -1            \n",
    "            mu, sigma = mu_all[lab_subj, :], sigma_all[lab_subj, :]  \n",
    "            \n",
    "            ix = fs[subj][sess][:, 0]\n",
    "            test_indices = np.zeros((len(ix), 4)).astype(int)\n",
    "            test_indices[:, 0] = subj\n",
    "            test_indices[:, 1] = sess\n",
    "            test_indices[:, 2] = ix            \n",
    "            print(\"Subj, sess, indices shape: \", subj, sess, ix.shape, test_indices.shape)\n",
    "                \n",
    "            lab_subj = subj+2 if subj<5 else 100            \n",
    "            model_path_src = model_folder_src+\"/subj_\"+str(lab_subj)            \n",
    "            pred = train_test_model(ds, mu=mu, sigma=sigma, train_indices=[], test_indices=test_indices, model_path_src=model_path_src)            \n",
    "            print(\"Prediction shape: \", pred.shape, np.sum(pred), np.sum(pred>=0.5))            \n",
    "            assert len(pred)==len(test_indices)\n",
    "            print(pred[:20])\n",
    "            \n",
    "            ixpred = np.zeros((len(pred), 2))\n",
    "            ixpred[:, 0] = ix\n",
    "            ixpred[:, 1] = pred            \n",
    "            res[(subj, sess)] = ixpred\n",
    "            print(\"Result shape, total_prob, pos_count: \", res[(subj, sess)].shape, np.sum(res[(subj, sess)][:, 1]), np.sum(res[(subj, sess)][:, 1]>=0.5))            \n",
    "            \n",
    "    mfileu.write_file('all_proba', 'all_proba_bite_free_MCDCNN.pkl', res)\n",
    "    print(\"Done Free Bite Testing\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
