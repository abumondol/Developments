{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_path = 'C:/ASM/Dropbox/Developments/Jupyter/Eating/myutils' if 'C:' in os.getcwd() else './myutils'\n",
    "sys.path.append(util_path)\n",
    "import my_file_utils as mfileu\n",
    "import my_classification_utils as mclfu\n",
    "import my_steven_lab_utils as mslabu\n",
    "import my_tensorflow_cnn_utils as mcnnu\n",
    "import my_tensorflow_lstm_utils as mlstmu\n",
    "import my_tensorflow_dense_utils as mdenseu\n",
    "#importlib.reload(biteu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win size: 80 , Step size: 8 , var min: 1 , var max: 25 , gx_th: -0.25\n"
     ]
    }
   ],
   "source": [
    "axis_count, label_shape_1 = 6, 1 \n",
    "win_size, step_size, feature_count = 5*16, 8, 32\n",
    "var_min, var_max, gx_th = 1, 25, -0.25\n",
    "print(\"Win size:\", win_size, \", Step size:\", step_size, \", var min:\", var_min, \", var max:\", var_max, ', gx_th:', gx_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_data(ds, indices, win_size, offset=0):\n",
    "    count = len(indices)\n",
    "    w = np.zeros((count, win_size, 6))    \n",
    "    features = np.zeros((count, 32))    \n",
    "    \n",
    "    for i in range(count):\n",
    "        subj, sess, ix = indices[i, 0], indices[i, 1], indices[i, 2]\n",
    "        accel = ds[subj][sess][ix:ix+win_size, 1:4]\n",
    "        #grav = ds[subj][sess][ix:ix+win_size, -3:]\n",
    "        gyro = ds[subj][sess][ix:ix+win_size, 4:7]\n",
    "        w[i, :, :] = np.concatenate((accel, gyro), axis=1)        \n",
    "    \n",
    "    return w, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj, num_epochs, train_test = 0, 1, 'test_lab'\n",
    "if 'C:' not in mfileu.get_path():    \n",
    "    subj, num_epochs, train_test = int(sys.argv[1]), int(sys.argv[2]), sys.argv[3]\n",
    "    \n",
    "assert train_test in ['train', 'test_lab', 'test_free']\n",
    "params={}\n",
    "params['learning_rate'] = 0.001\n",
    "params['num_epochs'] = num_epochs\n",
    "params['batch_size'] = 128\n",
    "params['keep_prob_val'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(ds, train_indices, test_indices, params, model_path_dest=None, model_path_src=None):\n",
    "    learning_rate = params['learning_rate']\n",
    "    num_epochs = params['num_epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    keep_prob_val = params['keep_prob_val']\n",
    "    print(\"****** Learning rate \", learning_rate)\n",
    "    \n",
    "    print_out, sys.stdout = sys.stdout, open(os.devnull, 'w')    \n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, win_size, axis_count], name=\"x\")    \n",
    "    #features = tf.placeholder(tf.float32, [None, feature_count], name=\"features\")\n",
    "    y = tf.placeholder(tf.float32, [None, label_shape_1], name=\"y\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")    \n",
    "    \n",
    "    cnn_out = mcnnu.all_sensor_net(x, name=\"all_sensor_net\")\n",
    "    lstm_out_fw, lstm_out_bw = mlstmu.multi_layer_biLSTM(cnn_out, batch_size=batch_size, n_hidden=64, n_layer=1)\n",
    "    lstm_out = tf.concat([lstm_out_fw[:, -1, :], lstm_out_bw[:, 0, :]], axis =1)\n",
    "    \n",
    "    print(\"Lstm out shapes(fw, bw): \", lstm_out_fw.get_shape().as_list(), lstm_out_bw.get_shape().as_list())\n",
    "    print(\"FW LSTM out shape:\", lstm_out_fw[:, -1, :].get_shape().as_list())\n",
    "    print(\"BW LSTM out shape:\", lstm_out_bw[:, -1, :].get_shape().as_list())    \n",
    "    print(\"Lstm out shape final: \", lstm_out.get_shape().as_list())\n",
    "    \n",
    "    drop_layer = tf.nn.dropout(lstm_out, keep_prob=keep_prob, name=\"dropout1\")\n",
    "    #fc_layer= mdenseu.fc_layer(drop_layer1, 64, name=\"Dense_1\", activation='relu')    \n",
    "    #drop_layer = tf.nn.dropout(fc_layer, keep_prob=keep_prob, name=\"dropout2\")\n",
    "    print(\"Drop layer shape: \",drop_layer.get_shape().as_list())\n",
    "    logits = mdenseu.fc_layer(drop_layer, 1, name=\"Logits\")    \n",
    "    \n",
    "    print(\"Logit shape: \",logits.get_shape().as_list())\n",
    "    prediction = tf.nn.sigmoid(logits, name=\"prediction\")\n",
    "    correct_prediction = tf.equal(tf.greater(prediction, 0.5), tf.equal(y,1), name=\"correct_prediction\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y), name=\"loss_op\")        \n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_op, name=\"train_step\")\n",
    "\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    sys.stdout = print_out\n",
    "    ########## Train and then save the model ########################\n",
    "    if len(train_indices)>0:                \n",
    "        sess.run(tf.global_variables_initializer())    \n",
    "            \n",
    "        train_indices, _ = mclfu.adjust_for_batch_size(train_indices, train_indices, batch_size)\n",
    "\n",
    "        train_count = len(train_indices)\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch:\", epoch)\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for ix in range(0, train_count, batch_size):                                            \n",
    "                batch_x, batch_features = get_window_data(ds, train_indices[ix:ix+batch_size], win_size)                 \n",
    "                batch_y = train_indices[ix:ix+batch_size, -1].reshape((-1,1))                 \n",
    "                sess.run(train_step, feed_dict={x:batch_x, y:batch_y, keep_prob:keep_prob_val})                \n",
    "\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={x:batch_x, y:batch_y, keep_prob:keep_prob_val})        \n",
    "                total_loss+= loss*batch_size\n",
    "                total_acc += acc*batch_size                \n",
    "            print('  Train loss: {:.4f}, acc: {:.4f}'.format(total_loss/train_count, total_acc/train_count))\n",
    "\n",
    "            test_count = len(test_indices)\n",
    "            if test_count>0:            \n",
    "                test_indices, _ = mclfu.adjust_for_batch_size(test_indices, test_indices, batch_size)\n",
    "                total_loss, total_acc = 0, 0\n",
    "                for ix in range(0, test_count, batch_size):                \n",
    "                    batch_x, batch_features = get_window_data(ds, test_indices[ix:ix+batch_size], win_size)\n",
    "                    batch_y = test_indices[ix:ix+batch_size, -1].reshape((-1,1))                      \n",
    "                    loss, acc = sess.run([loss_op, accuracy], feed_dict={x:batch_x, y:batch_y, keep_prob:1.0})                \n",
    "                    total_loss+= loss*batch_size\n",
    "                    total_acc += acc*batch_size                \n",
    "                print('  Test loss: {:.4f}, acc: {:.4f}'.format(total_loss/test_count, total_acc/test_count))\n",
    "\n",
    "        print('!!!!!!!!!!!!!!! Optimization Finished !!!!!!!!!!!!!!!!!')\n",
    "\n",
    "        if model_path_dest:\n",
    "            saver = tf.train.Saver()            \n",
    "            mfileu.create_directory(model_path_dest)\n",
    "            saver.save(sess, model_path_dest+'/model')    \n",
    "            print(\"Model Saved!\")\n",
    "        sess.close()\n",
    "        \n",
    "    ########## Restore the model and then Test  ########################\n",
    "    else:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path_src+'/model')\n",
    "        print(\"Model Loaded for test!\")\n",
    "        \n",
    "        test_count_original = len(test_indices)        \n",
    "        test_indices, _ = mclfu.adjust_for_batch_size(test_indices, test_indices, batch_size)\n",
    "        test_count = len(test_indices)\n",
    "        res = np.zeros((test_count, 1))\n",
    "        \n",
    "        for ix in range(0, test_count, batch_size):                \n",
    "            batch_x, batch_features = get_window_data(ds, test_indices[ix:ix+batch_size], win_size)\n",
    "            batch_y = test_indices[ix:ix+batch_size, -1].reshape((-1,1))  \n",
    "            pred = sess.run([prediction], feed_dict={x: batch_x, y:batch_y, keep_prob:1.0})            \n",
    "            res[ix:ix+batch_size, 0] = np.array(pred).reshape((-1, ))\n",
    "        \n",
    "        res = res[:test_count_original, :]        \n",
    "        sess.close()\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================  Train/Test: test_lab =============================\n",
      "\n",
      "============ Subject, Epochs, Win Size: 0, 1, 80 =============\n"
     ]
    }
   ],
   "source": [
    "print(\"=============================  Train/Test: {} =============================\".format(train_test))\n",
    "print(\"\\n============ Subject, Epochs, Win Size: {}, {}, {} =============\".format(subj, num_epochs, win_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = mfileu.get_path()\n",
    "model_folder_src = path+'/bite_models/our'\n",
    "model_folder_dest = path+'/bite_models/our'\n",
    "result_folder = 'our_test_proba_bite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_indices_lab(ssilv, exclude_subj, step=8):    \n",
    "    ssil_all = []\n",
    "    subj_count = len(ssilv)\n",
    "    \n",
    "    for subj in range(subj_count):        \n",
    "        if subj==exclude_subj:\n",
    "            continue\n",
    "        for sess in range(len(ssilv[subj])):                         \n",
    "            ssil = ssilv[subj][sess][::step, :4]\n",
    "            ssil_all = ssil if len(ssil_all)==0 else np.concatenate((ssil_all, ssil))\n",
    "    \n",
    "    ssil_all = ssil_all.astype(int)\n",
    "    cond = (ssil_all[:,-1]>=2) | (ssil_all[:,-1]<0 ) \n",
    "    ssil_all[cond, -1] = 0\n",
    "    return ssil_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if train_test == 'train':\n",
    "    print(\"Training.....\")\n",
    "    \n",
    "    dlab = mfileu.read_file('data', 'lab_data_steven_right_smoothed.pkl')\n",
    "    ssilv = mfileu.read_file('ssilv', 'lab_ssilv_steven_right.pkl')\n",
    "    \n",
    "    indices = get_all_indices_lab(ssilv, exclude_subj=subj)\n",
    "    \n",
    "    assert np.sum(indices[:, 0]==subj) == 0    \n",
    "    assert np.sum(indices[:, -1]>1) == 0\n",
    "    assert np.sum(indices[:, -1]<0) == 0\n",
    "    print(\"Indices summary after subject filter total, neg, pos:\", len(indices), np.sum(indices[:, -1]==0), np.sum(indices[:, -1]==1))\n",
    "        \n",
    "    indices = shuffle(indices)    \n",
    "    train_indices, val_indices = train_test_split(indices, test_size=0.1, stratify=indices[:, -1])\n",
    "    \n",
    "    print(\"train, val shapes: \", train_indices.shape, val_indices.shape, np.sum(train_indices[:, -1]), np.sum(val_indices[:, -1]))\n",
    "    \n",
    "    model_path_dest = model_folder_dest+\"/subj_\"+str(subj)\n",
    "    train_test_model(dlab, train_indices, val_indices, params, model_path_dest=model_path_dest)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test = 'test'\n",
    "if train_test=='test_free':\n",
    "    print(\"Testing Free.....\")\n",
    "    \n",
    "    dfree = mfileu.read_file('data', 'free_data_steven_right_smoothed.pkl')\n",
    "    ssilv_free = mfileu.read_file('ssilv', 'free_ssilv_steven_right.pkl')\n",
    "    ba = mfileu.read_file('data', 'free_data_steven_blank_array.pkl')\n",
    "        \n",
    "    for subj in range(11):        \n",
    "        for sess in range(len(dfree[subj])):\n",
    "            ssil = ssilv_free[subj][sess][:, :4]\n",
    "            indices = ssil.astype(int)\n",
    "            print(\"\\n\\nSubj, sess, indices shape: \", subj, sess, indices.shape)\n",
    "            print(\"Indices summary total, neg, pos:\", len(indices), np.sum(indices[:, -1]==0), np.sum(indices[:, -1]==1))\n",
    "        \n",
    "            \n",
    "            src_subj = subj+2 if subj<5 else 100            \n",
    "            model_path_src = model_folder_src+\"/subj_\"+str(src_subj)            \n",
    "            proba = train_test_model(dfree, [], indices, params, model_path_src=model_path_src)            \n",
    "            print(\"Prediction shape: \", proba.shape, np.sum(proba), np.sum(proba>=0.5))            \n",
    "            assert len(proba)==len(indices)\n",
    "            \n",
    "            gt = np.copy(ssilv_free[subj][sess][:, 3])\n",
    "            ssilv_free[subj][sess][:, 3] = proba.reshape((-1, ))                    \n",
    "            ba[subj][sess] = ssilv_free[subj][sess][:, 2:]\n",
    "            print(\"Proba shape, pos, gtpos:\", proba.shape, np.sum(proba>=0.5), np.sum(gt==1)/40)\n",
    "            \n",
    "    mfileu.write_file('all_proba', 'all_proba_bite_free_our.pkl', ba)         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Lab .....\n",
      "\n",
      "\n",
      "Subj, sess, indices shape:  0 0 (363907, 4)\n",
      "Indices summary total, neg, pos: 363907 355501 3876\n",
      "****** Learning rate  0.001\n",
      "INFO:tensorflow:Restoring parameters from C:/ASM/DevData/eating/bite_models/our/subj_0/model\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to get matching files on C:/ASM/DevData/eating/bite_models/our/subj_0/model: Not found: FindFirstFile failed for: C:/ASM/DevData/eating/bite_models/our/subj_0 : The system cannot find the path specified.\r\n; No such process\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\t [[Node: save/RestoreV2/_79 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_84_save/RestoreV2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\asyncio\\base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-4e030473f9ed>\", line 18, in <module>\n    proba = train_test_model(dlab, [], indices, params, model_path_src=model_path_src)\n  File \"<ipython-input-6-53debe6fb1b4>\", line 86, in train_test_model\n    saver = tf.train.Saver()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1311, in __init__\n    self.build()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1320, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1357, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 809, in _build_internal\n    restore_sequentially, reshape)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 448, in _AddRestoreOps\n    restore_sequentially)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 860, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1541, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on C:/ASM/DevData/eating/bite_models/our/subj_0/model: Not found: FindFirstFile failed for: C:/ASM/DevData/eating/bite_models/our/subj_0 : The system cannot find the path specified.\r\n; No such process\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\t [[Node: save/RestoreV2/_79 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_84_save/RestoreV2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32mc:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on C:/ASM/DevData/eating/bite_models/our/subj_0/model: Not found: FindFirstFile failed for: C:/ASM/DevData/eating/bite_models/our/subj_0 : The system cannot find the path specified.\r\n; No such process\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\t [[Node: save/RestoreV2/_79 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_84_save/RestoreV2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4e030473f9ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0msrc_subj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mmodel_path_src\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_folder_src\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/subj_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_subj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdlab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path_src\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_path_src\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Prediction shape: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproba\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproba\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-53debe6fb1b4>\u001b[0m in \u001b[0;36mtrain_test_model\u001b[1;34m(ds, train_indices, test_indices, params, model_path_dest, model_path_src)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path_src\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model Loaded for test!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1775\u001b[1;33m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on C:/ASM/DevData/eating/bite_models/our/subj_0/model: Not found: FindFirstFile failed for: C:/ASM/DevData/eating/bite_models/our/subj_0 : The system cannot find the path specified.\r\n; No such process\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\t [[Node: save/RestoreV2/_79 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_84_save/RestoreV2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\asyncio\\base_events.py\", line 1431, in _run_once\n    handle._run()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-4e030473f9ed>\", line 18, in <module>\n    proba = train_test_model(dlab, [], indices, params, model_path_src=model_path_src)\n  File \"<ipython-input-6-53debe6fb1b4>\", line 86, in train_test_model\n    saver = tf.train.Saver()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1311, in __init__\n    self.build()\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1320, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1357, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 809, in _build_internal\n    restore_sequentially, reshape)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 448, in _AddRestoreOps\n    restore_sequentially)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 860, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1541, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Unsuccessful TensorSliceReader constructor: Failed to get matching files on C:/ASM/DevData/eating/bite_models/our/subj_0/model: Not found: FindFirstFile failed for: C:/ASM/DevData/eating/bite_models/our/subj_0 : The system cannot find the path specified.\r\n; No such process\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\t [[Node: save/RestoreV2/_79 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_84_save/RestoreV2\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "#train_test = 'test'\n",
    "if train_test=='test_lab':\n",
    "    print(\"Testing Lab .....\")\n",
    "    \n",
    "    dlab = mfileu.read_file('data', 'lab_data_steven_right_smoothed.pkl')\n",
    "    ssilv_lab = mfileu.read_file('ssilv', 'lab_ssilv_steven_right.pkl')\n",
    "    ba = mfileu.read_file('data', 'lab_data_steven_blank_array.pkl')\n",
    "        \n",
    "    for subj in range(len(dlab)):        \n",
    "        for sess in range(len(dlab[subj])):\n",
    "            ssil = ssilv_lab[subj][sess][:, :4]\n",
    "            indices = ssil.astype(int)\n",
    "            print(\"\\n\\nSubj, sess, indices shape: \", subj, sess, indices.shape)\n",
    "            print(\"Indices summary total, neg, pos:\", len(indices), np.sum(indices[:, -1]==0), np.sum(indices[:, -1]==1))\n",
    "        \n",
    "            src_subj = subj\n",
    "            model_path_src = model_folder_src+\"/subj_\"+str(src_subj)            \n",
    "            proba = train_test_model(dlab, [], indices, params, model_path_src=model_path_src)            \n",
    "            print(\"Prediction shape: \", proba.shape, np.sum(proba), np.sum(proba>=0.5))            \n",
    "            assert len(proba)==len(indices)\n",
    "            \n",
    "            gt = np.copy(ssilv_lab[subj][sess][:, 3])\n",
    "            ssilv_lab[subj][sess][:, 3] = proba.reshape((-1, ))                    \n",
    "            ba[subj][sess] = ssilv_lab[subj][sess][:, 2:]\n",
    "            print(\"Proba shape, pos, gtpos:\", proba.shape, np.sum(proba>=0.5), np.sum(gt==1)/40)\n",
    "            \n",
    "    mfileu.write_file('all_proba', 'all_proba_bite_lab_our.pkl', ba)         \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
