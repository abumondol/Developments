{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'my_tensorflow_cnn_utils' from 'C:/ASM/Dropbox/Developments/Jupyter/Eating/myutils\\\\my_tensorflow_cnn_utils.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util_path = 'C:/ASM/Dropbox/Developments/Jupyter/Eating/myutils' if 'C:' in os.getcwd() else './myutils'\n",
    "sys.path.append(util_path)\n",
    "import my_file_utils as mfileu\n",
    "import my_data_process_utils as mdpu\n",
    "import my_tensorflow_cnn_utils as mcnnu\n",
    "import my_classification_utils as mclfu\n",
    "importlib.reload(mcnnu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(train_x, train_y, test_x, test_y, params, model_path=None, model_name=None):\n",
    "    learning_rate = params['learning_rate']\n",
    "    num_epochs = params['num_epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    keep_prob_val = params['keep_prob_val']\n",
    "    \n",
    "    train_result, test_result = [], []\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, train_x.shape[1], train_x.shape[2]], name=\"x\")\n",
    "    y = tf.placeholder(tf.float32, [None, train_y.shape[1]], name=\"y\")\n",
    "    #keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")    \n",
    "    \n",
    "    \n",
    "    logits, feature_layer = mcnnu.all_sensor_net(x, y, name=\"all_sensor_net\")\n",
    "    print(\"Logit shape: \",logits.get_shape().as_list())\n",
    "    prediction = tf.nn.sigmoid(logits, name=\"prediction\")\n",
    "    correct_prediction = tf.equal(tf.greater(prediction, 0.5), tf.equal(y,1), name=\"correct_prediction\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y), name=\"loss_op\")    \n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_op, name=\"train_step\")\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    train_x, train_y = mclfu.adjust_for_batch_size(train_x, train_y, batch_size)   \n",
    "    \n",
    "    train_count = len(train_x)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        for ix in range(0, train_count, batch_size):                            \n",
    "            batch_x, batch_y = train_x[ix:ix+batch_size], train_y[ix:ix+batch_size]\n",
    "            sess.run(train_step, feed_dict={x:batch_x, y:batch_y})            \n",
    "        \n",
    "        #loss, acc = sess.run([loss_op, accuracy], feed_dict={x: train_x, y: train_y})        \n",
    "        #print('  Train loss: {:.4f}, acc: {:.4f}'.format(loss, acc))\n",
    "        \n",
    "        if len(test_x)>0:\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={x: test_x, y: test_y})                \n",
    "            print('  Test loss: {:.4f}, acc: {:.4f}'.format(loss, acc))\n",
    "        \n",
    "    print('!!!!!!!!!!!!!!! Optimization Finished !!!!!!!!!!!!!!!!!')\n",
    "    \n",
    "    if model_name:\n",
    "        saver = tf.train.Saver()            \n",
    "        mfileu.create_directory(model_path)\n",
    "        saver.save(sess, model_path+'/'+model_name)    \n",
    "        print(\"Model Saved!\")\n",
    "        \n",
    "    sess.close()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(exclude_subj):    \n",
    "    w, l =[], []\n",
    "    for subj in range(11):\n",
    "        if subj==exclude_subj: \n",
    "            continue\n",
    "            \n",
    "        d = mfileu.read_file('windows_free_cnn', 'subj_'+str(subj)+\".pkl\")\n",
    "        if len(w)==0:\n",
    "            w, l = d[\"windows\"], d[\"labels\"]\n",
    "        else:\n",
    "            w = np.concatenate((w, d[\"windows\"]), axis=0)\n",
    "            l = np.concatenate((l, d[\"labels\"]), axis=0)\n",
    "    \n",
    "    return w, l       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ Subject: 100 =============\n",
      "Shapes windows,labels, pos count: (2382348, 16, 9) (2382348, 1) 811001\n"
     ]
    }
   ],
   "source": [
    "subj, num_epochs = 100, 50\n",
    "if 'C:' not in mfileu.get_path():    \n",
    "    subj, num_epochs = int(sys.argv[1]), int(sys.argv[2])\n",
    "\n",
    "print(\"\\n============ Subject: {} =============\".format(subj))\n",
    "print(\"\\n============ Epochs: {}  =============\".format(num_epochs))\n",
    "\n",
    "windows, labels = get_train_data(subj)\n",
    "print(\"Shapes windows,labels, pos count:\", windows.shape, labels.shape, np.sum(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shapes:  (2144113, 16, 9) (2144113, 1) 729901\n",
      "val shapes:  (238235, 16, 9) (238235, 1) 81100\n"
     ]
    }
   ],
   "source": [
    "windows, labels = shuffle(windows, labels)\n",
    "train_x, val_x, train_y, val_y = train_test_split(windows, labels, test_size=0.1, stratify=labels)\n",
    "print(\"train shapes: \", train_x.shape, train_y.shape, np.sum(train_y))\n",
    "print(\"val shapes: \", val_x.shape, val_y.shape, np.sum(val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside all_sensor_net: x_shape, y_shape : [None, 16, 9] [None, 1]\n",
      "Inside one_3dsensor_conv_net:  one_3dsensor_conv_net_0 , x_shape [None, 16, 3]\n",
      "  Axis count:  3\n",
      "  Conv_1, maxpool_1 shape:  [None, 14, 1, 64] [None, 7, 1, 64]\n",
      "  Conv_2, maxpool_2 shape:  [None, 5, 1, 128] [None, 1, 1, 128]\n",
      "One 3d Sensor flattened shape:  0 [None, 128]\n",
      "Inside one_3dsensor_conv_net:  one_3dsensor_conv_net_1 , x_shape [None, 16, 3]\n",
      "  Axis count:  3\n",
      "  Conv_1, maxpool_1 shape:  [None, 14, 1, 64] [None, 7, 1, 64]\n",
      "  Conv_2, maxpool_2 shape:  [None, 5, 1, 128] [None, 1, 1, 128]\n",
      "One 3d Sensor flattened shape:  3 [None, 128]\n",
      "Inside one_3dsensor_conv_net:  one_3dsensor_conv_net_2 , x_shape [None, 16, 3]\n",
      "  Axis count:  3\n",
      "  Conv_1, maxpool_1 shape:  [None, 14, 1, 64] [None, 7, 1, 64]\n",
      "  Conv_2, maxpool_2 shape:  [None, 5, 1, 128] [None, 1, 1, 128]\n",
      "One 3d Sensor flattened shape:  6 [None, 128]\n",
      "All sensor list size: 3\n",
      "Combo shape all sensor net:  [None, 384]\n",
      "Relu activation\n",
      "Relu activation\n",
      "Tanh activation\n",
      "Logit shape:  [None, 1]\n",
      "Epoch: 0\n",
      "  Train loss: 0.3461, acc: 0.8337\n",
      "Epoch: 1\n",
      "  Train loss: 0.3103, acc: 0.8533\n",
      "!!!!!!!!!!!!!!! Optimization Finished !!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'bdu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-c8acbd082b89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keep_prob_val'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmfileu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_test_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/feature_models\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"subj_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-d5451d4b6361>\u001b[0m in \u001b[0;36mtrain_test_model\u001b[1;34m(train_x, train_y, test_x, test_y, params, model_path, model_name)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mbdu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model Saved!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bdu' is not defined"
     ]
    }
   ],
   "source": [
    "params={}\n",
    "params['learning_rate'] = 0.001\n",
    "params['num_epochs'] = num_epochs\n",
    "params['batch_size'] = 128\n",
    "params['keep_prob_val'] = 0.5\n",
    "path = mfileu.get_path()\n",
    "train_test_model(train_x, train_y, val_x, val_y, params, path+\"/feature_models\", \"subj_\"+str(subj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
