{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        print('Creating directory: ', path)\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(lines, offset, annots):\n",
    "    #print(annots)    \n",
    "    tokens = lines[0].split(\",\")\n",
    "    if len(tokens)==1:\n",
    "        lines = lines[1:]\n",
    "    \n",
    "    data = []\n",
    "    for line in lines:\n",
    "        tokens = line.split(\",\")\n",
    "        sensor_id = int(tokens[1])\n",
    "        if sensor_id in [1, 4, 9, 11]:\n",
    "            t = int(tokens[0])\n",
    "            x = float(tokens[3])\n",
    "            y = float(tokens[4])\n",
    "            z = float(tokens[5])\n",
    "            data.append([t, sensor_id, x, y, z])\n",
    "        \n",
    "    data = np.array(data)  \n",
    "    data[:, 0] = data[:, 0] - data[0, 0]\n",
    "    data[:, 0] = data[:, 0]/1e9\n",
    "        \n",
    "    if len(annots)==0:                               \n",
    "        res_annots = np.array([])        \n",
    "    else:\n",
    "        annots.sort(axis=0)\n",
    "        annots = annots[annots[:, 1]<1000, :]\n",
    "        annots[:, 0] = annots[:, 0] - offset\n",
    "                    \n",
    "    return data, annots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_folder = 'C:/ASM/DevData/eating/datasets'\n",
    "create_directory(dest_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_serial = 0\n",
    "all_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0102\n",
      "0103\n",
      "0301\n",
      "0601\n",
      "0602\n",
      "0603\n",
      "0801\n",
      "0803\n",
      "0901\n",
      "0902\n",
      "1001\n",
      "1303\n",
      "1304\n",
      "1305\n"
     ]
    }
   ],
   "source": [
    "#read data usc\n",
    "srcFolder = 'D:/Box Sync/MyData/Eating m2fed/usc_meals';\n",
    "subjects = [\n",
    "    ['0102', 94.6],\n",
    "    ['0103', 67.5],\n",
    "    ['0301', 59.8],\n",
    "    ['0601', 26.1],\n",
    "    ['0602', 16.0],\n",
    "    ['0603', 17.1],\n",
    "    ['0801', 82.0],\n",
    "    ['0803', 67.9],\n",
    "    ['0901', 63.9],\n",
    "    ['0902', 123.2],\n",
    "    ['1001', 89.5],    \n",
    "    ['1303', 123.8],\n",
    "    ['1304', 107.1],\n",
    "    ['1305', 135.5]]\n",
    "\n",
    "\n",
    "for subj in range(len(subjects)):\n",
    "    subject_id = subjects[subj][0]\n",
    "    print(subject_id)\n",
    "    file_path = srcFolder+'/sensor_data/'+subject_id+'_right.wada'\n",
    "    with open(file_path) as file:\n",
    "        raw_data = file.readlines()\n",
    "\n",
    "    file_path = srcFolder+'/annotations/processed/'+ subject_id+ '_annotations_right.csv'\n",
    "    annots = np.genfromtxt(file_path, delimiter=\",\")\n",
    "    \n",
    "    offset = subjects[subj][1]\n",
    "\n",
    "    data, annots = process_data(raw_data, offset, annots)\n",
    "    if subj ==0:             \n",
    "        data = data[data[:,0]<42*60, :]        \n",
    "    \n",
    "    #np.savetxt(dest_file_path+'data_'+str(file_serial), data, delimiter=\",\")\n",
    "    #np.savetxt(dest_file_path+'annots_'+str(file_serial), annots, delimiter=\",\")\n",
    "    \n",
    "    d = {'data':data, 'annots':annots, 'type':'usc'}\n",
    "    all_data.append([d])    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16.2, 7.8], [10.5, 18.3, 8.3, 23.3, 149.5], [9.5, 21.1], [7.9, 16.0], [8.0, 5.6, 4.6, 8.5], [8.1, 30.3, 3.4, 15.6, 5.5], [7.4]]\n",
      "Processing  1 1\n",
      "Processing  1 2\n",
      "Processing  2 1\n",
      "Processing  2 2\n",
      "Processing  2 3\n",
      "Processing  2 4\n",
      "Processing  3 1\n",
      "Processing  3 2\n",
      "Processing  4 1\n",
      "Processing  4 2\n",
      "Processing  5 1\n",
      "Processing  5 2\n",
      "Processing  5 3\n",
      "Processing  5 4\n",
      "Processing  6 1\n",
      "Processing  6 2\n",
      "Processing  6 3\n",
      "Processing  6 4\n",
      "Processing  6 5\n",
      "Processing  7 1\n"
     ]
    }
   ],
   "source": [
    "#read data uva\n",
    "session_counts = [2, 4, 2, 2, 4, 5, 1]\n",
    "srcFolder = 'D:/Box Sync/MyData/Eating m2fed/uva_meals'\n",
    "\n",
    "file_path = srcFolder +'/offsets.csv'\n",
    "all_offsets = []\n",
    "with open(file_path) as file:\n",
    "    lines = file.readlines()\n",
    "    for line in lines:       \n",
    "        tokens = line.split(\",\")\n",
    "        ofs = []\n",
    "        for t in tokens:\n",
    "            t = t.rstrip()            \n",
    "            if len(t)>0:\n",
    "                t= float(t)\n",
    "                ofs.append(t)\n",
    "        all_offsets.append(ofs)\n",
    "        \n",
    "print(all_offsets)\n",
    "\n",
    "for subj in range(7):\n",
    "    sess_count = session_counts[subj]\n",
    "    subject_data = []\n",
    "    for sess in range(sess_count):\n",
    "        print('Processing ', (subj+1), (sess+1))\n",
    "        file_path = srcFolder+'/subject_'+ str(subj+1)+ '/subject'+ str(subj+1)+ '_right_session'+ str(sess+1)+'.wada'\n",
    "        with open(file_path) as file:\n",
    "            raw_data = file.readlines()\n",
    "        \n",
    "        file_path = srcFolder + '/annotations/processed/subject'+ str(subj+1)+ '_annotations_right_session'+ str(sess+1) + '.csv'\n",
    "        annots = np.genfromtxt(file_path, delimiter=\",\")\n",
    "        \n",
    "        offset = all_offsets[subj][sess]\n",
    "        \n",
    "        data, annots = process_data(raw_data, offset, annots)\n",
    "        #np.savetxt(dest_file_path+'data_'+str(file_serial), data, delimiter=\",\")\n",
    "        #np.savetxt(dest_file_path+'annots_'+str(file_serial), annots, delimiter=\",\")\n",
    "        d = {'data':data, 'annots':annots, 'type':'uva'}\n",
    "        subject_data.append(d)\n",
    "        \n",
    "    all_data.append(subject_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dest_folder+'/our_lab_dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(all_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total subjects in lab:  21\n",
      "Subject 0: Session count 1\n",
      "Subject 1: Session count 1\n",
      "Subject 2: Session count 1\n",
      "Subject 3: Session count 1\n",
      "Subject 4: Session count 1\n",
      "Subject 5: Session count 1\n",
      "Subject 6: Session count 1\n",
      "Subject 7: Session count 1\n",
      "Subject 8: Session count 1\n",
      "Subject 9: Session count 1\n",
      "Subject 10: Session count 1\n",
      "Subject 11: Session count 1\n",
      "Subject 12: Session count 1\n",
      "Subject 13: Session count 1\n",
      "Subject 14: Session count 2\n",
      "Subject 15: Session count 4\n",
      "Subject 16: Session count 2\n",
      "Subject 17: Session count 2\n",
      "Subject 18: Session count 4\n",
      "Subject 19: Session count 5\n",
      "Subject 20: Session count 1\n"
     ]
    }
   ],
   "source": [
    "print(\"total subjects in lab: \", len(all_data))\n",
    "for s in range(len(all_data)):\n",
    "    print('Subject {}: Session count {}'.format(s, len(all_data[s])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/abu_home_noneat/14402D24F306CE8-m2fed-04-01-Right-2017-03-08-22-39-00.wada\n",
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/abu_home_noneat/14402D24F306CE8-m2fed-04-01-Right-2017-03-08-23-30-58.wada\n",
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/abu_home_noneat/14452D1EF8D6D9E-m2fed-04-01-Right-2013-03-29-06-08-27.wada\n",
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/emi_home_noneat/14422D2DF3B6AA2-m2fed-03-03-Right-2013-02-06-02-28-46.wada\n",
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/emi_home_noneat/14422D2DF3B6AA2-m2fed-03-03-Right-2013-02-07-06-05-48.wada\n",
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/emi_home_noneat/14422D2DF3B6AA2-m2fed-03-03-Right-2013-02-12-01-21-54.wada\n",
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/emi_home_noneat/14422D2DF3B6AA2-m2fed-03-03-Right-2013-02-12-05-46-34.wada\n",
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/sarah_home_noneat/14402D24F306CE8-m2fed-01-01-Right-2017-02-27-16-29-48.wada\n",
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/sarah_home_noneat/14402D24F306CE8-m2fed-01-01-Right-2017-02-27-19-42-25.wada\n",
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/sarah_home_noneat/14402D24F306CE8-m2fed-01-01-Right-2017-02-27-21-22-14.wada\n",
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/meiyi_home_noneat/14452D1EF8D6D9E-m2fed-02-01-Left-2013-03-20-21-55-01.wada\n",
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/meiyi_home_noneat/14452D1EF8D6D9E-m2fed-02-01-Left-2013-03-20-22-55-36.wada\n",
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/meiyi_home_noneat/14452D1EF8D6D9E-m2fed-02-01-Left-2013-03-20-23-55-59.wada\n",
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/meiyi_home_noneat/14452D1EF8D6D9E-m2fed-02-01-Right-2013-03-21-20-54-30.wada\n",
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/meiyi_home_noneat/14452D1EF8D6D9E-m2fed-02-01-Right-2013-03-21-21-54-43.wada\n",
      "D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/meiyi_home_noneat/14452D1EF8D6D9E-m2fed-02-01-Right-2013-03-21-23-00-43.wada\n"
     ]
    }
   ],
   "source": [
    "srcFolder = 'D:/Box Sync/MyData/Eating m2fed/uva_noneat_home/'\n",
    "subjects = ['abu','emi','sarah','meiyi']\n",
    "\n",
    "all_data = []\n",
    "for subj in subjects:    \n",
    "    folder_path = srcFolder + subj + '_home_noneat'\n",
    "    files = os.listdir(folder_path);\n",
    "    for f in files:\n",
    "        file_path = folder_path+ '/' + f\n",
    "        print(file_path)\n",
    "        with open(file_path) as file:\n",
    "            raw_data = file.readlines()\n",
    "        \n",
    "        annots = [];        \n",
    "        offset = 0;        \n",
    "       \n",
    "        data, annots = process_data(raw_data, offset, annots)\n",
    "        #np.savetxt(dest_file_path+'accel_'+str(file_serial), accel, delimiter=\",\")\n",
    "        d = {'data':data, 'annots':annots, 'subject_name':subj}\n",
    "        all_data.append(d)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dest_folder+'/our_free_dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(all_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
