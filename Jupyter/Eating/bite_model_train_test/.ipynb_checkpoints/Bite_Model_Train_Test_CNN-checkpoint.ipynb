{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_path = 'C:/ASM/Dropbox/Developments/Jupyter/Eating/myutils' if 'C:' in os.getcwd() else './myutils'\n",
    "sys.path.append(util_path)\n",
    "import my_file_utils as mfileu\n",
    "import my_classification_utils as mclfu\n",
    "import my_tensorflow_cnn_utils as mcnnu\n",
    "import my_tensorflow_lstm_utils as mlstmu\n",
    "import my_tensorflow_dense_utils as mdenseu\n",
    "#importlib.reload(biteu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_lab = mfileu.read_file('data', 'lab_data_steven_right_smoothed_accel_gyro_normalized.pkl')\n",
    "ssil_lab = mfileu.read_file('peaks', 'lab_ssil_steven_right.pkl')\n",
    "ds_free = mfileu.read_file('data', 'free_data_steven_right_smoothed_accel_gyro_normalized.pkl')\n",
    "ssil_free = mfileu.read_file('peaks', 'free_ssil_steven_right.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [ds_lab, ds_free]\n",
    "\n",
    "count = len(ssil_lab)\n",
    "ds_index = np.zeros((count, 1), dtype=np.int32)\n",
    "dssil_lab =np.concatenate((ds_index, ssil_lab), axis=1).astype(int)\n",
    "print(\"Lab ssil, dssil: \", ssil_lab.shape, dssil_lab.shape, np.sum(dssil_lab[:,0]))\n",
    "l = dssil_lab[:, -1]\n",
    "print(\"Lab label summary:\",  np.sum(l==0), np.sum(l==1), np.sum(l==2), np.sum(l==3))\n",
    "print()\n",
    "\n",
    "count = len(ssil_free)\n",
    "ds_index = np.ones((count, 1), dtype=np.int32)\n",
    "dssil_free =np.concatenate((ds_index, ssil_free), axis=1).astype(int)\n",
    "print(\"Free ssil, dssil: \", ssil_free.shape, dssil_free.shape, np.sum(dssil_free[:,0]))\n",
    "l = dssil_free[:, -1]\n",
    "print(\"Free label summary:\",  np.sum(l==0), np.sum(l==1), np.sum(l==2), np.sum(l==3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_count, label_shape_1 = 6, 1 \n",
    "win_size = 10*16\n",
    "print(\"Win size:\", win_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_data(ds, dssil, win_size):\n",
    "    count = len(dssil)\n",
    "    w = np.zeros((count, win_size, axis_count))\n",
    "    features = []\n",
    "    \n",
    "    half_win_size = win_size//2\n",
    "    for i in range(count):\n",
    "        d, subj, sess, ix = dssil[i, 0], dssil[i, 1], dssil[i, 2], dssil[i, 3]\n",
    "        w[i, :, :] = ds[d][subj][sess][ix-half_win_size:ix+half_win_size, 1:7]        \n",
    "    \n",
    "    return w, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj, num_epochs, train_test = 0, 50, 'train'\n",
    "if 'C:' not in mfileu.get_path():    \n",
    "    subj, num_epochs, train_test = int(sys.argv[1]), int(sys.argv[2]), sys.argv[3]\n",
    "    \n",
    "assert train_test in ['train', 'test_lab', 'test_free']\n",
    "params={}\n",
    "params['learning_rate'] = 0.001\n",
    "params['num_epochs'] = num_epochs\n",
    "params['batch_size'] = 128\n",
    "params['keep_prob_val'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(ds, train_indices, test_indices, params, model_path_dest=None, model_path_src=None):\n",
    "    learning_rate = params['learning_rate']\n",
    "    num_epochs = params['num_epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    keep_prob_val = params['keep_prob_val']\n",
    "    print(\"****** Learning rate \", learning_rate)\n",
    "    \n",
    "    #print_out, sys.stdout = sys.stdout, open(os.devnull, 'w')    \n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, win_size, axis_count], name=\"x\")    \n",
    "    #features = tf.placeholder(tf.float32, [None, feature_count], name=\"features\")\n",
    "    y = tf.placeholder(tf.float32, [None, label_shape_1], name=\"y\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")    \n",
    "    \n",
    "    cnn_out = mcnnu.all_sensor_net(x, name=\"all_sensor_net\")    \n",
    "    lstm_out_fw, lstm_out_bw = mlstmu.multi_layer_biLSTM(cnn_out, batch_size=batch_size, n_hidden=128, n_layer=1)\n",
    "    lstm_out = tf.concat([lstm_out_fw[:, -1, :], lstm_out_bw[:, 0, :]], axis =1)\n",
    "    \n",
    "    print(\"Lstm out shapes(fw, bw): \", lstm_out_fw.get_shape().as_list(), lstm_out_bw.get_shape().as_list())\n",
    "    print(\"FW LSTM out shape:\", lstm_out_fw[:, -1, :].get_shape().as_list())\n",
    "    print(\"BW LSTM out shape:\", lstm_out_bw[:, -1, :].get_shape().as_list())    \n",
    "    print(\"Lstm out shape final: \", lstm_out.get_shape().as_list())\n",
    "    \n",
    "    drop_layer = tf.nn.dropout(lstm_out, keep_prob=keep_prob, name=\"dropout1\")\n",
    "    #fc_layer= mdenseu.fc_layer(drop_layer1, 64, name=\"Dense_1\", activation='relu')    \n",
    "    #drop_layer = tf.nn.dropout(fc_layer, keep_prob=keep_prob, name=\"dropout2\")\n",
    "    print(\"Drop layer shape: \",drop_layer.get_shape().as_list())\n",
    "    logits = mdenseu.fc_layer(drop_layer, 1, name=\"Logits\")    \n",
    "    \n",
    "    print(\"Logit shape: \",logits.get_shape().as_list())\n",
    "    prediction = tf.nn.sigmoid(logits, name=\"prediction\")\n",
    "    correct_prediction = tf.equal(tf.greater(prediction, 0.5), tf.equal(y,1), name=\"correct_prediction\")\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "    loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y), name=\"loss_op\")        \n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_op, name=\"train_step\")\n",
    "\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    #sys.stdout = print_out\n",
    "    ########## Train and then save the model ########################\n",
    "    if len(train_indices)>0:                \n",
    "        sess.run(tf.global_variables_initializer())    \n",
    "            \n",
    "        train_indices, _ = mclfu.adjust_for_batch_size(train_indices, train_indices, batch_size)\n",
    "\n",
    "        train_count = len(train_indices)\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch:\", epoch)\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for ix in range(0, train_count, batch_size):                                            \n",
    "                batch_x, batch_features = get_window_data(ds, train_indices[ix:ix+batch_size], win_size)                 \n",
    "                batch_y = train_indices[ix:ix+batch_size, -1].reshape((-1,1))                 \n",
    "                sess.run(train_step, feed_dict={x:batch_x, y:batch_y, keep_prob:keep_prob_val})                \n",
    "\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={x:batch_x, y:batch_y, keep_prob:keep_prob_val})        \n",
    "                total_loss+= loss*batch_size\n",
    "                total_acc += acc*batch_size                \n",
    "            print('  Train loss: {:.4f}, acc: {:.4f}'.format(total_loss/train_count, total_acc/train_count))\n",
    "\n",
    "            test_count = len(test_indices)\n",
    "            if test_count>0:            \n",
    "                test_indices, _ = mclfu.adjust_for_batch_size(test_indices, test_indices, batch_size)\n",
    "                total_loss, total_acc = 0, 0\n",
    "                for ix in range(0, test_count, batch_size):                \n",
    "                    batch_x, batch_features = get_window_data(ds, test_indices[ix:ix+batch_size], win_size)\n",
    "                    batch_y = test_indices[ix:ix+batch_size, -1].reshape((-1,1))                      \n",
    "                    loss, acc = sess.run([loss_op, accuracy], feed_dict={x:batch_x, y:batch_y, keep_prob:1.0})                \n",
    "                    total_loss+= loss*batch_size\n",
    "                    total_acc += acc*batch_size                \n",
    "                print('  Test loss: {:.4f}, acc: {:.4f}'.format(total_loss/test_count, total_acc/test_count))\n",
    "\n",
    "        print('!!!!!!!!!!!!!!! Optimization Finished !!!!!!!!!!!!!!!!!')\n",
    "\n",
    "        if model_path_dest:\n",
    "            saver = tf.train.Saver()            \n",
    "            mfileu.create_directory(model_path_dest)\n",
    "            saver.save(sess, model_path_dest+'/model')    \n",
    "            print(\"Model Saved!\")\n",
    "        sess.close()\n",
    "        \n",
    "    ########## Restore the model and then Test  ########################\n",
    "    else:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path_src+'/model')\n",
    "        print(\"Model Loaded for test!\")\n",
    "        \n",
    "        test_count_original = len(test_indices)        \n",
    "        test_indices, _ = mclfu.adjust_for_batch_size(test_indices, test_indices, batch_size)\n",
    "        test_count = len(test_indices)\n",
    "        res = np.zeros((test_count, 1))\n",
    "        \n",
    "        for ix in range(0, test_count, batch_size):                \n",
    "            batch_x, batch_features = get_window_data(ds, test_indices[ix:ix+batch_size], win_size)\n",
    "            batch_y = test_indices[ix:ix+batch_size, -1].reshape((-1,1))  \n",
    "            pred = sess.run([prediction], feed_dict={x: batch_x, y:batch_y, keep_prob:1.0})            \n",
    "            res[ix:ix+batch_size, 0] = np.array(pred).reshape((-1, ))\n",
    "        \n",
    "        res = res[:test_count_original, :]        \n",
    "        sess.close()\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============================  Train/Test: {} =============================\".format(train_test))\n",
    "print(\"\\n============ Subject, Epochs, Win Size: {}, {}, {} =============\".format(subj, num_epochs, win_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = mfileu.get_path()\n",
    "model_folder_src = path+'/free_models_our/net_1'\n",
    "model_folder_dest = model_folder_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dssil(subj, dssil_lab, dssil_free):\n",
    "    \n",
    "    ########### Lab ###########\n",
    "    if subj<5:\n",
    "        cond = (dssil_lab[:,1]!=subj+2)\n",
    "        dssil_lab = dssil_lab[cond, :]\n",
    "    elif subj>=200:\n",
    "        cond = (dssil_lab[:,1]!=subj-200)\n",
    "        dssil_lab = dssil_lab[cond, :]\n",
    "    \n",
    "    print(\"Lab dssil: \", dssil_lab.shape, np.sum(dssil_lab[:, 0]))\n",
    "    l = dssil_lab[:, -1]\n",
    "    print(\"Lab label summary:\",  np.sum(l==0), np.sum(l==1), np.sum(l==2), np.sum(l==3))\n",
    "    dssil_lab = dssil_lab[(l==1) |(l==2), :]\n",
    "    dssil_lab[:, -1] = 1    \n",
    "    l = dssil_lab[:, -1]\n",
    "    print(\"Lab label summary:\",  np.sum(l==0), np.sum(l==1), np.sum(l==2), np.sum(l==3))\n",
    "    \n",
    "    ############ Free #############\n",
    "    if subj<100:\n",
    "        cond = (dssil_free[:,1]!=subj)\n",
    "        dssil_free = dssil_free[cond, :]\n",
    "    \n",
    "    print(\"\\nFree dssil: \", dssil_free.shape, np.sum(dssil_free[:,0]))\n",
    "    l = dssil_free[:, -1]\n",
    "    print(\"Free label summary:\",  np.sum(l==0), np.sum(l==1), np.sum(l==2), np.sum(l==3))\n",
    "    #dssil_free[l==2, -1] = 1\n",
    "    #dssil_free[l==3, -1] = 0    \n",
    "    l = dssil_free[:, -1]\n",
    "    dssil_free = dssil_free[(l==0), :] #taking only negatives from free\n",
    "    l = dssil_free[:, -1]\n",
    "    print(\"Free label summary:\",  np.sum(l==0), np.sum(l==1), np.sum(l==2), np.sum(l==3))\n",
    "\n",
    "    ############ Combo #############\n",
    "    dssil = np.concatenate((dssil_lab, dssil_free))\n",
    "    print(\"\\nCombo dssil: \", dssil.shape, np.sum(dssil[:,0]==0), np.sum(dssil[:,0]==1))    \n",
    "    l = dssil[:, -1]\n",
    "    print(\"Combo label summary:\",  np.sum(l==0), np.sum(l==1), np.sum(l==2), np.sum(l==3))\n",
    "    \n",
    "    return dssil\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if train_test == 'train':\n",
    "    print(\"Training.....\")\n",
    "    \n",
    "    dssil = get_train_dssil(subj, dssil_lab, dssil_free)\n",
    "    \n",
    "    assert np.sum(dssil[:, -1]>1) == 0\n",
    "    assert np.sum(dssil[:, -1]<0) == 0    \n",
    "        \n",
    "    dssil = shuffle(dssil)    \n",
    "    train_indices, val_indices = train_test_split(dssil, test_size=0.1, stratify=dssil[:, -1])\n",
    "    \n",
    "    print(\"\\nTrain, Val shapes: \", train_indices.shape, val_indices.shape, np.sum(train_indices[:, -1]), np.sum(val_indices[:, -1]))\n",
    "    \n",
    "    model_path_dest = model_folder_dest+\"/subj_\"+str(subj)\n",
    "    train_test_model(ds, train_indices, val_indices, params, model_path_dest=model_path_dest)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test = 'test'\n",
    "if train_test=='test_free':       \n",
    "    print(\"Testing Free .....\")    \n",
    "    ba = mfileu.read_file('data', 'free_data_steven_blank_array.pkl')\n",
    "    \n",
    "    print(ssil_free.shape, dssil_free.shape)\n",
    "    for subj in range(len(ds_free)):\n",
    "        cond = (dssil_free[:, 1]==subj)\n",
    "        dssil = dssil_free[cond, :]\n",
    "        assert np.sum(dssil[:, 1]!=subj) == 0\n",
    "\n",
    "        l = dssil[:, -1]\n",
    "        print(\"\\n\\nSubj, Label summary:\", subj, dssil.shape, np.sum(l==0), np.sum(l==1), np.sum(l==2), np.sum(l==3))\n",
    "\n",
    "        model_path_src = model_folder_src+\"/subj_\"+str(subj)            \n",
    "        proba = train_test_model(ds, [], dssil, params, model_path_src=model_path_src)            \n",
    "        print(\"\\n\\nPrediction shape, sum, >=0.5: \", proba.shape, np.sum(proba), np.sum(proba>=0.5))            \n",
    "        assert len(proba)==len(dssil)\n",
    "        \n",
    "        for sess in range(len(ds_free[subj])):\n",
    "            cond = (dssil[:, 2]==sess)\n",
    "            ssil = dssil[cond, 1:]\n",
    "            p = proba[cond, :]\n",
    "            assert np.sum(ssil[:, 0]!=subj) == 0\n",
    "            assert np.sum(ssil[:, 1]!=sess) == 0\n",
    "            \n",
    "            print(\"Subj, sess: \", subj, sess, len(ssil))\n",
    "            \n",
    "            ba[subj][sess] = np.concatenate((ssil, p), axis=1)\n",
    "            print(\"ssilp shape: \", ba[subj][sess].shape)\n",
    "\n",
    "    mfileu.write_file('peak_ssilp', 'free_ssilp_our_net_1.pkl', ba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
