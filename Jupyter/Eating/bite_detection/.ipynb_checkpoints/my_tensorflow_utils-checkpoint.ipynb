{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import my_classification_utils as mcu\n",
    "import numpy as np\n",
    "import my_bite_detection_utils as bdu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(x, size_in, size_out, ksize, strides, padding, name):    \n",
    "    strides = [1, strides[0], strides[1], 1]   \n",
    "    with tf.name_scope(name):\n",
    "        W = tf.Variable(tf.truncated_normal([ksize[0], ksize[1], size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"b\")\n",
    "        conv = tf.nn.conv2d(x, W, strides=strides, padding=padding)\n",
    "        output = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"weights\", W)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"outputs\", output)\n",
    "        return output\n",
    "    \n",
    "def maxpool_layer(x, ksize, strides, padding, name):\n",
    "    ksize = [1, ksize[0], ksize[1], 1]\n",
    "    strides = [1, strides[0], strides[1], 1]\n",
    "    with tf.name_scope(name):\n",
    "        output = tf.nn.max_pool(x, ksize=ksize, strides=strides, padding=padding)\n",
    "        tf.summary.histogram(\"outputs\", output)\n",
    "        return output\n",
    "    \n",
    "def fc_layer(x, size_in, size_out, name, relu=False):\n",
    "    with tf.name_scope(name):\n",
    "        W = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"b\")\n",
    "        output = tf.matmul(x, W) + b\n",
    "        if relu:\n",
    "            output = tf.nn.relu(output)\n",
    "            \n",
    "        tf.summary.histogram('weights', W)\n",
    "        tf.summary.histogram('biases', b)\n",
    "        tf.summary.histogram('outputs', output)\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_axis_conv_net(x, name):\n",
    "    x_shape =x.get_shape().as_list()\n",
    "    print('Inside one_axis_net: ',name,', x_shape', x_shape)\n",
    "    \n",
    "    axis_count = x_shape[-1]\n",
    "    print(\"  Axis count: \", axis_count)\n",
    "    with tf.name_scope(name):\n",
    "        x = tf.reshape(x, shape=[-1, x.shape[1], x.shape[2], 1], name=\"reshape\")\n",
    "        conv_1 = conv_layer(x, size_in=1, size_out=16, ksize=[5,axis_count], strides=[1,axis_count], padding=\"VALID\", name='conv_1')        \n",
    "        maxpool_1 = maxpool_layer(conv_1, ksize=[2,1], strides=[2,1], padding=\"VALID\", name=\"maxpool_1\")\n",
    "        print(\"  Conv_1, maxpool_1 shape: \", conv_1.get_shape().as_list(), maxpool_1.get_shape().as_list())\n",
    "\n",
    "        conv_2 = conv_layer(maxpool_1, size_in=16, size_out=32, ksize=[5,1], strides=[1,1], padding=\"VALID\", name='conv_2')\n",
    "        maxpool_2 = maxpool_layer(conv_2, ksize=[2,1], strides=[2,1], padding=\"VALID\", name=\"maxpool_2\")\n",
    "        print(\"  Conv_2, maxpool_2 shape: \", conv_2.get_shape().as_list(), maxpool_2.get_shape().as_list())\n",
    "        \n",
    "        conv_3 = conv_layer(maxpool_2, size_in=32, size_out=64, ksize=[5,1], strides=[1,1], padding=\"VALID\", name='conv_3')\n",
    "        maxpool_3 = maxpool_layer(conv_3, ksize=[2,1], strides=[2,1], padding=\"VALID\", name=\"maxpool_3\")\n",
    "        print(\"  Conv_3, maxpool_3 shape: \", conv_3.get_shape().as_list(), maxpool_3.get_shape().as_list())\n",
    "        \n",
    "        sz = maxpool_3.get_shape().as_list()\n",
    "        flattened = tf.reshape(maxpool_3, shape=[-1, sz[1]*sz[2]*sz[3]], name=\"Flattened\")\n",
    "        return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_axes_net(x, y, keep_prob, name):\n",
    "    x_shape = x.get_shape().as_list()\n",
    "    y_shape = y.get_shape().as_list()\n",
    "    print('Inside all_axes_net: x_shape, y_shape :', x_shape, y_shape)\n",
    "    print(type(x_shape))\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "        all_axes = list(range(x_shape[-1]))\n",
    "        for i in range(x_shape[-1]):\n",
    "            axis_data = x[:, :, i]\n",
    "            axis_data = tf.reshape(axis_data, shape=[-1, x.shape[1], 1], name=\"reshape\")\n",
    "            #axis_data = tf.slice(x, [0,0,i], [-1, -1, 1], name=\"slice_\"+str(i))\n",
    "            all_axes[i] = one_axis_conv_net(axis_data, name=\"conv_axis_\"+str(i))\n",
    "            print(\"Shape \", i, all_axes[i].get_shape().as_list())\n",
    "            \n",
    "        \n",
    "        for i in range(0, x_shape[-1], 3):\n",
    "            axis_data = x[:, :, i:i+3]            \n",
    "            a = one_axis_conv_net(axis_data, name=\"conv_3_axes_\"+str(i))\n",
    "            all_axes.append(a)\n",
    "            print(\"Shape \", i, all_axes[-1].get_shape().as_list())\n",
    "        \n",
    "        print(\"All axis list size:\", len(all_axes))        \n",
    "        combo_flattened = tf.concat(all_axes, axis=1, name='concat_axis')\n",
    "        combo_shape = combo_flattened.get_shape().as_list()\n",
    "        print(\"Combo shape one net: \", combo_shape)\n",
    "\n",
    "        fc_1 = fc_layer(combo_flattened, combo_shape[-1], 512, name='FC_1', relu=True)\n",
    "        fcdrop_1 = tf.nn.dropout(fc_1, keep_prob, name='fcdrop_1')    \n",
    "        fc_2 = fc_layer(fcdrop_1, 512, 512, name='FC_2', relu=True)\n",
    "        fcdrop_2 = tf.nn.dropout(fc_2, keep_prob, name='fcdrop_2') \n",
    "        #fc_3 = fc_layer(fcdrop_2, 512, 512, name='FC_3', relu=True)\n",
    "        #fcdrop_3 = tf.nn.dropout(fc_3, keep_prob, name='fcdrop_3') \n",
    "        logits = fc_layer(fcdrop_2, 512, y_shape[-1], name='FC_Final')\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combo_net(xleft, xmid, xright, y, keep_prob):\n",
    "    xmid_shape = xmid.get_shape().as_list()\n",
    "    xleft_shape = xleft.get_shape().as_list()\n",
    "    xright_shape = xright.get_shape().as_list()\n",
    "    y_shape = y.get_shape().as_list()\n",
    "    print('Inside combo net(Shapes): xmid, xmid, xright, y :', xmid_shape, xleft_shape, xright_shape, y_shape)\n",
    "    \n",
    "    fc_left = all_axes_net(xleft, y, keep_prob, name=\"left\")\n",
    "    fc_mid = all_axes_net(xmid, y, keep_prob, name=\"left\")\n",
    "    fc_right = all_axes_net(xright, y, keep_prob, name=\"left\")\n",
    "    \n",
    "    combo_flattened = tf.concat([fc_left, fc_mid, fc_right], axis=1, name='concat_combo')\n",
    "    combo_shape = combo_flattened.get_shape().as_list()\n",
    "    print(\"Combo shape all net: \", combo_shape)\n",
    "    \n",
    "    fc_1 = fc_layer(combo_flattened, combo_shape[-1], 512, name='FC_1_combo', relu=True)\n",
    "    fcdrop_1 = tf.nn.dropout(fc_1, keep_prob, name='fcdrop_1_combo')    \n",
    "    #fc_2 = fc_layer(fcdrop_1, 512, 256, name='FC_2_combo', relu=True)\n",
    "    #fcdrop_2 = tf.nn.dropout(fc_2, keep_prob, name='fcdrop_2_combo')\n",
    "    logits = fc_layer(fcdrop_1, 512, y_shape[-1], name='FC_Final_combo')\n",
    "    \n",
    "    return logits    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_for_batch_size(train_x, train_y, batch_size):\n",
    "    train_count = len(train_x)    \n",
    "    last_batch_size = train_count%batch_size\n",
    "    if last_batch_size >0:\n",
    "        last_batch_gap = batch_size - last_batch_size        \n",
    "        train_x = np.concatenate((train_x, train_x[0:last_batch_gap]), axis=0)\n",
    "        train_y = np.concatenate((train_y, train_y[0:last_batch_gap]), axis=0)\n",
    "        \n",
    "    train_count = len(train_x)    \n",
    "    assert train_count%batch_size == 0\n",
    "    return train_x, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(train_x, train_y, test_x, test_y, folder_path, params, save_model=True):\n",
    "    learning_rate = params['learning_rate']\n",
    "    num_epochs = params['num_epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    keep_prob_val = params['keep_prob_val']\n",
    "    \n",
    "    train_result, test_result = [], []\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, train_x.shape[1], train_x.shape[2]], name=\"x\")\n",
    "    y = tf.placeholder(tf.float32, [None, train_y.shape[1]], name=\"y\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")    \n",
    "    \n",
    "    #a = train_x.shape[1]//3\n",
    "    #b = 2*a\n",
    "    #xleft = x[:, 0:a, :]\n",
    "    #xmid = x[:, a:b, :]\n",
    "    #xright = x[:, b:, :]    \n",
    "    #print(\"Window shapes: \", xleft.shape, xmid.shape, xright.shape)    \n",
    "    #logits = mtu.combo_net(xleft, xmid, xright, y, keep_prob)\n",
    "    \n",
    "    logits = all_axes_net(x, y, keep_prob, name=\"all_axes_net\")\n",
    "    print(\"Logit shape: \",logits.get_shape().as_list())\n",
    "    prediction = tf.nn.sigmoid(logits, name=\"prediction\")\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1), name=\"correct_prediction\")\n",
    "    \n",
    "\n",
    "    loss_op = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y), name=\"loss_op\")\n",
    "    #tf.summary.scalar(\"loss_op_summary\", loss_op)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss_op, name=\"train_step\")\n",
    "\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"accuracy\")    \n",
    "    #tf.summary.scalar(\"accuracy_summary\", accuracy)\n",
    "\n",
    "    #summ = tf.summary.merge_all()    \n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #LOGDIR = folder_path +\"/tensorboard\"\n",
    "    #bdu.create_directory(LOGDIR)\n",
    "    #writer = tf.summary.FileWriter(LOGDIR)\n",
    "    #writer.add_graph(sess.graph)\n",
    "\n",
    "    train_x, train_y = adjust_for_batch_size(train_x, train_y, batch_size)   \n",
    "    \n",
    "    train_count = len(train_x)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch:\", epoch)\n",
    "        for ix in range(0, train_count, batch_size):                            \n",
    "            batch_x, batch_y = train_x[ix:ix+batch_size], train_y[ix:ix+batch_size]\n",
    "            sess.run(train_step, feed_dict={x:batch_x, y:batch_y, keep_prob:keep_prob_val})            \n",
    "            \n",
    "        #loss, acc, s = sess.run([loss_op, accuracy, summ], feed_dict={x: train_x, y: train_y, keep_prob:1.0})                \n",
    "        #writer.add_summary(s, epoch)\n",
    "        \n",
    "        loss, acc = sess.run([loss_op, accuracy], feed_dict={x: train_x, y: train_y, keep_prob:1.0})        \n",
    "        pred = sess.run(prediction, feed_dict={x:train_x, y: train_y, keep_prob:1.0})        \n",
    "        tn, tp, fn, fp, pr, rc, sp, f1 = mcu.get_metrics(pred, train_y)        \n",
    "        train_result.append([epoch, loss, acc, tn, tp, fn, fp, pr, rc, sp, f1])\n",
    "        print('  Train loss: {:.4f}, acc: {:.4f}, pr: {:.4f}, rc: {:.4f}, sp: {:.4f}, f1: {:.4f}'.format(loss, acc, pr, rc, sp, f1))\n",
    "        print('                                   tn: {:.0f}, tp: {:.0f}, fn: {:.0f}, fp: {:.0f}'.format(tn, tp, fn, fp))\n",
    "        \n",
    "        \n",
    "        loss, acc = sess.run([loss_op, accuracy], feed_dict={x: test_x, y: test_y, keep_prob:1.0})                \n",
    "        pred = sess.run(prediction, feed_dict={x:test_x, y: test_y, keep_prob:1.0})        \n",
    "        tn, tp, fn, fp, pr, rc, sp, f1 = mcu.get_metrics(pred, test_y)        \n",
    "        test_result.append([epoch, loss, acc, tn, tp, fn, fp, pr, rc, sp, f1])\n",
    "        print('  Test  loss: {:.4f}, acc: {:.4f}, pr: {:.4f}, rc: {:.4f}, sp: {:.4f}, f1: {:.4f}'.format(loss, acc, pr, rc, sp, f1))        \n",
    "        print('                                   tn: {:.0f}, tp: {:.0f}, fn: {:.0f}, fp: {:.0f}'.format(tn, tp, fn, fp))\n",
    "        \n",
    "    \n",
    "    print('Optimization Finished!')\n",
    "    \n",
    "    if save_model:\n",
    "        saver = tf.train.Saver()    \n",
    "        path = folder_path +\"/model\"\n",
    "        bdu.create_directory(path)\n",
    "        saver.save(sess, path+'/model')    \n",
    "    \n",
    "    test_pred = sess.run(prediction, feed_dict={x:test_x, y: test_y, keep_prob:1.0})    \n",
    "    sess.close()\n",
    "        \n",
    "    return test_pred, train_result, test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
