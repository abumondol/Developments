{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import my_bite_detection_utils as bdu\n",
    "import my_classification_utils as mcu\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import my_tensorflow_utils as mtu\n",
    "import importlib\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'my_classification_utils' from 'C:\\\\ASM\\\\Dropbox\\\\Developments\\\\Jupyter\\\\Eating\\\\bite_detection\\\\my_classification_utils.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(mtu)\n",
    "importlib.reload(bdu)\n",
    "importlib.reload(mcu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path:  C:/ASM/DevData/eating\n",
      "Creating windows and labels ... x_th: 0, min_bite_interval: 32, window_size: 96\n",
      "(52477, 96, 6) (52477, 4) [47886  4591]\n"
     ]
    }
   ],
   "source": [
    "root_path ='C:/ASM/DevData/eating' if \"C:\" in os.getcwd() else \".\"\n",
    "print(\"Root path: \", root_path)\n",
    "\n",
    "params = {\"x_th\": -0, \"min_bite_interval\": 2*16, \"window_size\": 6*16}\n",
    "windows, ssml, labels, features = bdu.get_windows_lab(params)\n",
    "labels[:, 1] = labels[:, 1] + labels[:, 2]\n",
    "labels = labels[:, :2]\n",
    "print(windows.shape, ssml.shape, np.sum(labels, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Parameters\n",
    "net_params={}\n",
    "net_params['learning_rate'] = 0.001\n",
    "net_params['num_epochs'] = 50\n",
    "net_params['batch_size'] = 128\n",
    "net_params['keep_prob_val'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13639, 96, 6) (13639, 4) [9719 3046  874]\n",
      "******** Exclude Subject 1 *********\n",
      "{'x_th': -0.4, 'min_bite_interval': 32, 'window_size': 96, 'var_th': 1.0}\n",
      "Train Test shapes:  (12994, 96, 6) (645, 96, 6)\n",
      "Train Test labels: [9110 3026  858] [609  20  16]\n",
      "Inside all_axes_net: x_shape, y_shape : [None, 96, 6] [None, 3]\n",
      "<class 'list'>\n",
      "Inside one_axis_net:  conv_axis_0 , x_shape [None, 96, 1]\n",
      "  Axis count:  1\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  0 [None, 512]\n",
      "Inside one_axis_net:  conv_axis_1 , x_shape [None, 96, 1]\n",
      "  Axis count:  1\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  1 [None, 512]\n",
      "Inside one_axis_net:  conv_axis_2 , x_shape [None, 96, 1]\n",
      "  Axis count:  1\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  2 [None, 512]\n",
      "Inside one_axis_net:  conv_axis_3 , x_shape [None, 96, 1]\n",
      "  Axis count:  1\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  3 [None, 512]\n",
      "Inside one_axis_net:  conv_axis_4 , x_shape [None, 96, 1]\n",
      "  Axis count:  1\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  4 [None, 512]\n",
      "Inside one_axis_net:  conv_axis_5 , x_shape [None, 96, 1]\n",
      "  Axis count:  1\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  5 [None, 512]\n",
      "Inside one_axis_net:  conv_3_axes_0 , x_shape [None, 96, 3]\n",
      "  Axis count:  3\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  0 [None, 512]\n",
      "Inside one_axis_net:  conv_3_axes_3 , x_shape [None, 96, 3]\n",
      "  Axis count:  3\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  3 [None, 512]\n",
      "All axis list size: 8\n",
      "Combo shape one net:  [None, 4096]\n",
      "Logit shape:  [None, 3]\n",
      "Epoch: 0\n",
      "  Train loss: 1.2548, acc: 0.2325, pr: 0.2983, rc: 1.0000, sp: 0.0000, f1: 0.4595\n",
      "                                   tn: 0, tp: 3894, fn: 0, fp: 9162\n",
      "  Test  loss: 1.4443, acc: 0.0310, pr: 0.0558, rc: 1.0000, sp: 0.0000, f1: 0.1057\n",
      "                                   tn: 0, tp: 36, fn: 0, fp: 609\n",
      "Epoch: 1\n",
      "  Train loss: 1.2025, acc: 0.2559, pr: 0.3007, rc: 0.9997, sp: 0.0117, f1: 0.4623\n",
      "                                   tn: 107, tp: 3893, fn: 1, fp: 9055\n",
      "  Test  loss: 1.4080, acc: 0.0403, pr: 0.0559, rc: 1.0000, sp: 0.0016, f1: 0.1059\n",
      "                                   tn: 1, tp: 36, fn: 0, fp: 608\n",
      "Epoch: 2\n",
      "  Train loss: 1.3503, acc: 0.2353, pr: 0.2983, rc: 1.0000, sp: 0.0000, f1: 0.4595\n",
      "                                   tn: 0, tp: 3894, fn: 0, fp: 9162\n",
      "  Test  loss: 1.6050, acc: 0.0310, pr: 0.0558, rc: 1.0000, sp: 0.0000, f1: 0.1057\n",
      "                                   tn: 0, tp: 36, fn: 0, fp: 609\n",
      "Epoch: 3\n",
      "  Train loss: 1.2303, acc: 0.2521, pr: 0.2991, rc: 1.0000, sp: 0.0043, f1: 0.4605\n",
      "                                   tn: 39, tp: 3894, fn: 0, fp: 9123\n",
      "  Test  loss: 1.4421, acc: 0.0372, pr: 0.0559, rc: 1.0000, sp: 0.0016, f1: 0.1059\n",
      "                                   tn: 1, tp: 36, fn: 0, fp: 608\n",
      "Epoch: 4\n",
      "  Train loss: 1.3539, acc: 0.2784, pr: 0.3039, rc: 0.9977, sp: 0.0289, f1: 0.4659\n",
      "                                   tn: 265, tp: 3885, fn: 9, fp: 8897\n",
      "  Test  loss: 1.6412, acc: 0.0853, pr: 0.0574, rc: 1.0000, sp: 0.0296, f1: 0.1086\n",
      "                                   tn: 18, tp: 36, fn: 0, fp: 591\n",
      "Epoch: 5\n",
      "  Train loss: 1.2569, acc: 0.3000, pr: 0.3078, rc: 0.9946, sp: 0.0494, f1: 0.4701\n",
      "                                   tn: 453, tp: 3873, fn: 21, fp: 8709\n",
      "  Test  loss: 1.5196, acc: 0.0822, pr: 0.0571, rc: 1.0000, sp: 0.0230, f1: 0.1079\n",
      "                                   tn: 14, tp: 36, fn: 0, fp: 595\n",
      "Epoch: 6\n",
      "  Train loss: 1.1112, acc: 0.3163, pr: 0.3084, rc: 0.9956, sp: 0.0509, f1: 0.4709\n",
      "                                   tn: 466, tp: 3877, fn: 17, fp: 8696\n",
      "  Test  loss: 1.2834, acc: 0.1209, pr: 0.0578, rc: 1.0000, sp: 0.0361, f1: 0.1093\n",
      "                                   tn: 22, tp: 36, fn: 0, fp: 587\n",
      "Epoch: 7\n",
      "  Train loss: 1.0357, acc: 0.4862, pr: 0.3076, rc: 0.9933, sp: 0.0498, f1: 0.4698\n",
      "                                   tn: 456, tp: 3868, fn: 26, fp: 8706\n",
      "  Test  loss: 1.1596, acc: 0.4465, pr: 0.0586, rc: 1.0000, sp: 0.0509, f1: 0.1108\n",
      "                                   tn: 31, tp: 36, fn: 0, fp: 578\n",
      "Epoch: 8\n",
      "  Train loss: 0.9517, acc: 0.3880, pr: 0.2991, rc: 1.0000, sp: 0.0041, f1: 0.4605\n",
      "                                   tn: 38, tp: 3894, fn: 0, fp: 9124\n",
      "  Test  loss: 1.0065, acc: 0.2543, pr: 0.0559, rc: 1.0000, sp: 0.0016, f1: 0.1059\n",
      "                                   tn: 1, tp: 36, fn: 0, fp: 608\n",
      "Epoch: 9\n",
      "  Train loss: 0.9615, acc: 0.4312, pr: 0.2983, rc: 1.0000, sp: 0.0000, f1: 0.4595\n",
      "                                   tn: 0, tp: 3894, fn: 0, fp: 9162\n",
      "  Test  loss: 1.0357, acc: 0.2837, pr: 0.0558, rc: 1.0000, sp: 0.0000, f1: 0.1057\n",
      "                                   tn: 0, tp: 36, fn: 0, fp: 609\n",
      "Epoch: 10\n",
      "  Train loss: 0.8647, acc: 0.6612, pr: 0.3001, rc: 0.9967, sp: 0.0119, f1: 0.4613\n",
      "                                   tn: 109, tp: 3881, fn: 13, fp: 9053\n",
      "  Test  loss: 0.8317, acc: 0.7442, pr: 0.0562, rc: 1.0000, sp: 0.0066, f1: 0.1064\n",
      "                                   tn: 4, tp: 36, fn: 0, fp: 605\n",
      "Epoch: 11\n",
      "  Train loss: 0.8405, acc: 0.7025, pr: 0.3428, rc: 0.9158, sp: 0.2539, f1: 0.4989\n",
      "                                   tn: 2326, tp: 3566, fn: 328, fp: 6836\n",
      "  Test  loss: 0.7992, acc: 0.7969, pr: 0.0627, rc: 0.8889, sp: 0.2151, f1: 0.1172\n",
      "                                   tn: 131, tp: 32, fn: 4, fp: 478\n",
      "Epoch: 12\n",
      "  Train loss: 0.8787, acc: 0.6423, pr: 0.3042, rc: 0.9969, sp: 0.0307, f1: 0.4661\n",
      "                                   tn: 281, tp: 3882, fn: 12, fp: 8881\n",
      "  Test  loss: 0.8911, acc: 0.6775, pr: 0.0570, rc: 1.0000, sp: 0.0213, f1: 0.1078\n",
      "                                   tn: 13, tp: 36, fn: 0, fp: 596\n",
      "Epoch: 13\n",
      "  Train loss: 0.8114, acc: 0.7158, pr: 0.4160, rc: 0.5907, sp: 0.6476, f1: 0.4882\n",
      "                                   tn: 5933, tp: 2300, fn: 1594, fp: 3229\n",
      "  Test  loss: 0.7465, acc: 0.8279, pr: 0.0596, rc: 0.3889, sp: 0.6371, f1: 0.1033\n",
      "                                   tn: 388, tp: 14, fn: 22, fp: 221\n",
      "Epoch: 14\n",
      "  Train loss: 0.7933, acc: 0.7269, pr: 0.4752, rc: 0.4823, sp: 0.7736, f1: 0.4787\n",
      "                                   tn: 7088, tp: 1878, fn: 2016, fp: 2074\n",
      "  Test  loss: 0.7222, acc: 0.8403, pr: 0.0748, rc: 0.3056, sp: 0.7767, f1: 0.1202\n",
      "                                   tn: 473, tp: 11, fn: 25, fp: 136\n",
      "Epoch: 15\n",
      "  Train loss: 0.7983, acc: 0.7103, pr: 0.4469, rc: 0.5837, sp: 0.6930, f1: 0.5062\n",
      "                                   tn: 6349, tp: 2273, fn: 1621, fp: 2813\n",
      "  Test  loss: 0.7463, acc: 0.8016, pr: 0.0548, rc: 0.3333, sp: 0.6601, f1: 0.0941\n",
      "                                   tn: 402, tp: 12, fn: 24, fp: 207\n",
      "Epoch: 16\n",
      "  Train loss: 0.7801, acc: 0.7253, pr: 0.4759, rc: 0.5455, sp: 0.7447, f1: 0.5083\n",
      "                                   tn: 6823, tp: 2124, fn: 1770, fp: 2339\n",
      "  Test  loss: 0.7084, acc: 0.8388, pr: 0.0649, rc: 0.3333, sp: 0.7159, f1: 0.1086\n",
      "                                   tn: 436, tp: 12, fn: 24, fp: 173\n",
      "Epoch: 17\n",
      "  Train loss: 0.7623, acc: 0.7298, pr: 0.4945, rc: 0.5473, sp: 0.7623, f1: 0.5196\n",
      "                                   tn: 6984, tp: 2131, fn: 1763, fp: 2178\n",
      "  Test  loss: 0.6795, acc: 0.8465, pr: 0.0788, rc: 0.3611, sp: 0.7504, f1: 0.1294\n",
      "                                   tn: 457, tp: 13, fn: 23, fp: 152\n",
      "Epoch: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 0.7536, acc: 0.7326, pr: 0.5033, rc: 0.5300, sp: 0.7777, f1: 0.5163\n",
      "                                   tn: 7125, tp: 2064, fn: 1830, fp: 2037\n",
      "  Test  loss: 0.6676, acc: 0.8481, pr: 0.0710, rc: 0.3056, sp: 0.7635, f1: 0.1152\n",
      "                                   tn: 465, tp: 11, fn: 25, fp: 144\n",
      "Epoch: 19\n",
      "  Train loss: 0.7384, acc: 0.7414, pr: 0.5413, rc: 0.4869, sp: 0.8246, f1: 0.5126\n",
      "                                   tn: 7555, tp: 1896, fn: 1998, fp: 1607\n",
      "  Test  loss: 0.6329, acc: 0.8620, pr: 0.0763, rc: 0.2500, sp: 0.8210, f1: 0.1169\n",
      "                                   tn: 500, tp: 9, fn: 27, fp: 109\n",
      "Epoch: 20\n",
      "  Train loss: 0.7416, acc: 0.7438, pr: 0.5378, rc: 0.5259, sp: 0.8079, f1: 0.5318\n",
      "                                   tn: 7402, tp: 2048, fn: 1846, fp: 1760\n",
      "  Test  loss: 0.6396, acc: 0.8713, pr: 0.0698, rc: 0.2500, sp: 0.8030, f1: 0.1091\n",
      "                                   tn: 489, tp: 9, fn: 27, fp: 120\n",
      "Epoch: 21\n",
      "  Train loss: 0.7426, acc: 0.7482, pr: 0.5021, rc: 0.6713, sp: 0.7171, f1: 0.5745\n",
      "                                   tn: 6570, tp: 2614, fn: 1280, fp: 2592\n",
      "  Test  loss: 0.6379, acc: 0.8822, pr: 0.0751, rc: 0.3611, sp: 0.7373, f1: 0.1244\n",
      "                                   tn: 449, tp: 13, fn: 23, fp: 160\n",
      "Epoch: 22\n",
      "  Train loss: 0.7594, acc: 0.7502, pr: 0.4465, rc: 0.8005, sp: 0.5783, f1: 0.5732\n",
      "                                   tn: 5298, tp: 3117, fn: 777, fp: 3864\n",
      "  Test  loss: 0.6493, acc: 0.8868, pr: 0.1071, rc: 0.6667, sp: 0.6716, f1: 0.1846\n",
      "                                   tn: 409, tp: 24, fn: 12, fp: 200\n",
      "Epoch: 23\n",
      "  Train loss: 0.7457, acc: 0.7547, pr: 0.4680, rc: 0.7853, sp: 0.6206, f1: 0.5865\n",
      "                                   tn: 5686, tp: 3058, fn: 836, fp: 3476\n",
      "  Test  loss: 0.6341, acc: 0.8791, pr: 0.1014, rc: 0.5833, sp: 0.6946, f1: 0.1728\n",
      "                                   tn: 423, tp: 21, fn: 15, fp: 186\n",
      "Epoch: 24\n",
      "  Train loss: 0.7297, acc: 0.7539, pr: 0.5217, rc: 0.6687, sp: 0.7395, f1: 0.5862\n",
      "                                   tn: 6775, tp: 2604, fn: 1290, fp: 2387\n",
      "  Test  loss: 0.6187, acc: 0.8806, pr: 0.1069, rc: 0.4722, sp: 0.7668, f1: 0.1744\n",
      "                                   tn: 467, tp: 17, fn: 19, fp: 142\n",
      "Epoch: 25\n",
      "  Train loss: 0.7308, acc: 0.7550, pr: 0.4637, rc: 0.7964, sp: 0.6085, f1: 0.5861\n",
      "                                   tn: 5575, tp: 3101, fn: 793, fp: 3587\n",
      "  Test  loss: 0.6228, acc: 0.8868, pr: 0.0905, rc: 0.5556, sp: 0.6700, f1: 0.1556\n",
      "                                   tn: 408, tp: 20, fn: 16, fp: 201\n",
      "Epoch: 26\n",
      "  Train loss: 0.7267, acc: 0.7521, pr: 0.4928, rc: 0.7147, sp: 0.6874, f1: 0.5834\n",
      "                                   tn: 6298, tp: 2783, fn: 1111, fp: 2864\n",
      "  Test  loss: 0.6227, acc: 0.8837, pr: 0.1207, rc: 0.5833, sp: 0.7488, f1: 0.2000\n",
      "                                   tn: 456, tp: 21, fn: 15, fp: 153\n",
      "Epoch: 27\n",
      "  Train loss: 0.7134, acc: 0.7569, pr: 0.4922, rc: 0.7486, sp: 0.6718, f1: 0.5939\n",
      "                                   tn: 6155, tp: 2915, fn: 979, fp: 3007\n",
      "  Test  loss: 0.6037, acc: 0.8899, pr: 0.1022, rc: 0.5278, sp: 0.7258, f1: 0.1712\n",
      "                                   tn: 442, tp: 19, fn: 17, fp: 167\n",
      "Epoch: 28\n",
      "  Train loss: 0.7017, acc: 0.7580, pr: 0.5209, rc: 0.6764, sp: 0.7355, f1: 0.5885\n",
      "                                   tn: 6739, tp: 2634, fn: 1260, fp: 2423\n",
      "  Test  loss: 0.6078, acc: 0.8775, pr: 0.1118, rc: 0.5278, sp: 0.7521, f1: 0.1845\n",
      "                                   tn: 458, tp: 19, fn: 17, fp: 151\n",
      "Epoch: 29\n",
      "  Train loss: 0.7022, acc: 0.7574, pr: 0.4935, rc: 0.7548, sp: 0.6707, f1: 0.5968\n",
      "                                   tn: 6145, tp: 2939, fn: 955, fp: 3017\n",
      "  Test  loss: 0.6080, acc: 0.8822, pr: 0.0884, rc: 0.5278, sp: 0.6782, f1: 0.1514\n",
      "                                   tn: 413, tp: 19, fn: 17, fp: 196\n",
      "Epoch: 30\n",
      "  Train loss: 0.6768, acc: 0.7629, pr: 0.5466, rc: 0.6944, sp: 0.7552, f1: 0.6117\n",
      "                                   tn: 6919, tp: 2704, fn: 1190, fp: 2243\n",
      "  Test  loss: 0.5647, acc: 0.8899, pr: 0.1126, rc: 0.4722, sp: 0.7800, f1: 0.1818\n",
      "                                   tn: 475, tp: 17, fn: 19, fp: 134\n",
      "Epoch: 31\n",
      "  Train loss: 0.6778, acc: 0.7639, pr: 0.5258, rc: 0.7476, sp: 0.7135, f1: 0.6174\n",
      "                                   tn: 6537, tp: 2911, fn: 983, fp: 2625\n",
      "  Test  loss: 0.5759, acc: 0.8806, pr: 0.1045, rc: 0.5833, sp: 0.7044, f1: 0.1772\n",
      "                                   tn: 429, tp: 21, fn: 15, fp: 180\n",
      "Epoch: 32\n",
      "  Train loss: 0.6650, acc: 0.7644, pr: 0.5489, rc: 0.7101, sp: 0.7520, f1: 0.6192\n",
      "                                   tn: 6890, tp: 2765, fn: 1129, fp: 2272\n",
      "  Test  loss: 0.5664, acc: 0.8698, pr: 0.1366, rc: 0.6111, sp: 0.7718, f1: 0.2234\n",
      "                                   tn: 470, tp: 22, fn: 14, fp: 139\n",
      "Epoch: 33\n",
      "  Train loss: 0.7118, acc: 0.7472, pr: 0.4577, rc: 0.8328, sp: 0.5806, f1: 0.5907\n",
      "                                   tn: 5319, tp: 3243, fn: 651, fp: 3843\n",
      "  Test  loss: 0.6713, acc: 0.8202, pr: 0.0848, rc: 0.6667, sp: 0.5747, f1: 0.1505\n",
      "                                   tn: 350, tp: 24, fn: 12, fp: 259\n",
      "Epoch: 34\n",
      "  Train loss: 0.6967, acc: 0.7583, pr: 0.4447, rc: 0.8598, sp: 0.5437, f1: 0.5862\n",
      "                                   tn: 4981, tp: 3348, fn: 546, fp: 4181\n",
      "  Test  loss: 0.6276, acc: 0.8651, pr: 0.0888, rc: 0.7500, sp: 0.5452, f1: 0.1588\n",
      "                                   tn: 332, tp: 27, fn: 9, fp: 277\n",
      "Epoch: 35\n",
      "  Train loss: 0.6639, acc: 0.7763, pr: 0.5432, rc: 0.7375, sp: 0.7364, f1: 0.6256\n",
      "                                   tn: 6747, tp: 2872, fn: 1022, fp: 2415\n",
      "  Test  loss: 0.5540, acc: 0.9054, pr: 0.1384, rc: 0.6111, sp: 0.7750, f1: 0.2256\n",
      "                                   tn: 472, tp: 22, fn: 14, fp: 137\n",
      "Epoch: 36\n",
      "  Train loss: 0.6425, acc: 0.7793, pr: 0.5728, rc: 0.7234, sp: 0.7707, f1: 0.6394\n",
      "                                   tn: 7061, tp: 2817, fn: 1077, fp: 2101\n",
      "  Test  loss: 0.5442, acc: 0.9054, pr: 0.1429, rc: 0.6111, sp: 0.7833, f1: 0.2316\n",
      "                                   tn: 477, tp: 22, fn: 14, fp: 132\n",
      "Epoch: 37\n",
      "  Train loss: 0.6462, acc: 0.7747, pr: 0.5672, rc: 0.7203, sp: 0.7664, f1: 0.6347\n",
      "                                   tn: 7022, tp: 2805, fn: 1089, fp: 2140\n",
      "  Test  loss: 0.5367, acc: 0.8992, pr: 0.1571, rc: 0.6111, sp: 0.8062, f1: 0.2500\n",
      "                                   tn: 491, tp: 22, fn: 14, fp: 118\n",
      "Epoch: 38\n",
      "  Train loss: 0.6438, acc: 0.7820, pr: 0.5254, rc: 0.7861, sp: 0.6982, f1: 0.6298\n",
      "                                   tn: 6397, tp: 3061, fn: 833, fp: 2765\n",
      "  Test  loss: 0.5496, acc: 0.8992, pr: 0.1176, rc: 0.6111, sp: 0.7291, f1: 0.1973\n",
      "                                   tn: 444, tp: 22, fn: 14, fp: 165\n",
      "Epoch: 39\n",
      "  Train loss: 0.6107, acc: 0.7817, pr: 0.6014, rc: 0.7237, sp: 0.7961, f1: 0.6569\n",
      "                                   tn: 7294, tp: 2818, fn: 1076, fp: 1868\n",
      "  Test  loss: 0.5072, acc: 0.8961, pr: 0.1724, rc: 0.5556, sp: 0.8424, f1: 0.2632\n",
      "                                   tn: 513, tp: 20, fn: 16, fp: 96\n",
      "Epoch: 40\n",
      "  Train loss: 0.6175, acc: 0.7802, pr: 0.5880, rc: 0.7524, sp: 0.7759, f1: 0.6601\n",
      "                                   tn: 7109, tp: 2930, fn: 964, fp: 2053\n",
      "  Test  loss: 0.5355, acc: 0.8837, pr: 0.1678, rc: 0.6944, sp: 0.7964, f1: 0.2703\n",
      "                                   tn: 485, tp: 25, fn: 11, fp: 124\n",
      "Epoch: 41\n",
      "  Train loss: 0.5869, acc: 0.7911, pr: 0.6114, rc: 0.7514, sp: 0.7970, f1: 0.6742\n",
      "                                   tn: 7302, tp: 2926, fn: 968, fp: 1860\n",
      "  Test  loss: 0.5020, acc: 0.8915, pr: 0.1833, rc: 0.6111, sp: 0.8391, f1: 0.2821\n",
      "                                   tn: 511, tp: 22, fn: 14, fp: 98\n",
      "Epoch: 42\n",
      "  Train loss: 0.5512, acc: 0.8003, pr: 0.6603, rc: 0.7334, sp: 0.8397, f1: 0.6950\n",
      "                                   tn: 7693, tp: 2856, fn: 1038, fp: 1469\n",
      "  Test  loss: 0.4631, acc: 0.8915, pr: 0.1982, rc: 0.6111, sp: 0.8539, f1: 0.2993\n",
      "                                   tn: 520, tp: 22, fn: 14, fp: 89\n",
      "Epoch: 43\n",
      "  Train loss: 0.6196, acc: 0.7855, pr: 0.5329, rc: 0.8277, sp: 0.6917, f1: 0.6484\n",
      "                                   tn: 6337, tp: 3223, fn: 671, fp: 2825\n",
      "  Test  loss: 0.5418, acc: 0.8930, pr: 0.1299, rc: 0.6389, sp: 0.7471, f1: 0.2160\n",
      "                                   tn: 455, tp: 23, fn: 13, fp: 154\n",
      "Epoch: 44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 0.6015, acc: 0.7895, pr: 0.5720, rc: 0.7745, sp: 0.7537, f1: 0.6580\n",
      "                                   tn: 6905, tp: 3016, fn: 878, fp: 2257\n",
      "  Test  loss: 0.5300, acc: 0.8915, pr: 0.1429, rc: 0.5556, sp: 0.8030, f1: 0.2273\n",
      "                                   tn: 489, tp: 20, fn: 16, fp: 120\n",
      "Epoch: 45\n",
      "  Train loss: 0.5749, acc: 0.8030, pr: 0.5757, rc: 0.8179, sp: 0.7438, f1: 0.6758\n",
      "                                   tn: 6815, tp: 3185, fn: 709, fp: 2347\n",
      "  Test  loss: 0.5197, acc: 0.8853, pr: 0.1419, rc: 0.6111, sp: 0.7816, f1: 0.2304\n",
      "                                   tn: 476, tp: 22, fn: 14, fp: 133\n",
      "Epoch: 46\n",
      "  Train loss: 0.5905, acc: 0.8028, pr: 0.6133, rc: 0.7904, sp: 0.7881, f1: 0.6907\n",
      "                                   tn: 7221, tp: 3078, fn: 816, fp: 1941\n",
      "  Test  loss: 0.5206, acc: 0.8961, pr: 0.1938, rc: 0.6944, sp: 0.8292, f1: 0.3030\n",
      "                                   tn: 505, tp: 25, fn: 11, fp: 104\n",
      "Epoch: 47\n",
      "  Train loss: 0.5799, acc: 0.8015, pr: 0.5839, rc: 0.8210, sp: 0.7514, f1: 0.6825\n",
      "                                   tn: 6884, tp: 3197, fn: 697, fp: 2278\n",
      "  Test  loss: 0.5264, acc: 0.8946, pr: 0.1733, rc: 0.7222, sp: 0.7964, f1: 0.2796\n",
      "                                   tn: 485, tp: 26, fn: 10, fp: 124\n",
      "Epoch: 48\n",
      "  Train loss: 0.5668, acc: 0.8091, pr: 0.6284, rc: 0.7979, sp: 0.7995, f1: 0.7031\n",
      "                                   tn: 7325, tp: 3107, fn: 787, fp: 1837\n",
      "  Test  loss: 0.5056, acc: 0.8899, pr: 0.1774, rc: 0.6111, sp: 0.8325, f1: 0.2750\n",
      "                                   tn: 507, tp: 22, fn: 14, fp: 102\n",
      "Epoch: 49\n",
      "  Train loss: 0.5469, acc: 0.8128, pr: 0.6217, rc: 0.8074, sp: 0.7912, f1: 0.7025\n",
      "                                   tn: 7249, tp: 3144, fn: 750, fp: 1913\n",
      "  Test  loss: 0.4966, acc: 0.8961, pr: 0.1756, rc: 0.6389, sp: 0.8227, f1: 0.2754\n",
      "                                   tn: 501, tp: 23, fn: 13, fp: 108\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "params[\"x_th\"] = -.4\n",
    "for var_th in np.arange(0, 2.1, .25):    \n",
    "    var_th = int(var_th*100)/100    \n",
    "    if var_th !=1.0: \n",
    "        continue\n",
    "    \n",
    "    params[\"var_th\"] = var_th \n",
    "    cond = (features[:, 0] <=params[\"x_th\"]) & (features[:, -1] >=params[\"var_th\"]) #& (ssml[:, 0]>=7)\n",
    "    w = windows[cond]\n",
    "    s = ssml[cond]\n",
    "    l = labels[cond]\n",
    "    f = features[cond]\n",
    "    print(w.shape, s.shape, np.sum(l, axis=0))\n",
    "    \n",
    "    for exclude_subject in range(1,2):\n",
    "        print(\"******** Exclude Subject {} *********\".format(exclude_subject))\n",
    "        print(params)\n",
    "        cond = s[:,0]!=exclude_subject\n",
    "        train_x = w[cond]\n",
    "        train_y = l[cond]\n",
    "        \n",
    "        cond = s[:,0]==exclude_subject\n",
    "        test_x = w[cond]        \n",
    "        test_y = l[cond]\n",
    "\n",
    "        print(\"Train Test shapes: \", train_x.shape, test_x.shape)\n",
    "        print(\"Train Test labels:\", np.sum(train_y, axis=0), np.sum(test_y, axis=0))\n",
    "\n",
    "        path = root_path+'/outputs/bite_detection_lopo/'+bdu.param_string(params)+\"/subject_\"+str(exclude_subject)     \n",
    "        test_pred, train_result, test_result = mtu.train_test_model(train_x, train_y, test_x, test_y, folder_path=path, params=net_params)\n",
    "        path = path+'/results'\n",
    "        bdu.create_directory(path)\n",
    "        np.savetxt(path+'/test_y.csv', test_y, fmt='%.4f', delimiter=',')    \n",
    "        np.savetxt(path+'/test_prediction.csv', test_pred, fmt='%.4f', delimiter=',')\n",
    "        np.savetxt(path+'/train_result.csv', train_result, fmt='%.4f', delimiter=',')\n",
    "        np.savetxt(path+'/test_result.csv', test_result, fmt='%.4f', delimiter=',') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 1.42637718e+00 3.10077518e-02 0.00000000e+00\n",
      "  3.60000000e+01 0.00000000e+00 6.09000000e+02 5.58139535e-02\n",
      "  1.00000000e+00 0.00000000e+00 1.05726872e-01]\n",
      " [1.00000000e+00 1.43103886e+00 3.72093022e-02 1.00000000e+00\n",
      "  3.60000000e+01 0.00000000e+00 6.08000000e+02 5.59006211e-02\n",
      "  1.00000000e+00 1.64203612e-03 1.05882353e-01]\n",
      " [2.00000000e+00 1.52142847e+00 3.10077518e-02 0.00000000e+00\n",
      "  3.60000000e+01 0.00000000e+00 6.09000000e+02 5.58139535e-02\n",
      "  1.00000000e+00 0.00000000e+00 1.05726872e-01]\n",
      " [3.00000000e+00 1.70276618e+00 3.72093022e-02 3.00000000e+00\n",
      "  3.60000000e+01 0.00000000e+00 6.06000000e+02 5.60747664e-02\n",
      "  1.00000000e+00 4.92610837e-03 1.06194690e-01]\n",
      " [4.00000000e+00 1.42516303e+00 3.72093022e-02 0.00000000e+00\n",
      "  3.60000000e+01 0.00000000e+00 6.09000000e+02 5.58139535e-02\n",
      "  1.00000000e+00 0.00000000e+00 1.05726872e-01]\n",
      " [5.00000000e+00 1.54905903e+00 3.41085270e-02 0.00000000e+00\n",
      "  3.60000000e+01 0.00000000e+00 6.09000000e+02 5.58139535e-02\n",
      "  1.00000000e+00 0.00000000e+00 1.05726872e-01]\n",
      " [6.00000000e+00 1.45962977e+00 6.97674453e-02 9.00000000e+00\n",
      "  3.60000000e+01 0.00000000e+00 6.00000000e+02 5.66037736e-02\n",
      "  1.00000000e+00 1.47783251e-02 1.07142857e-01]\n",
      " [7.00000000e+00 1.47132766e+00 6.20155036e-02 1.00000000e+01\n",
      "  3.60000000e+01 0.00000000e+00 5.99000000e+02 5.66929134e-02\n",
      "  1.00000000e+00 1.64203612e-02 1.07302534e-01]\n",
      " [8.00000000e+00 1.37940860e+00 1.22480623e-01 2.90000000e+01\n",
      "  3.60000000e+01 0.00000000e+00 5.80000000e+02 5.84415584e-02\n",
      "  1.00000000e+00 4.76190476e-02 1.10429448e-01]\n",
      " [9.00000000e+00 1.21933293e+00 2.62015492e-01 8.00000000e+01\n",
      "  3.50000000e+01 1.00000000e+00 5.29000000e+02 6.20567376e-02\n",
      "  9.72222222e-01 1.31362890e-01 1.16666667e-01]\n",
      " [1.00000000e+01 9.88013625e-01 5.84496140e-01 1.08000000e+02\n",
      "  3.50000000e+01 1.00000000e+00 5.01000000e+02 6.52985075e-02\n",
      "  9.72222222e-01 1.77339901e-01 1.22377622e-01]\n",
      " [1.10000000e+01 1.11306608e+00 4.43410844e-01 6.20000000e+01\n",
      "  3.50000000e+01 1.00000000e+00 5.47000000e+02 6.01374570e-02\n",
      "  9.72222222e-01 1.01806240e-01 1.13268608e-01]\n",
      " [1.20000000e+01 8.04858804e-01 7.87596881e-01 2.77000000e+02\n",
      "  2.40000000e+01 1.20000000e+01 3.32000000e+02 6.74157303e-02\n",
      "  6.66666667e-01 4.54844007e-01 1.22448980e-01]\n",
      " [1.30000000e+01 8.46647680e-01 7.64341056e-01 1.31000000e+02\n",
      "  3.00000000e+01 6.00000000e+00 4.78000000e+02 5.90551181e-02\n",
      "  8.33333333e-01 2.15106732e-01 1.10294118e-01]\n",
      " [1.40000000e+01 7.10501373e-01 8.71317804e-01 3.73000000e+02\n",
      "  1.20000000e+01 2.40000000e+01 2.36000000e+02 4.83870968e-02\n",
      "  3.33333333e-01 6.12479475e-01 8.45070423e-02]\n",
      " [1.50000000e+01 6.86828375e-01 8.63565862e-01 4.90000000e+02\n",
      "  1.00000000e+01 2.60000000e+01 1.19000000e+02 7.75193798e-02\n",
      "  2.77777778e-01 8.04597701e-01 1.21212121e-01]\n",
      " [1.60000000e+01 6.88712716e-01 8.49612415e-01 5.08000000e+02\n",
      "  1.10000000e+01 2.50000000e+01 1.01000000e+02 9.82142857e-02\n",
      "  3.05555556e-01 8.34154351e-01 1.48648649e-01]\n",
      " [1.70000000e+01 7.60876119e-01 7.44186044e-01 3.20000000e+02\n",
      "  1.90000000e+01 1.70000000e+01 2.89000000e+02 6.16883117e-02\n",
      "  5.27777778e-01 5.25451560e-01 1.10465116e-01]\n",
      " [1.80000000e+01 6.25823498e-01 8.60465109e-01 5.16000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 9.30000000e+01 8.82352941e-02\n",
      "  2.50000000e-01 8.47290640e-01 1.30434783e-01]\n",
      " [1.90000000e+01 7.38630712e-01 7.70542622e-01 3.13000000e+02\n",
      "  2.10000000e+01 1.50000000e+01 2.96000000e+02 6.62460568e-02\n",
      "  5.83333333e-01 5.13957307e-01 1.18980170e-01]\n",
      " [2.00000000e+01 6.09935462e-01 8.83720934e-01 5.25000000e+02\n",
      "  6.00000000e+00 3.00000000e+01 8.40000000e+01 6.66666667e-02\n",
      "  1.66666667e-01 8.62068966e-01 9.52380952e-02]\n",
      " [2.10000000e+01 6.02523386e-01 8.71317804e-01 5.14000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 9.50000000e+01 8.65384615e-02\n",
      "  2.50000000e-01 8.44006568e-01 1.28571429e-01]\n",
      " [2.20000000e+01 6.59553051e-01 8.49612415e-01 4.51000000e+02\n",
      "  1.30000000e+01 2.30000000e+01 1.58000000e+02 7.60233918e-02\n",
      "  3.61111111e-01 7.40558292e-01 1.25603865e-01]\n",
      " [2.30000000e+01 6.45678043e-01 8.74418616e-01 5.03000000e+02\n",
      "  8.00000000e+00 2.80000000e+01 1.06000000e+02 7.01754386e-02\n",
      "  2.22222222e-01 8.25944171e-01 1.06666667e-01]\n",
      " [2.40000000e+01 6.08760178e-01 8.71317804e-01 5.17000000e+02\n",
      "  1.10000000e+01 2.50000000e+01 9.20000000e+01 1.06796117e-01\n",
      "  3.05555556e-01 8.48932677e-01 1.58273381e-01]\n",
      " [2.50000000e+01 6.67463422e-01 8.97674441e-01 4.72000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 1.37000000e+02 6.16438356e-02\n",
      "  2.50000000e-01 7.75041051e-01 9.89010989e-02]\n",
      " [2.60000000e+01 6.18695796e-01 8.86821687e-01 5.02000000e+02\n",
      "  1.00000000e+01 2.60000000e+01 1.07000000e+02 8.54700855e-02\n",
      "  2.77777778e-01 8.24302135e-01 1.30718954e-01]\n",
      " [2.70000000e+01 5.89274466e-01 8.83720934e-01 4.96000000e+02\n",
      "  1.00000000e+01 2.60000000e+01 1.13000000e+02 8.13008130e-02\n",
      "  2.77777778e-01 8.14449918e-01 1.25786164e-01]\n",
      " [2.80000000e+01 5.62067807e-01 8.96124005e-01 5.29000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 8.00000000e+01 1.01123596e-01\n",
      "  2.50000000e-01 8.68637110e-01 1.44000000e-01]\n",
      " [2.90000000e+01 5.84091961e-01 8.75968993e-01 5.25000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 8.40000000e+01 9.67741935e-02\n",
      "  2.50000000e-01 8.62068966e-01 1.39534884e-01]\n",
      " [3.00000000e+01 5.56252658e-01 8.88372064e-01 5.29000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 8.00000000e+01 1.01123596e-01\n",
      "  2.50000000e-01 8.68637110e-01 1.44000000e-01]\n",
      " [3.10000000e+01 5.55335045e-01 8.83720934e-01 5.28000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 8.10000000e+01 1.00000000e-01\n",
      "  2.50000000e-01 8.66995074e-01 1.42857143e-01]\n",
      " [3.20000000e+01 5.19085467e-01 8.93023252e-01 5.30000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 7.90000000e+01 1.02272727e-01\n",
      "  2.50000000e-01 8.70279146e-01 1.45161290e-01]\n",
      " [3.30000000e+01 5.90006113e-01 8.75968993e-01 4.63000000e+02\n",
      "  1.60000000e+01 2.00000000e+01 1.46000000e+02 9.87654321e-02\n",
      "  4.44444444e-01 7.60262726e-01 1.61616162e-01]\n",
      " [3.40000000e+01 5.23498595e-01 8.74418616e-01 5.18000000e+02\n",
      "  1.30000000e+01 2.30000000e+01 9.10000000e+01 1.25000000e-01\n",
      "  3.61111111e-01 8.50574713e-01 1.85714286e-01]\n",
      " [3.50000000e+01 5.41376889e-01 9.03875947e-01 4.75000000e+02\n",
      "  1.30000000e+01 2.30000000e+01 1.34000000e+02 8.84353741e-02\n",
      "  3.61111111e-01 7.79967159e-01 1.42076503e-01]\n",
      " [3.60000000e+01 5.07163048e-01 8.96124005e-01 5.13000000e+02\n",
      "  1.20000000e+01 2.40000000e+01 9.60000000e+01 1.11111111e-01\n",
      "  3.33333333e-01 8.42364532e-01 1.66666667e-01]\n",
      " [3.70000000e+01 5.24312794e-01 8.94573629e-01 4.92000000e+02\n",
      "  1.50000000e+01 2.10000000e+01 1.17000000e+02 1.13636364e-01\n",
      "  4.16666667e-01 8.07881773e-01 1.78571429e-01]\n",
      " [3.80000000e+01 5.02228796e-01 8.94573629e-01 5.01000000e+02\n",
      "  1.50000000e+01 2.10000000e+01 1.08000000e+02 1.21951220e-01\n",
      "  4.16666667e-01 8.22660099e-01 1.88679245e-01]\n",
      " [3.90000000e+01 4.92781043e-01 9.14728701e-01 4.96000000e+02\n",
      "  1.30000000e+01 2.30000000e+01 1.13000000e+02 1.03174603e-01\n",
      "  3.61111111e-01 8.14449918e-01 1.60493827e-01]\n",
      " [4.00000000e+01 4.99624312e-01 8.97674441e-01 4.87000000e+02\n",
      "  1.80000000e+01 1.80000000e+01 1.22000000e+02 1.28571429e-01\n",
      "  5.00000000e-01 7.99671593e-01 2.04545455e-01]\n",
      " [4.10000000e+01 5.52969694e-01 8.51162791e-01 4.28000000e+02\n",
      "  2.30000000e+01 1.30000000e+01 1.81000000e+02 1.12745098e-01\n",
      "  6.38888889e-01 7.02791461e-01 1.91666667e-01]\n",
      " [4.20000000e+01 5.72475374e-01 8.18604648e-01 4.25000000e+02\n",
      "  2.10000000e+01 1.50000000e+01 1.84000000e+02 1.02439024e-01\n",
      "  5.83333333e-01 6.97865353e-01 1.74273859e-01]\n",
      " [4.30000000e+01 4.92370427e-01 9.02325571e-01 5.12000000e+02\n",
      "  2.10000000e+01 1.50000000e+01 9.70000000e+01 1.77966102e-01\n",
      "  5.83333333e-01 8.40722496e-01 2.72727273e-01]\n",
      " [4.40000000e+01 5.06713390e-01 8.71317804e-01 4.99000000e+02\n",
      "  2.00000000e+01 1.60000000e+01 1.10000000e+02 1.53846154e-01\n",
      "  5.55555556e-01 8.19376026e-01 2.40963855e-01]\n",
      " [4.50000000e+01 4.75012958e-01 8.85271311e-01 5.21000000e+02\n",
      "  2.00000000e+01 1.60000000e+01 8.80000000e+01 1.85185185e-01\n",
      "  5.55555556e-01 8.55500821e-01 2.77777778e-01]\n",
      " [4.60000000e+01 4.80443150e-01 8.88372064e-01 5.14000000e+02\n",
      "  2.00000000e+01 1.60000000e+01 9.50000000e+01 1.73913043e-01\n",
      "  5.55555556e-01 8.44006568e-01 2.64900662e-01]\n",
      " [4.70000000e+01 4.73893553e-01 8.65116298e-01 4.98000000e+02\n",
      "  2.00000000e+01 1.60000000e+01 1.11000000e+02 1.52671756e-01\n",
      "  5.55555556e-01 8.17733990e-01 2.39520958e-01]\n",
      " [4.80000000e+01 4.90187258e-01 8.75968993e-01 5.14000000e+02\n",
      "  2.10000000e+01 1.50000000e+01 9.50000000e+01 1.81034483e-01\n",
      "  5.83333333e-01 8.44006568e-01 2.76315789e-01]\n",
      " [4.90000000e+01 4.72280532e-01 8.85271311e-01 4.94000000e+02\n",
      "  2.30000000e+01 1.30000000e+01 1.15000000e+02 1.66666667e-01\n",
      "  6.38888889e-01 8.11165846e-01 2.64367816e-01]]\n"
     ]
    }
   ],
   "source": [
    "test_result = np.array(test_result)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
