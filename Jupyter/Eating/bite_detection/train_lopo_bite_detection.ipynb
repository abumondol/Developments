{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import my_bite_detection_utils as bdu\n",
    "import my_classification_utils as mcu\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import my_tensorflow_utils as mtu\n",
    "import importlib\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'my_classification_utils' from 'C:\\\\ASM\\\\Dropbox\\\\Developments\\\\Jupyter\\\\Eating\\\\bite_detection\\\\my_classification_utils.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(mtu)\n",
    "importlib.reload(bdu)\n",
    "importlib.reload(mcu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path:  C:/ASM/DevData/eating\n",
      "Creating windows and labels ... x_th: 0, min_bite_interval: 32, window_size: 96\n",
      "(52477, 96, 6) (52477, 4) [47886  4591]\n"
     ]
    }
   ],
   "source": [
    "root_path ='C:/ASM/DevData/eating' if \"C:\" in os.getcwd() else \".\"\n",
    "print(\"Root path: \", root_path)\n",
    "\n",
    "params = {\"x_th\": -0, \"min_bite_interval\": 2*16, \"window_size\": 6*16}\n",
    "windows, ssml, labels, features = bdu.get_windows_lab(params)\n",
    "labels[:, 1] = labels[:, 1] + labels[:, 2]\n",
    "labels = labels[:, :2]\n",
    "print(windows.shape, ssml.shape, np.sum(labels, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Parameters\n",
    "net_params={}\n",
    "net_params['learning_rate'] = 0.001\n",
    "net_params['num_epochs'] = 50\n",
    "net_params['batch_size'] = 128\n",
    "net_params['keep_prob_val'] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13639, 96, 6) (13639, 4) [9719 3920]\n",
      "******** Exclude Subject 1 *********\n",
      "{'x_th': -0.4, 'min_bite_interval': 32, 'window_size': 96, 'var_th': 1.0}\n",
      "Train Test shapes:  (12994, 96, 6) (645, 96, 6)\n",
      "Train Test labels: [9110 3884] [609  36]\n",
      "Inside all_axes_net: x_shape, y_shape : [None, 96, 6] [None, 2]\n",
      "<class 'list'>\n",
      "Inside one_axis_net:  conv_axis_0 , x_shape [None, 96, 1]\n",
      "  Axis count:  1\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  0 [None, 512]\n",
      "Inside one_axis_net:  conv_axis_1 , x_shape [None, 96, 1]\n",
      "  Axis count:  1\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  1 [None, 512]\n",
      "Inside one_axis_net:  conv_axis_2 , x_shape [None, 96, 1]\n",
      "  Axis count:  1\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  2 [None, 512]\n",
      "Inside one_axis_net:  conv_axis_3 , x_shape [None, 96, 1]\n",
      "  Axis count:  1\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  3 [None, 512]\n",
      "Inside one_axis_net:  conv_axis_4 , x_shape [None, 96, 1]\n",
      "  Axis count:  1\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  4 [None, 512]\n",
      "Inside one_axis_net:  conv_axis_5 , x_shape [None, 96, 1]\n",
      "  Axis count:  1\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  5 [None, 512]\n",
      "Inside one_axis_net:  conv_3_axes_0 , x_shape [None, 96, 3]\n",
      "  Axis count:  3\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  0 [None, 512]\n",
      "Inside one_axis_net:  conv_3_axes_3 , x_shape [None, 96, 3]\n",
      "  Axis count:  3\n",
      "  Conv_1, maxpool_1 shape:  [None, 92, 1, 16] [None, 46, 1, 16]\n",
      "  Conv_2, maxpool_2 shape:  [None, 42, 1, 32] [None, 21, 1, 32]\n",
      "  Conv_3, maxpool_3 shape:  [None, 17, 1, 64] [None, 8, 1, 64]\n",
      "Shape  3 [None, 512]\n",
      "All axis list size: 8\n",
      "Combo shape one net:  [None, 4096]\n",
      "Logit shape:  [None, 2]\n",
      "Epoch: 0\n",
      "  Train loss: 1.1796, acc: 0.2983, pr: 0.2983, rc: 1.0000, sp: 0.0000, f1: 0.4595\n",
      "                                   tn: 0, tp: 3894, fn: 0, fp: 9162\n",
      "  Test  loss: 1.5388, acc: 0.0558, pr: 0.0558, rc: 1.0000, sp: 0.0000, f1: 0.1057\n",
      "                                   tn: 0, tp: 36, fn: 0, fp: 609\n",
      "Epoch: 1\n",
      "  Train loss: 1.1288, acc: 0.2991, pr: 0.2985, rc: 1.0000, sp: 0.0012, f1: 0.4598\n",
      "                                   tn: 11, tp: 3894, fn: 0, fp: 9151\n",
      "  Test  loss: 1.4768, acc: 0.0558, pr: 0.0558, rc: 1.0000, sp: 0.0000, f1: 0.1057\n",
      "                                   tn: 0, tp: 36, fn: 0, fp: 609\n",
      "Epoch: 2\n",
      "  Train loss: 0.9654, acc: 0.3222, pr: 0.3053, rc: 0.9977, sp: 0.0350, f1: 0.4675\n",
      "                                   tn: 321, tp: 3885, fn: 9, fp: 8841\n",
      "  Test  loss: 1.2473, acc: 0.0698, pr: 0.0566, rc: 1.0000, sp: 0.0148, f1: 0.1071\n",
      "                                   tn: 9, tp: 36, fn: 0, fp: 600\n",
      "Epoch: 3\n",
      "  Train loss: 1.0390, acc: 0.3439, pr: 0.3119, rc: 0.9951, sp: 0.0671, f1: 0.4750\n",
      "                                   tn: 615, tp: 3875, fn: 19, fp: 8547\n",
      "  Test  loss: 1.4026, acc: 0.0868, pr: 0.0576, rc: 1.0000, sp: 0.0328, f1: 0.1089\n",
      "                                   tn: 20, tp: 36, fn: 0, fp: 589\n",
      "Epoch: 4\n",
      "  Train loss: 1.0737, acc: 0.3375, pr: 0.3099, rc: 0.9951, sp: 0.0580, f1: 0.4726\n",
      "                                   tn: 531, tp: 3875, fn: 19, fp: 8631\n",
      "  Test  loss: 1.4276, acc: 0.0930, pr: 0.0580, rc: 1.0000, sp: 0.0394, f1: 0.1096\n",
      "                                   tn: 24, tp: 36, fn: 0, fp: 585\n",
      "Epoch: 5\n",
      "  Train loss: 0.9143, acc: 0.3457, pr: 0.3123, rc: 0.9928, sp: 0.0706, f1: 0.4751\n",
      "                                   tn: 647, tp: 3866, fn: 28, fp: 8515\n",
      "  Test  loss: 1.1846, acc: 0.0899, pr: 0.0578, rc: 1.0000, sp: 0.0361, f1: 0.1093\n",
      "                                   tn: 22, tp: 36, fn: 0, fp: 587\n",
      "Epoch: 6\n",
      "  Train loss: 0.9624, acc: 0.3857, pr: 0.3241, rc: 0.9764, sp: 0.1347, f1: 0.4867\n",
      "                                   tn: 1234, tp: 3802, fn: 92, fp: 7928\n",
      "  Test  loss: 1.2385, acc: 0.1566, pr: 0.0606, rc: 0.9722, sp: 0.1084, f1: 0.1140\n",
      "                                   tn: 66, tp: 35, fn: 1, fp: 543\n",
      "Epoch: 7\n",
      "  Train loss: 0.8648, acc: 0.4472, pr: 0.3451, rc: 0.9507, sp: 0.2331, f1: 0.5064\n",
      "                                   tn: 2136, tp: 3702, fn: 192, fp: 7026\n",
      "  Test  loss: 1.0814, acc: 0.2326, pr: 0.0612, rc: 0.8889, sp: 0.1938, f1: 0.1145\n",
      "                                   tn: 118, tp: 32, fn: 4, fp: 491\n",
      "Epoch: 8\n",
      "  Train loss: 0.8074, acc: 0.4133, pr: 0.3328, rc: 0.9628, sp: 0.1798, f1: 0.4947\n",
      "                                   tn: 1647, tp: 3749, fn: 145, fp: 7515\n",
      "  Test  loss: 0.9698, acc: 0.1721, pr: 0.0569, rc: 0.8889, sp: 0.1297, f1: 0.1070\n",
      "                                   tn: 79, tp: 32, fn: 4, fp: 530\n",
      "Epoch: 9\n",
      "  Train loss: 0.8499, acc: 0.4324, pr: 0.3399, rc: 0.9587, sp: 0.2088, f1: 0.5019\n",
      "                                   tn: 1913, tp: 3733, fn: 161, fp: 7249\n",
      "  Test  loss: 1.0699, acc: 0.1969, pr: 0.0554, rc: 0.8333, sp: 0.1593, f1: 0.1038\n",
      "                                   tn: 97, tp: 30, fn: 6, fp: 512\n",
      "Epoch: 10\n",
      "  Train loss: 0.8627, acc: 0.4249, pr: 0.3357, rc: 0.9481, sp: 0.2025, f1: 0.4958\n",
      "                                   tn: 1855, tp: 3692, fn: 202, fp: 7307\n",
      "  Test  loss: 1.1048, acc: 0.1798, pr: 0.0542, rc: 0.8333, sp: 0.1412, f1: 0.1019\n",
      "                                   tn: 86, tp: 30, fn: 6, fp: 523\n",
      "Epoch: 11\n",
      "  Train loss: 0.7238, acc: 0.4992, pr: 0.3631, rc: 0.9009, sp: 0.3285, f1: 0.5176\n",
      "                                   tn: 3010, tp: 3508, fn: 386, fp: 6152\n",
      "  Test  loss: 0.8384, acc: 0.3163, pr: 0.0569, rc: 0.7222, sp: 0.2923, f1: 0.1055\n",
      "                                   tn: 178, tp: 26, fn: 10, fp: 431\n",
      "Epoch: 12\n",
      "  Train loss: 0.7838, acc: 0.4703, pr: 0.3528, rc: 0.9299, sp: 0.2749, f1: 0.5115\n",
      "                                   tn: 2519, tp: 3621, fn: 273, fp: 6643\n",
      "  Test  loss: 0.9644, acc: 0.2713, pr: 0.0535, rc: 0.7222, sp: 0.2447, f1: 0.0996\n",
      "                                   tn: 149, tp: 26, fn: 10, fp: 460\n",
      "Epoch: 13\n",
      "  Train loss: 0.8712, acc: 0.4655, pr: 0.3514, rc: 0.9368, sp: 0.2652, f1: 0.5111\n",
      "                                   tn: 2430, tp: 3648, fn: 246, fp: 6732\n",
      "  Test  loss: 1.1045, acc: 0.2775, pr: 0.0612, rc: 0.8333, sp: 0.2447, f1: 0.1141\n",
      "                                   tn: 149, tp: 30, fn: 6, fp: 460\n",
      "Epoch: 14\n",
      "  Train loss: 0.9316, acc: 0.4374, pr: 0.3415, rc: 0.9551, sp: 0.2174, f1: 0.5031\n",
      "                                   tn: 1992, tp: 3719, fn: 175, fp: 7170\n",
      "  Test  loss: 1.2372, acc: 0.2109, pr: 0.0596, rc: 0.8889, sp: 0.1708, f1: 0.1117\n",
      "                                   tn: 104, tp: 32, fn: 4, fp: 505\n",
      "Epoch: 15\n",
      "  Train loss: 0.7448, acc: 0.5215, pr: 0.3725, rc: 0.8824, sp: 0.3682, f1: 0.5238\n",
      "                                   tn: 3373, tp: 3436, fn: 458, fp: 5789\n",
      "  Test  loss: 0.8744, acc: 0.4140, pr: 0.0704, rc: 0.7778, sp: 0.3924, f1: 0.1290\n",
      "                                   tn: 239, tp: 28, fn: 8, fp: 370\n",
      "Epoch: 16\n",
      "  Train loss: 0.6749, acc: 0.6486, pr: 0.4370, rc: 0.6176, sp: 0.6618, f1: 0.5118\n",
      "                                   tn: 6063, tp: 2405, fn: 1489, fp: 3099\n",
      "  Test  loss: 0.7351, acc: 0.6434, pr: 0.0783, rc: 0.5000, sp: 0.6519, f1: 0.1353\n",
      "                                   tn: 397, tp: 18, fn: 18, fp: 212\n",
      "Epoch: 17\n",
      "  Train loss: 0.7220, acc: 0.6514, pr: 0.4383, rc: 0.5991, sp: 0.6737, f1: 0.5062\n",
      "                                   tn: 6172, tp: 2333, fn: 1561, fp: 2990\n",
      "  Test  loss: 0.8149, acc: 0.6388, pr: 0.0502, rc: 0.3056, sp: 0.6585, f1: 0.0863\n",
      "                                   tn: 401, tp: 11, fn: 25, fp: 208\n",
      "Epoch: 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 0.6550, acc: 0.7131, pr: 0.5214, rc: 0.4625, sp: 0.8196, f1: 0.4902\n",
      "                                   tn: 7509, tp: 1801, fn: 2093, fp: 1653\n",
      "  Test  loss: 0.6895, acc: 0.7783, pr: 0.0720, rc: 0.2500, sp: 0.8095, f1: 0.1118\n",
      "                                   tn: 493, tp: 9, fn: 27, fp: 116\n",
      "Epoch: 19\n",
      "  Train loss: 0.8157, acc: 0.5227, pr: 0.3635, rc: 0.7989, sp: 0.4054, f1: 0.4996\n",
      "                                   tn: 3714, tp: 3111, fn: 783, fp: 5448\n",
      "  Test  loss: 1.0305, acc: 0.3178, pr: 0.0491, rc: 0.6111, sp: 0.3005, f1: 0.0909\n",
      "                                   tn: 183, tp: 22, fn: 14, fp: 426\n",
      "Epoch: 20\n",
      "  Train loss: 0.6819, acc: 0.6211, pr: 0.4160, rc: 0.6692, sp: 0.6006, f1: 0.5130\n",
      "                                   tn: 5503, tp: 2606, fn: 1288, fp: 3659\n",
      "  Test  loss: 0.7731, acc: 0.5271, pr: 0.0561, rc: 0.4722, sp: 0.5304, f1: 0.1003\n",
      "                                   tn: 323, tp: 17, fn: 19, fp: 286\n",
      "Epoch: 21\n",
      "  Train loss: 0.6411, acc: 0.6730, pr: 0.4619, rc: 0.5842, sp: 0.7108, f1: 0.5159\n",
      "                                   tn: 6512, tp: 2275, fn: 1619, fp: 2650\n",
      "  Test  loss: 0.6893, acc: 0.6605, pr: 0.0580, rc: 0.3333, sp: 0.6798, f1: 0.0988\n",
      "                                   tn: 414, tp: 12, fn: 24, fp: 195\n",
      "Epoch: 22\n",
      "  Train loss: 0.5838, acc: 0.7385, pr: 0.5884, rc: 0.4101, sp: 0.8781, f1: 0.4834\n",
      "                                   tn: 8045, tp: 1597, fn: 2297, fp: 1117\n",
      "  Test  loss: 0.5542, acc: 0.8388, pr: 0.0854, rc: 0.1944, sp: 0.8768, f1: 0.1186\n",
      "                                   tn: 534, tp: 7, fn: 29, fp: 75\n",
      "Epoch: 23\n",
      "  Train loss: 0.5647, acc: 0.7525, pr: 0.6486, rc: 0.3711, sp: 0.9145, f1: 0.4721\n",
      "                                   tn: 8379, tp: 1445, fn: 2449, fp: 783\n",
      "  Test  loss: 0.5125, acc: 0.8667, pr: 0.0690, rc: 0.1111, sp: 0.9113, f1: 0.0851\n",
      "                                   tn: 555, tp: 4, fn: 32, fp: 54\n",
      "Epoch: 24\n",
      "  Train loss: 0.5577, acc: 0.7538, pr: 0.6453, rc: 0.3878, sp: 0.9094, f1: 0.4844\n",
      "                                   tn: 8332, tp: 1510, fn: 2384, fp: 830\n",
      "  Test  loss: 0.5056, acc: 0.8589, pr: 0.0769, rc: 0.1389, sp: 0.9015, f1: 0.0990\n",
      "                                   tn: 549, tp: 5, fn: 31, fp: 60\n",
      "Epoch: 25\n",
      "  Train loss: 0.6110, acc: 0.7152, pr: 0.5243, rc: 0.4841, sp: 0.8134, f1: 0.5034\n",
      "                                   tn: 7452, tp: 1885, fn: 2009, fp: 1710\n",
      "  Test  loss: 0.6261, acc: 0.7705, pr: 0.0625, rc: 0.2222, sp: 0.8030, f1: 0.0976\n",
      "                                   tn: 489, tp: 8, fn: 28, fp: 120\n",
      "Epoch: 26\n",
      "  Train loss: 0.5970, acc: 0.7367, pr: 0.5774, rc: 0.4368, sp: 0.8641, f1: 0.4974\n",
      "                                   tn: 7917, tp: 1701, fn: 2193, fp: 1245\n",
      "  Test  loss: 0.5952, acc: 0.8341, pr: 0.0824, rc: 0.1944, sp: 0.8719, f1: 0.1157\n",
      "                                   tn: 531, tp: 7, fn: 29, fp: 78\n",
      "Epoch: 27\n",
      "  Train loss: 0.5938, acc: 0.7127, pr: 0.5189, rc: 0.5041, sp: 0.8014, f1: 0.5114\n",
      "                                   tn: 7342, tp: 1963, fn: 1931, fp: 1820\n",
      "  Test  loss: 0.5983, acc: 0.7705, pr: 0.0692, rc: 0.2500, sp: 0.8013, f1: 0.1084\n",
      "                                   tn: 488, tp: 9, fn: 27, fp: 121\n",
      "Epoch: 28\n",
      "  Train loss: 0.5548, acc: 0.7482, pr: 0.6096, rc: 0.4327, sp: 0.8822, f1: 0.5062\n",
      "                                   tn: 8083, tp: 1685, fn: 2209, fp: 1079\n",
      "  Test  loss: 0.5094, acc: 0.8465, pr: 0.1013, rc: 0.2222, sp: 0.8834, f1: 0.1391\n",
      "                                   tn: 538, tp: 8, fn: 28, fp: 71\n",
      "Epoch: 29\n",
      "  Train loss: 0.5870, acc: 0.6787, pr: 0.4727, rc: 0.6685, sp: 0.6830, f1: 0.5538\n",
      "                                   tn: 6258, tp: 2603, fn: 1291, fp: 2904\n",
      "  Test  loss: 0.6163, acc: 0.6419, pr: 0.0705, rc: 0.4444, sp: 0.6535, f1: 0.1217\n",
      "                                   tn: 398, tp: 16, fn: 20, fp: 211\n",
      "Epoch: 30\n",
      "  Train loss: 0.5424, acc: 0.7591, pr: 0.6569, rc: 0.4027, sp: 0.9106, f1: 0.4993\n",
      "                                   tn: 8343, tp: 1568, fn: 2326, fp: 819\n",
      "  Test  loss: 0.4831, acc: 0.8822, pr: 0.1154, rc: 0.1667, sp: 0.9245, f1: 0.1364\n",
      "                                   tn: 563, tp: 6, fn: 30, fp: 46\n",
      "Epoch: 31\n",
      "  Train loss: 0.5394, acc: 0.7564, pr: 0.6415, rc: 0.4150, sp: 0.9014, f1: 0.5040\n",
      "                                   tn: 8259, tp: 1616, fn: 2278, fp: 903\n",
      "  Test  loss: 0.4791, acc: 0.8667, pr: 0.1212, rc: 0.2222, sp: 0.9048, f1: 0.1569\n",
      "                                   tn: 551, tp: 8, fn: 28, fp: 58\n",
      "Epoch: 32\n",
      "  Train loss: 0.5315, acc: 0.7646, pr: 0.6766, rc: 0.4040, sp: 0.9179, f1: 0.5059\n",
      "                                   tn: 8410, tp: 1573, fn: 2321, fp: 752\n",
      "  Test  loss: 0.4613, acc: 0.8729, pr: 0.1034, rc: 0.1667, sp: 0.9146, f1: 0.1277\n",
      "                                   tn: 557, tp: 6, fn: 30, fp: 52\n",
      "Epoch: 33\n",
      "  Train loss: 0.5253, acc: 0.7653, pr: 0.6684, rc: 0.4230, sp: 0.9108, f1: 0.5181\n",
      "                                   tn: 8345, tp: 1647, fn: 2247, fp: 817\n",
      "  Test  loss: 0.4556, acc: 0.8806, pr: 0.1132, rc: 0.1667, sp: 0.9228, f1: 0.1348\n",
      "                                   tn: 562, tp: 6, fn: 30, fp: 47\n",
      "Epoch: 34\n",
      "  Train loss: 0.5228, acc: 0.7704, pr: 0.6921, rc: 0.4145, sp: 0.9216, f1: 0.5185\n",
      "                                   tn: 8444, tp: 1614, fn: 2280, fp: 718\n",
      "  Test  loss: 0.4477, acc: 0.8899, pr: 0.1277, rc: 0.1667, sp: 0.9327, f1: 0.1446\n",
      "                                   tn: 568, tp: 6, fn: 30, fp: 41\n",
      "Epoch: 35\n",
      "  Train loss: 0.5175, acc: 0.7757, pr: 0.7099, rc: 0.4191, sp: 0.9272, f1: 0.5270\n",
      "                                   tn: 8495, tp: 1632, fn: 2262, fp: 667\n",
      "  Test  loss: 0.4355, acc: 0.8853, pr: 0.1200, rc: 0.1667, sp: 0.9278, f1: 0.1395\n",
      "                                   tn: 565, tp: 6, fn: 30, fp: 44\n",
      "Epoch: 36\n",
      "  Train loss: 0.5141, acc: 0.7732, pr: 0.6690, rc: 0.4743, sp: 0.9002, f1: 0.5551\n",
      "                                   tn: 8248, tp: 1847, fn: 2047, fp: 914\n",
      "  Test  loss: 0.4536, acc: 0.8713, pr: 0.1148, rc: 0.1944, sp: 0.9113, f1: 0.1443\n",
      "                                   tn: 555, tp: 7, fn: 29, fp: 54\n",
      "Epoch: 37\n",
      "  Train loss: 0.5164, acc: 0.7725, pr: 0.6707, rc: 0.4661, sp: 0.9028, f1: 0.5500\n",
      "                                   tn: 8271, tp: 1815, fn: 2079, fp: 891\n",
      "  Test  loss: 0.4489, acc: 0.8651, pr: 0.1077, rc: 0.1944, sp: 0.9048, f1: 0.1386\n",
      "                                   tn: 551, tp: 7, fn: 29, fp: 58\n",
      "Epoch: 38\n",
      "  Train loss: 0.5097, acc: 0.7744, pr: 0.6785, rc: 0.4633, sp: 0.9067, f1: 0.5506\n",
      "                                   tn: 8307, tp: 1804, fn: 2090, fp: 855\n",
      "  Test  loss: 0.4419, acc: 0.8775, pr: 0.1356, rc: 0.2222, sp: 0.9163, f1: 0.1684\n",
      "                                   tn: 558, tp: 8, fn: 28, fp: 51\n",
      "Epoch: 39\n",
      "  Train loss: 0.5037, acc: 0.7837, pr: 0.6879, rc: 0.5031, sp: 0.9030, f1: 0.5811\n",
      "                                   tn: 8273, tp: 1959, fn: 1935, fp: 889\n",
      "  Test  loss: 0.4336, acc: 0.8868, pr: 0.1509, rc: 0.2222, sp: 0.9261, f1: 0.1798\n",
      "                                   tn: 564, tp: 8, fn: 28, fp: 45\n",
      "Epoch: 40\n",
      "  Train loss: 0.5193, acc: 0.7718, pr: 0.6329, rc: 0.5596, sp: 0.8620, f1: 0.5940\n",
      "                                   tn: 7898, tp: 2179, fn: 1715, fp: 1264\n",
      "  Test  loss: 0.4764, acc: 0.8574, pr: 0.1410, rc: 0.3056, sp: 0.8900, f1: 0.1930\n",
      "                                   tn: 542, tp: 11, fn: 25, fp: 67\n",
      "Epoch: 41\n",
      "  Train loss: 0.5004, acc: 0.7825, pr: 0.6545, rc: 0.5734, sp: 0.8713, f1: 0.6113\n",
      "                                   tn: 7983, tp: 2233, fn: 1661, fp: 1179\n",
      "  Test  loss: 0.4501, acc: 0.8667, pr: 0.1711, rc: 0.3611, sp: 0.8966, f1: 0.2321\n",
      "                                   tn: 546, tp: 13, fn: 23, fp: 63\n",
      "Epoch: 42\n",
      "  Train loss: 0.5097, acc: 0.7578, pr: 0.5860, rc: 0.6405, sp: 0.8077, f1: 0.6120\n",
      "                                   tn: 7400, tp: 2494, fn: 1400, fp: 1762\n",
      "  Test  loss: 0.4882, acc: 0.7612, pr: 0.1118, rc: 0.4722, sp: 0.7783, f1: 0.1809\n",
      "                                   tn: 474, tp: 17, fn: 19, fp: 135\n",
      "Epoch: 43\n",
      "  Train loss: 0.4963, acc: 0.7820, pr: 0.6374, rc: 0.6243, sp: 0.8491, f1: 0.6308\n",
      "                                   tn: 7779, tp: 2431, fn: 1463, fp: 1383\n",
      "  Test  loss: 0.4586, acc: 0.8217, pr: 0.1504, rc: 0.4722, sp: 0.8424, f1: 0.2282\n",
      "                                   tn: 513, tp: 17, fn: 19, fp: 96\n",
      "Epoch: 44\n",
      "  Train loss: 0.5101, acc: 0.7760, pr: 0.6257, rc: 0.6192, sp: 0.8426, f1: 0.6224\n",
      "                                   tn: 7720, tp: 2411, fn: 1483, fp: 1442\n",
      "  Test  loss: 0.4599, acc: 0.8605, pr: 0.1932, rc: 0.4722, sp: 0.8834, f1: 0.2742\n",
      "                                   tn: 538, tp: 17, fn: 19, fp: 71\n",
      "Epoch: 45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 0.5216, acc: 0.7739, pr: 0.6185, rc: 0.6312, sp: 0.8345, f1: 0.6248\n",
      "                                   tn: 7646, tp: 2458, fn: 1436, fp: 1516\n",
      "  Test  loss: 0.4899, acc: 0.8388, pr: 0.1458, rc: 0.3889, sp: 0.8654, f1: 0.2121\n",
      "                                   tn: 527, tp: 14, fn: 22, fp: 82\n",
      "Epoch: 46\n",
      "  Train loss: 0.4983, acc: 0.7885, pr: 0.6725, rc: 0.5673, sp: 0.8826, f1: 0.6154\n",
      "                                   tn: 8086, tp: 2209, fn: 1685, fp: 1076\n",
      "  Test  loss: 0.4343, acc: 0.8744, pr: 0.1918, rc: 0.3889, sp: 0.9031, f1: 0.2569\n",
      "                                   tn: 550, tp: 14, fn: 22, fp: 59\n",
      "Epoch: 47\n",
      "  Train loss: 0.4930, acc: 0.7893, pr: 0.6849, rc: 0.5437, sp: 0.8937, f1: 0.6062\n",
      "                                   tn: 8188, tp: 2117, fn: 1777, fp: 974\n",
      "  Test  loss: 0.4313, acc: 0.8760, pr: 0.1765, rc: 0.3333, sp: 0.9080, f1: 0.2308\n",
      "                                   tn: 553, tp: 12, fn: 24, fp: 56\n",
      "Epoch: 48\n",
      "  Train loss: 0.4732, acc: 0.8012, pr: 0.7133, rc: 0.5573, sp: 0.9048, f1: 0.6257\n",
      "                                   tn: 8290, tp: 2170, fn: 1724, fp: 872\n",
      "  Test  loss: 0.3999, acc: 0.8899, pr: 0.2222, rc: 0.3889, sp: 0.9195, f1: 0.2828\n",
      "                                   tn: 560, tp: 14, fn: 22, fp: 49\n",
      "Epoch: 49\n",
      "  Train loss: 0.4706, acc: 0.8012, pr: 0.7050, rc: 0.5732, sp: 0.8981, f1: 0.6323\n",
      "                                   tn: 8228, tp: 2232, fn: 1662, fp: 934\n",
      "  Test  loss: 0.3824, acc: 0.9116, pr: 0.3158, rc: 0.5000, sp: 0.9360, f1: 0.3871\n",
      "                                   tn: 570, tp: 18, fn: 18, fp: 39\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "params[\"x_th\"] = -.4\n",
    "for var_th in np.arange(0, 2.1, .25):    \n",
    "    var_th = int(var_th*100)/100    \n",
    "    if var_th !=1.0: \n",
    "        continue\n",
    "    \n",
    "    params[\"var_th\"] = var_th \n",
    "    cond = (features[:, 0] <=params[\"x_th\"]) & (features[:, -1] >=params[\"var_th\"]) #& (ssml[:, 0]>=7)\n",
    "    w = windows[cond]\n",
    "    s = ssml[cond]\n",
    "    l = labels[cond]\n",
    "    f = features[cond]\n",
    "    print(w.shape, s.shape, np.sum(l, axis=0))\n",
    "    \n",
    "    for exclude_subject in range(1,2):\n",
    "        print(\"******** Exclude Subject {} *********\".format(exclude_subject))\n",
    "        print(params)\n",
    "        cond = s[:,0]!=exclude_subject\n",
    "        train_x = w[cond]\n",
    "        train_y = l[cond]\n",
    "        \n",
    "        cond = s[:,0]==exclude_subject\n",
    "        test_x = w[cond]        \n",
    "        test_y = l[cond]\n",
    "\n",
    "        print(\"Train Test shapes: \", train_x.shape, test_x.shape)\n",
    "        print(\"Train Test labels:\", np.sum(train_y, axis=0), np.sum(test_y, axis=0))\n",
    "\n",
    "        path = root_path+'/outputs/bite_detection_lopo/'+bdu.param_string(params)+\"/subject_\"+str(exclude_subject)     \n",
    "        test_pred, train_result, test_result = mtu.train_test_model(train_x, train_y, test_x, test_y, folder_path=path, params=net_params)\n",
    "        path = path+'/results'\n",
    "        bdu.create_directory(path)\n",
    "        np.savetxt(path+'/test_y.csv', test_y, fmt='%.4f', delimiter=',')    \n",
    "        np.savetxt(path+'/test_prediction.csv', test_pred, fmt='%.4f', delimiter=',')\n",
    "        np.savetxt(path+'/train_result.csv', train_result, fmt='%.4f', delimiter=',')\n",
    "        np.savetxt(path+'/test_result.csv', test_result, fmt='%.4f', delimiter=',') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 1.42637718e+00 3.10077518e-02 0.00000000e+00\n",
      "  3.60000000e+01 0.00000000e+00 6.09000000e+02 5.58139535e-02\n",
      "  1.00000000e+00 0.00000000e+00 1.05726872e-01]\n",
      " [1.00000000e+00 1.43103886e+00 3.72093022e-02 1.00000000e+00\n",
      "  3.60000000e+01 0.00000000e+00 6.08000000e+02 5.59006211e-02\n",
      "  1.00000000e+00 1.64203612e-03 1.05882353e-01]\n",
      " [2.00000000e+00 1.52142847e+00 3.10077518e-02 0.00000000e+00\n",
      "  3.60000000e+01 0.00000000e+00 6.09000000e+02 5.58139535e-02\n",
      "  1.00000000e+00 0.00000000e+00 1.05726872e-01]\n",
      " [3.00000000e+00 1.70276618e+00 3.72093022e-02 3.00000000e+00\n",
      "  3.60000000e+01 0.00000000e+00 6.06000000e+02 5.60747664e-02\n",
      "  1.00000000e+00 4.92610837e-03 1.06194690e-01]\n",
      " [4.00000000e+00 1.42516303e+00 3.72093022e-02 0.00000000e+00\n",
      "  3.60000000e+01 0.00000000e+00 6.09000000e+02 5.58139535e-02\n",
      "  1.00000000e+00 0.00000000e+00 1.05726872e-01]\n",
      " [5.00000000e+00 1.54905903e+00 3.41085270e-02 0.00000000e+00\n",
      "  3.60000000e+01 0.00000000e+00 6.09000000e+02 5.58139535e-02\n",
      "  1.00000000e+00 0.00000000e+00 1.05726872e-01]\n",
      " [6.00000000e+00 1.45962977e+00 6.97674453e-02 9.00000000e+00\n",
      "  3.60000000e+01 0.00000000e+00 6.00000000e+02 5.66037736e-02\n",
      "  1.00000000e+00 1.47783251e-02 1.07142857e-01]\n",
      " [7.00000000e+00 1.47132766e+00 6.20155036e-02 1.00000000e+01\n",
      "  3.60000000e+01 0.00000000e+00 5.99000000e+02 5.66929134e-02\n",
      "  1.00000000e+00 1.64203612e-02 1.07302534e-01]\n",
      " [8.00000000e+00 1.37940860e+00 1.22480623e-01 2.90000000e+01\n",
      "  3.60000000e+01 0.00000000e+00 5.80000000e+02 5.84415584e-02\n",
      "  1.00000000e+00 4.76190476e-02 1.10429448e-01]\n",
      " [9.00000000e+00 1.21933293e+00 2.62015492e-01 8.00000000e+01\n",
      "  3.50000000e+01 1.00000000e+00 5.29000000e+02 6.20567376e-02\n",
      "  9.72222222e-01 1.31362890e-01 1.16666667e-01]\n",
      " [1.00000000e+01 9.88013625e-01 5.84496140e-01 1.08000000e+02\n",
      "  3.50000000e+01 1.00000000e+00 5.01000000e+02 6.52985075e-02\n",
      "  9.72222222e-01 1.77339901e-01 1.22377622e-01]\n",
      " [1.10000000e+01 1.11306608e+00 4.43410844e-01 6.20000000e+01\n",
      "  3.50000000e+01 1.00000000e+00 5.47000000e+02 6.01374570e-02\n",
      "  9.72222222e-01 1.01806240e-01 1.13268608e-01]\n",
      " [1.20000000e+01 8.04858804e-01 7.87596881e-01 2.77000000e+02\n",
      "  2.40000000e+01 1.20000000e+01 3.32000000e+02 6.74157303e-02\n",
      "  6.66666667e-01 4.54844007e-01 1.22448980e-01]\n",
      " [1.30000000e+01 8.46647680e-01 7.64341056e-01 1.31000000e+02\n",
      "  3.00000000e+01 6.00000000e+00 4.78000000e+02 5.90551181e-02\n",
      "  8.33333333e-01 2.15106732e-01 1.10294118e-01]\n",
      " [1.40000000e+01 7.10501373e-01 8.71317804e-01 3.73000000e+02\n",
      "  1.20000000e+01 2.40000000e+01 2.36000000e+02 4.83870968e-02\n",
      "  3.33333333e-01 6.12479475e-01 8.45070423e-02]\n",
      " [1.50000000e+01 6.86828375e-01 8.63565862e-01 4.90000000e+02\n",
      "  1.00000000e+01 2.60000000e+01 1.19000000e+02 7.75193798e-02\n",
      "  2.77777778e-01 8.04597701e-01 1.21212121e-01]\n",
      " [1.60000000e+01 6.88712716e-01 8.49612415e-01 5.08000000e+02\n",
      "  1.10000000e+01 2.50000000e+01 1.01000000e+02 9.82142857e-02\n",
      "  3.05555556e-01 8.34154351e-01 1.48648649e-01]\n",
      " [1.70000000e+01 7.60876119e-01 7.44186044e-01 3.20000000e+02\n",
      "  1.90000000e+01 1.70000000e+01 2.89000000e+02 6.16883117e-02\n",
      "  5.27777778e-01 5.25451560e-01 1.10465116e-01]\n",
      " [1.80000000e+01 6.25823498e-01 8.60465109e-01 5.16000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 9.30000000e+01 8.82352941e-02\n",
      "  2.50000000e-01 8.47290640e-01 1.30434783e-01]\n",
      " [1.90000000e+01 7.38630712e-01 7.70542622e-01 3.13000000e+02\n",
      "  2.10000000e+01 1.50000000e+01 2.96000000e+02 6.62460568e-02\n",
      "  5.83333333e-01 5.13957307e-01 1.18980170e-01]\n",
      " [2.00000000e+01 6.09935462e-01 8.83720934e-01 5.25000000e+02\n",
      "  6.00000000e+00 3.00000000e+01 8.40000000e+01 6.66666667e-02\n",
      "  1.66666667e-01 8.62068966e-01 9.52380952e-02]\n",
      " [2.10000000e+01 6.02523386e-01 8.71317804e-01 5.14000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 9.50000000e+01 8.65384615e-02\n",
      "  2.50000000e-01 8.44006568e-01 1.28571429e-01]\n",
      " [2.20000000e+01 6.59553051e-01 8.49612415e-01 4.51000000e+02\n",
      "  1.30000000e+01 2.30000000e+01 1.58000000e+02 7.60233918e-02\n",
      "  3.61111111e-01 7.40558292e-01 1.25603865e-01]\n",
      " [2.30000000e+01 6.45678043e-01 8.74418616e-01 5.03000000e+02\n",
      "  8.00000000e+00 2.80000000e+01 1.06000000e+02 7.01754386e-02\n",
      "  2.22222222e-01 8.25944171e-01 1.06666667e-01]\n",
      " [2.40000000e+01 6.08760178e-01 8.71317804e-01 5.17000000e+02\n",
      "  1.10000000e+01 2.50000000e+01 9.20000000e+01 1.06796117e-01\n",
      "  3.05555556e-01 8.48932677e-01 1.58273381e-01]\n",
      " [2.50000000e+01 6.67463422e-01 8.97674441e-01 4.72000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 1.37000000e+02 6.16438356e-02\n",
      "  2.50000000e-01 7.75041051e-01 9.89010989e-02]\n",
      " [2.60000000e+01 6.18695796e-01 8.86821687e-01 5.02000000e+02\n",
      "  1.00000000e+01 2.60000000e+01 1.07000000e+02 8.54700855e-02\n",
      "  2.77777778e-01 8.24302135e-01 1.30718954e-01]\n",
      " [2.70000000e+01 5.89274466e-01 8.83720934e-01 4.96000000e+02\n",
      "  1.00000000e+01 2.60000000e+01 1.13000000e+02 8.13008130e-02\n",
      "  2.77777778e-01 8.14449918e-01 1.25786164e-01]\n",
      " [2.80000000e+01 5.62067807e-01 8.96124005e-01 5.29000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 8.00000000e+01 1.01123596e-01\n",
      "  2.50000000e-01 8.68637110e-01 1.44000000e-01]\n",
      " [2.90000000e+01 5.84091961e-01 8.75968993e-01 5.25000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 8.40000000e+01 9.67741935e-02\n",
      "  2.50000000e-01 8.62068966e-01 1.39534884e-01]\n",
      " [3.00000000e+01 5.56252658e-01 8.88372064e-01 5.29000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 8.00000000e+01 1.01123596e-01\n",
      "  2.50000000e-01 8.68637110e-01 1.44000000e-01]\n",
      " [3.10000000e+01 5.55335045e-01 8.83720934e-01 5.28000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 8.10000000e+01 1.00000000e-01\n",
      "  2.50000000e-01 8.66995074e-01 1.42857143e-01]\n",
      " [3.20000000e+01 5.19085467e-01 8.93023252e-01 5.30000000e+02\n",
      "  9.00000000e+00 2.70000000e+01 7.90000000e+01 1.02272727e-01\n",
      "  2.50000000e-01 8.70279146e-01 1.45161290e-01]\n",
      " [3.30000000e+01 5.90006113e-01 8.75968993e-01 4.63000000e+02\n",
      "  1.60000000e+01 2.00000000e+01 1.46000000e+02 9.87654321e-02\n",
      "  4.44444444e-01 7.60262726e-01 1.61616162e-01]\n",
      " [3.40000000e+01 5.23498595e-01 8.74418616e-01 5.18000000e+02\n",
      "  1.30000000e+01 2.30000000e+01 9.10000000e+01 1.25000000e-01\n",
      "  3.61111111e-01 8.50574713e-01 1.85714286e-01]\n",
      " [3.50000000e+01 5.41376889e-01 9.03875947e-01 4.75000000e+02\n",
      "  1.30000000e+01 2.30000000e+01 1.34000000e+02 8.84353741e-02\n",
      "  3.61111111e-01 7.79967159e-01 1.42076503e-01]\n",
      " [3.60000000e+01 5.07163048e-01 8.96124005e-01 5.13000000e+02\n",
      "  1.20000000e+01 2.40000000e+01 9.60000000e+01 1.11111111e-01\n",
      "  3.33333333e-01 8.42364532e-01 1.66666667e-01]\n",
      " [3.70000000e+01 5.24312794e-01 8.94573629e-01 4.92000000e+02\n",
      "  1.50000000e+01 2.10000000e+01 1.17000000e+02 1.13636364e-01\n",
      "  4.16666667e-01 8.07881773e-01 1.78571429e-01]\n",
      " [3.80000000e+01 5.02228796e-01 8.94573629e-01 5.01000000e+02\n",
      "  1.50000000e+01 2.10000000e+01 1.08000000e+02 1.21951220e-01\n",
      "  4.16666667e-01 8.22660099e-01 1.88679245e-01]\n",
      " [3.90000000e+01 4.92781043e-01 9.14728701e-01 4.96000000e+02\n",
      "  1.30000000e+01 2.30000000e+01 1.13000000e+02 1.03174603e-01\n",
      "  3.61111111e-01 8.14449918e-01 1.60493827e-01]\n",
      " [4.00000000e+01 4.99624312e-01 8.97674441e-01 4.87000000e+02\n",
      "  1.80000000e+01 1.80000000e+01 1.22000000e+02 1.28571429e-01\n",
      "  5.00000000e-01 7.99671593e-01 2.04545455e-01]\n",
      " [4.10000000e+01 5.52969694e-01 8.51162791e-01 4.28000000e+02\n",
      "  2.30000000e+01 1.30000000e+01 1.81000000e+02 1.12745098e-01\n",
      "  6.38888889e-01 7.02791461e-01 1.91666667e-01]\n",
      " [4.20000000e+01 5.72475374e-01 8.18604648e-01 4.25000000e+02\n",
      "  2.10000000e+01 1.50000000e+01 1.84000000e+02 1.02439024e-01\n",
      "  5.83333333e-01 6.97865353e-01 1.74273859e-01]\n",
      " [4.30000000e+01 4.92370427e-01 9.02325571e-01 5.12000000e+02\n",
      "  2.10000000e+01 1.50000000e+01 9.70000000e+01 1.77966102e-01\n",
      "  5.83333333e-01 8.40722496e-01 2.72727273e-01]\n",
      " [4.40000000e+01 5.06713390e-01 8.71317804e-01 4.99000000e+02\n",
      "  2.00000000e+01 1.60000000e+01 1.10000000e+02 1.53846154e-01\n",
      "  5.55555556e-01 8.19376026e-01 2.40963855e-01]\n",
      " [4.50000000e+01 4.75012958e-01 8.85271311e-01 5.21000000e+02\n",
      "  2.00000000e+01 1.60000000e+01 8.80000000e+01 1.85185185e-01\n",
      "  5.55555556e-01 8.55500821e-01 2.77777778e-01]\n",
      " [4.60000000e+01 4.80443150e-01 8.88372064e-01 5.14000000e+02\n",
      "  2.00000000e+01 1.60000000e+01 9.50000000e+01 1.73913043e-01\n",
      "  5.55555556e-01 8.44006568e-01 2.64900662e-01]\n",
      " [4.70000000e+01 4.73893553e-01 8.65116298e-01 4.98000000e+02\n",
      "  2.00000000e+01 1.60000000e+01 1.11000000e+02 1.52671756e-01\n",
      "  5.55555556e-01 8.17733990e-01 2.39520958e-01]\n",
      " [4.80000000e+01 4.90187258e-01 8.75968993e-01 5.14000000e+02\n",
      "  2.10000000e+01 1.50000000e+01 9.50000000e+01 1.81034483e-01\n",
      "  5.83333333e-01 8.44006568e-01 2.76315789e-01]\n",
      " [4.90000000e+01 4.72280532e-01 8.85271311e-01 4.94000000e+02\n",
      "  2.30000000e+01 1.30000000e+01 1.15000000e+02 1.66666667e-01\n",
      "  6.38888889e-01 8.11165846e-01 2.64367816e-01]]\n"
     ]
    }
   ],
   "source": [
    "test_result = np.array(test_result)\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
