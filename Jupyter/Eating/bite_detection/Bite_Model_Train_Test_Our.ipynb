{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\asm\\continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'my_tensorflow_cnn_utils' from 'C:/ASM/Dropbox/Developments/Jupyter/Eating/myutils\\\\my_tensorflow_cnn_utils.py'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util_path = 'C:/ASM/Dropbox/Developments/Jupyter/Eating/myutils' if 'C:' in os.getcwd() else './myutils'\n",
    "sys.path.append(util_path)\n",
    "import my_file_utils as mfileu\n",
    "import my_classification_utils as mclfu\n",
    "import my_steven_lab_utils as mslabu\n",
    "import my_tensorflow_cnn_utils as mcnnu\n",
    "import my_tensorflow_lstm_utils as mlstmu\n",
    "import my_tensorflow_dense_utils as mdenseu\n",
    "importlib.reload(mcnnu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win size: 80 , Step size: 2 , gx_th bite, free: -0.25 0\n"
     ]
    }
   ],
   "source": [
    "win_size, step_size = 5*16, 2\n",
    "axis_count, feature_count = 3, 32\n",
    "gx_th_bite, gx_th_free = -0.25, 0\n",
    "print(\"Win size:\", win_size, \", Step size:\", step_size, ', gx_th bite, free:', gx_th_bite, gx_th_free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mid_gx(ds, indices, win_size):\n",
    "    count = len(indices)\n",
    "    x = np.zeros((count, ))    \n",
    "    for i in range(count):\n",
    "        subj, sess, ix = indices[i, 0], indices[i, 1], indices[i, 2]+win_size//2\n",
    "        x[i] = ds[subj][sess][ix, -3]                \n",
    "    return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variance_accel(ds, indices, win_size):   \n",
    "    count = len(indices)\n",
    "    v = np.zeros((count, ))    \n",
    "    for i in range(count):\n",
    "        subj, sess, ix = indices[i, 0], indices[i, 1], indices[i, 2]\n",
    "        v[i] = np.sum(np.var(ds[subj][sess][ix:ix+win_size, 1:4], axis=0))                \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_data(ds, fs, indices, win_size, offset=0):\n",
    "    count = len(indices)\n",
    "    w = np.zeros((count, win_size, 3))    \n",
    "    features = np.zeros((count, 32))    \n",
    "    \n",
    "    for i in range(count):\n",
    "        subj, sess, ix = indices[i, 0], indices[i, 1], indices[i, 2]        \n",
    "        w[i, :, :] = ds[subj][sess][ix:ix+win_size, -3:]\n",
    "        features[i, :] = fs[subj][sess][ix//2, 1:]\n",
    "        assert ix == fs[subj][sess][ix//2, 0]\n",
    "    \n",
    "    return w, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_lab(indices, a, win_size):\n",
    "    a = a[a[:, 3]!=2, :] # right hand only    \n",
    "    wcount, acount = len(indices), len(a)  \n",
    "    \n",
    "    si = indices\n",
    "    q1 = si + win_size//4\n",
    "    mi = si + 2*win_size//4    \n",
    "    q3 = si + 3*win_size//4    \n",
    "    \n",
    "    labels=np.zeros((wcount,))    \n",
    "    for i in range(acount):         \n",
    "        if a[i, 2]==1: #bite\n",
    "            ix = a[i, 0]\n",
    "            cond = (q1<=ix) & (ix<=q3)\n",
    "            label = 1\n",
    "        else:#sip\n",
    "            ix1 = a[i, 0]\n",
    "            ix2 = a[i, 1]\n",
    "            cond = (mi>=ix1) & (mi<=ix2)\n",
    "            label = 2\n",
    "        \n",
    "        labels[cond] = label\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_free(indices, a, win_size):    \n",
    "    wcount, acount = len(indices), len(a)      \n",
    "    mi = indices + win_size//2    \n",
    "    \n",
    "    labels=np.zeros((wcount,))    \n",
    "    for i in range(acount):                 \n",
    "        ix1 = a[i, 0]\n",
    "        ix2 = a[i, 1]\n",
    "        cond = (mi>=ix1) & (mi<=ix2)\n",
    "        labels[cond] = a[i, 2]\n",
    "        \n",
    "    return labels.astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_indices_lab(exclude_subj=-1):\n",
    "    all_indices = np.zeros((0, 4))\n",
    "    for subj in range(len(dlab)):\n",
    "        if subj==exclude_subj:\n",
    "            continue\n",
    "        for sess in range(len(dlab[subj])):\n",
    "            d = ds[subj][sess]\n",
    "            a = annots[subj][sess]\n",
    "\n",
    "            count = (len(d)-win_size)//step_size + 1\n",
    "            indices = np.array(list(range(count)))*2\n",
    "            labels = get_labels_lab(indices, a, win_size)\n",
    "            labels[labels==2] = 0\n",
    "\n",
    "            ix = np.zeros((count, 4))\n",
    "            ix[:, 0] = subj\n",
    "            ix[:, 1] = sess\n",
    "            ix[:, 2] = indices\n",
    "            ix[:, 3] = labels\n",
    "            all_indices = np.concatenate((all_indices, ix))\n",
    "\n",
    "    all_indices = all_indices.astype(int)\n",
    "    assert np.sum(all_indices[:, -1]>1) == 0\n",
    "    assert np.sum(all_indices[:, -1]<0) == 0\n",
    "    print(\"All indices shape, neg, bite:\", len(all_indices), np.sum(all_indices[:, -1]==0), np.sum(all_indices[:, -1]==1))\n",
    "    return all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_indices_free(subj):\n",
    "    subj_indices = np.zeros((0, 4))    \n",
    "    for sess in range(len(dfree[subj])):\n",
    "        d = ds[subj][sess]\n",
    "        a = annots[subj][sess]\n",
    "        \n",
    "        count = (len(d)-win_size)//step_size + 1\n",
    "        indices = np.array(list(range(count)))*2\n",
    "        labels = get_labels_free(indices, a, win_size)\n",
    "                \n",
    "        ix = np.zeros((count, 4))\n",
    "        ix[:, 0] = subj\n",
    "        ix[:, 1] = sess\n",
    "        ix[:, 2] = indices\n",
    "        ix[:, 3] = labels\n",
    "        subj_indices = np.concatenate((subj_indices, ix))        \n",
    "    \n",
    "    subj_indices = subj_indices.astype(int)        \n",
    "    return all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_indices_free(exclude_subj=-1):\n",
    "    all_indices = np.zeros((0, 4))    \n",
    "    for subj in range(len(dfree)):\n",
    "        if subj == exclude_subj:\n",
    "            continue            \n",
    "        ix = get_subject_indices_free(subj)\n",
    "        all_indices = np.concatenate((all_indices, ix))\n",
    "            \n",
    "    all_indices = all_indices.astype(int)    \n",
    "    return all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj, num_epochs, train_test = 100, 50, 'train'\n",
    "if 'C:' not in mfileu.get_path():    \n",
    "    subj, num_epochs, train_test = int(sys.argv[1]), int(sys.argv[2]), sys.argv[3]\n",
    "    \n",
    "assert train_test in ['train', 'test_bite', 'retrain_o', 'retrain_p', 'test_free']\n",
    "params={}\n",
    "params['learning_rate'] = 0.001\n",
    "params['num_epochs'] = num_epochs\n",
    "params['batch_size'] = 128\n",
    "params['keep_prob_val'] = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_test=='train':\n",
    "    dlab = mfileu.read_file('data', 'lab_data_steven.pkl')\n",
    "    ds, _, annots = mslabu.separate_right_left_annots(dlab)\n",
    "    fs = mfileu.read_file('features', 'lab_features_steven_right.pkl')\n",
    "else:\n",
    "    ds = mfileu.read_file('data', 'free_data_steven_right.pkl')\n",
    "    annots = mfileu.read_file('data', 'free_data_steven_annots.pkl')\n",
    "    fs = mfileu.read_file('features', 'free_features_steven_right.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(train_indices, val_indices, params, model_path_dest=None, model_path_src=None, test_indices=[]):\n",
    "    learning_rate = params['learning_rate']\n",
    "    num_epochs = params['num_epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    keep_prob_val = params['keep_prob_val']\n",
    "    print(\"****** Learning rate \", learning_rate)\n",
    "    \n",
    "    #print_out, sys.stdout = sys.stdout, open(os.devnull, 'w')    \n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, win_size, axis_count], name=\"x\")    \n",
    "    features = tf.placeholder(tf.float32, [None, feature_count], name=\"features\")\n",
    "    y = tf.placeholder(tf.float32, [None, 1], name=\"y\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")    \n",
    "    \n",
    "    cnn_out = mcnnu.gravity_conv_net(x, name=\"gravity_conv_net\")\n",
    "    lstm_out_fw, lstm_out_bw = mlstmu.multi_layer_biLSTM(cnn_out, batch_size=batch_size, n_hidden=32, n_layer=1)\n",
    "    lstm_out = tf.concat([lstm_out_fw[:, -1, :], lstm_out_bw[:, 0, :]], axis =1)\n",
    "    print(\"Lstm out shapes(fw, bw): \", lstm_out_fw.get_shape().as_list(), lstm_out_bw.get_shape().as_list())\n",
    "    print(\"Lstm out shape final: \", lstm_out.get_shape().as_list())\n",
    "    \n",
    "    mlp_in = tf.concat([features, lstm_out], axis =1)\n",
    "    print(\"MLP Input shape final: \", mlp_in.get_shape().as_list())\n",
    "    drop_out = tf.nn.dropout(mlp_in, keep_prob=keep_prob, name=\"dropout_0\")\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        mlp = mdenseu.fc_layer(drop_out, 64, name=\"mlp_\"+str(i), activation='relu')\n",
    "        drop_out = tf.nn.dropout(mlp, keep_prob=keep_prob, name=\"dropout_\"+str(i))\n",
    "        \n",
    "    logits_bite = mdenseu.fc_layer(drop_out, 1, name=\"logits_bite\")    \n",
    "    print(\"logit bite shape in out: \", drop_out.get_shape().as_list(), logits_bite.get_shape().as_list() )\n",
    "    \n",
    "    for i in range(6, 11):\n",
    "        mlp = mdenseu.fc_layer(drop_out, 64, name=\"mlp_\"+str(i), activation='relu')\n",
    "        drop_out = tf.nn.dropout(mlp, keep_prob=keep_prob, name=\"dropout_\"+str(i))\n",
    "    \n",
    "    logits_free = mdenseu.fc_layer(drop_out, 1, name=\"logits_free\")    \n",
    "    print(\"logit free shape in, out: \", drop_out.get_shape().as_list(), logits_free.get_shape().as_list() )\n",
    "        \n",
    "    prediction_bite = tf.nn.sigmoid(logits_bite, name=\"prediction_bite\")\n",
    "    correct_prediction_bite = tf.equal(tf.greater(prediction_bite, 0.5), tf.equal(y,1), name=\"correct_prediction_bite\")\n",
    "    accuracy_bite = tf.reduce_mean(tf.cast(correct_prediction_bite, tf.float32), name=\"accuracy_bite\")\n",
    "    \n",
    "    prediction_free = tf.nn.sigmoid(logits_free, name=\"prediction_free\")\n",
    "    correct_prediction_free = tf.equal(tf.greater(prediction_free, 0.5), tf.equal(y,1), name=\"correct_prediction_free\")\n",
    "    accuracy_free = tf.reduce_mean(tf.cast(correct_prediction_free, tf.float32), name=\"accuracy_free\")\n",
    "        \n",
    "    loss_op_bite = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_bite, labels=y), name=\"loss_op_bite\")        \n",
    "    loss_op_free = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_free, labels=y), name=\"loss_op_free\")        \n",
    "    \n",
    "    train_step_bite = tf.train.AdamOptimizer(learning_rate).minimize(loss_op_bite, name=\"train_step_bite\")\n",
    "    train_step_free = tf.train.AdamOptimizer(learning_rate).minimize(loss_op_free, name=\"train_step_free\")\n",
    "\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    #sys.stdout = print_out\n",
    "    ########## Train and then save the model ########################\n",
    "    if len(train_indices)>0:\n",
    "        assert train_test in ['train', 'retrain_o', 'retrain_p']\n",
    "        if train_test=='train':                        \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            train_step, loss_op, accuracy, prediction = train_step_bite, loss_op_bite, accuracy_bite, prediction_bite\n",
    "            print(\"*********** Global variable initialized for training\")\n",
    "        else:            \n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, model_path_src+'/model')\n",
    "            print(\" ********* Model Loaded for retrain!\")\n",
    "            train_step, loss_op, accuracy, prediction = train_step_free, loss_op_free, accuracy_free, prediction_free\n",
    "            \n",
    "        train_indices, _ = mclfu.adjust_for_batch_size(train_indices, train_indices, batch_size)\n",
    "\n",
    "        train_count = len(train_indices)\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Epoch:\", epoch)\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for ix in range(0, train_count, batch_size):                                            \n",
    "                batch_x, batch_features = get_window_data(ds, fs, train_indices[ix:ix+batch_size], win_size)                 \n",
    "                batch_y = train_indices[ix:ix+batch_size, -1].reshape((-1,1))                 \n",
    "                sess.run(train_step, feed_dict={x:batch_x, features:batch_features, y:batch_y, keep_prob:keep_prob_val})                \n",
    "\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={x:batch_x, y:batch_y, features:batch_features, keep_prob:1.0})        \n",
    "                total_loss+= loss*batch_size\n",
    "                total_acc += acc*batch_size                \n",
    "            print('  Train loss: {:.4f}, acc: {:.4f}'.format(total_loss/train_count, total_acc/train_count))\n",
    "\n",
    "            val_count = len(val_indices)\n",
    "            if val_count>0:            \n",
    "                val_indices, _ = mclfu.adjust_for_batch_size(val_indices, val_indices, batch_size)\n",
    "                total_loss, total_acc = 0, 0\n",
    "                for ix in range(0, val_count, batch_size):                \n",
    "                    batch_x, batch_features = get_window_data(ds, fs, val_indices[ix:ix+batch_size], win_size)\n",
    "                    batch_y = val_indices[ix:ix+batch_size, -1].reshape((-1,1))                      \n",
    "                    loss, acc = sess.run([loss_op, accuracy], feed_dict={x:batch_x, y:batch_y, features:batch_features, keep_prob:1.0})                \n",
    "                    total_loss+= loss*batch_size\n",
    "                    total_acc += acc*batch_size                \n",
    "                print('  Val loss: {:.4f}, acc: {:.4f}'.format(total_loss/val_count, total_acc/val_count))\n",
    "\n",
    "        print('!!!!!!!!!!!!!!! Optimization Finished !!!!!!!!!!!!!!!!!')\n",
    "        \n",
    "        res = []\n",
    "        test_count_original = len(test_indices)        \n",
    "        if test_count_original>0:            \n",
    "            test_indices, _ = mclfu.adjust_for_batch_size(test_indices, test_indices, batch_size)\n",
    "            test_count = len(test_indices)\n",
    "            res = np.zeros((test_count, 1))\n",
    "\n",
    "            for ix in range(0, test_count, batch_size):                \n",
    "                batch_x, batch_features = get_window_data(ds, fs, test_indices[ix:ix+batch_size], win_size)\n",
    "                batch_y = test_indices[ix:ix+batch_size, -1].reshape((-1,1))  \n",
    "                pred = sess.run([prediction], feed_dict={x: batch_x, y:batch_y, features:batch_features, keep_prob:1.0})            \n",
    "                res[ix:ix+batch_size, 0] = np.array(pred).reshape((-1, ))\n",
    "\n",
    "            res = res[:test_count_original, :]       \n",
    "        \n",
    "        if model_path_dest:\n",
    "            saver = tf.train.Saver()            \n",
    "            mfileu.create_directory(model_path_dest)\n",
    "            saver.save(sess, model_path_dest+'/model')    \n",
    "            print(\"Model Saved!\")\n",
    "        \n",
    "    ########## Restore the model and then Test  ########################\n",
    "    else:\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, model_path_src+'/model')\n",
    "        print(\"Model Loaded for test!\")\n",
    "        \n",
    "        test_count_original = len(test_indices)        \n",
    "        test_indices, _ = mclfu.adjust_for_batch_size(test_indices, test_indices, batch_size)\n",
    "        test_count = len(test_indices)\n",
    "        res = np.zeros((test_count, 1))\n",
    "        \n",
    "        for ix in range(0, test_count, batch_size):                \n",
    "            batch_x, batch_features = get_window_data(ds, fs, test_indices[ix:ix+batch_size], win_size)\n",
    "            batch_y = test_indices[ix:ix+batch_size, -1].reshape((-1,1))  \n",
    "            pred = sess.run([prediction], feed_dict={x: batch_x, y:batch_y, keep_prob:1.0})            \n",
    "            res[ix:ix+batch_size, 0] = np.array(pred).reshape((-1, ))\n",
    "        \n",
    "        res = res[:test_count_original, :]        \n",
    "        \n",
    "    sess.close() \n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================  Train/Test: train =============================\n",
      "\n",
      "============ Subject, Epochs, Win Size: 0, 1, 80 =============\n"
     ]
    }
   ],
   "source": [
    "print(\"=============================  Train/Test: {} =============================\".format(train_test))\n",
    "print(\"\\n============ Subject, Epochs, Win Size: {}, {}, {} =============\".format(subj, num_epochs, win_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = mfileu.get_path()\n",
    "model_folder_src = path+'/our_bite_models'\n",
    "model_folder_dest = path+'/our_bite_models'\n",
    "result_folder = 'our_test_proba_retrain_personal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.....\n",
      "All indices shape, neg, bite: 1914621 1895991 18630\n",
      "Indices summary after subject filter total, neg, pos: 1914621 1895991 18630\n",
      "Indices summary after gx filter total, neg, pos: 619911 604172 15739\n",
      "train, val shapes:  (557919, 4) (61992, 4) 14165 1574\n",
      "****** Learning rate  0.001\n",
      "Inside one_3dsensor_conv_net:  all_sensor_net , x_shape [None, 80, 3]\n",
      "  Axis count:  3\n",
      "  Conv_1, maxpool_1 shape:  [None, 78, 1, 32] [None, 37, 1, 32]\n",
      "  Conv_3, maxpool_2 shape:  [None, 35, 1, 32] [None, 16, 1, 32]\n",
      "Lstm out shapes(fw, bw):  [128, 16, 16] [128, 16, 16]\n",
      "Lstm out shape final:  [128, 32]\n",
      "MLP Input shape final:  [128, 64]\n",
      "Relu activation\n",
      "Relu activation\n",
      "Relu activation\n",
      "Relu activation\n",
      "Relu activation\n",
      "logit bite shape in out:  [128, 32] [128, 1]\n",
      "Relu activation\n",
      "Relu activation\n",
      "Relu activation\n",
      "Relu activation\n",
      "Relu activation\n",
      "logit free shape in, out:  [128, 32] [128, 1]\n",
      "Epoch: 0\n",
      "  Train loss: 0.1428, acc: 0.9746\n",
      "  Val loss: 0.1004, acc: 0.9760\n",
      "!!!!!!!!!!!!!!! Optimization Finished !!!!!!!!!!!!!!!!!\n",
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "if train_test == 'train':\n",
    "    print(\"Training.....\")    \n",
    "    indices = get_all_indices_lab(exclude_subj=subj)\n",
    "    \n",
    "    assert np.sum(indices[:, 0]==subj) == 0    \n",
    "    assert np.sum(indices[:, -1]>1) == 0\n",
    "    assert np.sum(indices[:, -1]<0) == 0\n",
    "    print(\"Train Indices summary after subject filter total, neg, pos:\", len(indices), np.sum(indices[:, -1]==0), np.sum(indices[:, -1]==1))\n",
    "    \n",
    "    gx = get_mid_gx(ds, indices, win_size=win_size)\n",
    "    indices = indices[gx<=gx_th_bite]    \n",
    "    print(\"Train Indices summary after gx filter total, neg, pos:\", len(indices), np.sum(indices[:, -1]==0), np.sum(indices[:, -1]==1))\n",
    "    \n",
    "    indices = shuffle(indices)    \n",
    "    train_indices, val_indices = train_test_split(indices, test_size=0.1, stratify=indices[:, -1])\n",
    "    \n",
    "    print(\"train, val shapes: \", train_indices.shape, val_indices.shape, np.sum(train_indices[:, -1]), np.sum(val_indices[:, -1]))\n",
    "    \n",
    "    model_path_dest = \"our_bite_models/subj_\"+str(subj)\n",
    "    train_test_model(train_indices, val_indices, params, model_path_dest=model_path_dest)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if train_test == 'retrain_o':\n",
    "    print(\"Re-Training.....\")\n",
    "    params['learning_rate'] = 0.0001\n",
    "    \n",
    "    indices = get_all_indices_free(exclude_subj=subj)\n",
    "    assert np.sum(indices[:,0]==subj)==0\n",
    "    \n",
    "    indices[indices[:,-1]==2, -1] = 1\n",
    "    indices[indices[:,-1]==3, -1] = 0\n",
    "    assert np.sum((indices[:, -1]>1) | (indices[:, -1]<0)) == 0    \n",
    "    print(\"Train Indices summary total, neg, pos:\", len(indices), np.sum(indices[:, -1]==0), np.sum(indices[:, -1]==1))\n",
    "\n",
    "    gx = get_mid_gx(ds, indices, win_size=win_size)\n",
    "    indices = indices[gx<=gx_th]    \n",
    "    print(\"Train Indices summary after gx filter total, neg, pos:\", len(indices), np.sum(indices[:, -1]==0), np.sum(indices[:, -1]==1))\n",
    "\n",
    "    indices = shuffle(indices)    \n",
    "    train_indices, val_indices = train_test_split(indices, test_size=0.1, stratify=indices[:, -1])\n",
    "    print(\"train, val shapes: \", train_indices.shape, val_indices.shape, np.sum(train_indices[:, -1]), np.sum(val_indices[:, -1]))\n",
    "\n",
    "    test_indices = get_subject_indices_free(subj)\n",
    "    print(\"Test Indices summary total, neg, pos:\", len(test_indices), np.sum(test_indices[:, -1]==0), np.sum(test_indices[:, -1]==1))\n",
    "    \n",
    "    gx = get_mid_gx(ds, test_indices, win_size=win_size)\n",
    "    test_indices = test_indices[gx<=gx_th]    \n",
    "    print(\"Test Indices summary after gx filter total, neg, pos:\", len(test_indices), np.sum(test_indices[:, -1]==0), np.sum(test_indices[:, -1]==1))\n",
    "    \n",
    "    src_subj = subj+2 if subj<5 else 100            \n",
    "    model_path_src = \"our_bite_models/subj_\"+str(src_subj) \n",
    "    model_path_dest = \"our_free_models_general/subj_\"+str(subj)\n",
    "    pred = train_test_model(train_indices, val_indices, params, model_path_dest=model_path_dest, model_path_src=model_path_src, test_indices=test_indices)\n",
    "\n",
    "    print(\"Prediction shape: \", pred.shape, np.sum(pred), np.sum(pred>=0.5))            \n",
    "    assert len(pred)==len(test_indices)\n",
    "    pred = np.concatenate((test_indices[:, :3], pred), axis=1)\n",
    "    print(\"Prediction shape: \", pred.shape, np.sum(pred[:, 1]), np.sum(pred[:, 1]>=0.5))\n",
    "    \n",
    "    mfileu.write_file('our_free_results_general', 'window_free_'+str(subj)+\".pkl\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_test == 'retrain_p':\n",
    "    print(\"Re-Training.....\")\n",
    "    params['learning_rate'] = 0.0001\n",
    "    \n",
    "    subj_indices = get_subject_indices_free(subj)\n",
    "    assert np.sum(subj_indices[:,0]!=subj)==0\n",
    "    print(\"Total subject indices: \", subj_indices.shape)\n",
    "    \n",
    "    for sess in range(len(ds[subj])):        \n",
    "        indices = subj_indices[subj_indices[:,1]!=sess]\n",
    "\n",
    "        indices[indices[:,-1]==2, -1] = 1\n",
    "        indices[indices[:,-1]==3, -1] = 0\n",
    "        assert np.sum((indices[:, -1]>1) | (indices[:, -1]<0)) == 0    \n",
    "        print(\"Train Indices summary total, neg, pos:\", len(indices), np.sum(indices[:, -1]==0), np.sum(indices[:, -1]==1))\n",
    "\n",
    "        gx = get_mid_gx(ds, indices, win_size=win_size)\n",
    "        indices = indices[gx<=gx_th]    \n",
    "        print(\"Train Indices summary after gx filter total, neg, pos:\", len(indices), np.sum(indices[:, -1]==0), np.sum(indices[:, -1]==1))\n",
    "\n",
    "        indices = shuffle(indices)    \n",
    "        train_indices, val_indices = train_test_split(indices, test_size=0.1, stratify=indices[:, -1])\n",
    "        print(\"train, val shapes: \", train_indices.shape, val_indices.shape, np.sum(train_indices[:, -1]), np.sum(val_indices[:, -1]))\n",
    "\n",
    "        gx = get_mid_gx(ds, subj_indices, win_size=win_size)\n",
    "        test_indices = subj_indices[gx<=gx_th]    \n",
    "        print(\"Test Indices summary after gx filter total, neg, pos:\", len(test_indices), np.sum(test_indices[:, -1]==0), np.sum(test_indices[:, -1]==1))\n",
    "\n",
    "        model_path_src = \"our_bite_models_general/subj_\"+str(subj) \n",
    "        model_path_dest = \"our_free_models_personal/subj_\"+str(subj)\n",
    "        pred = train_test_model(train_indices, val_indices, params, model_path_dest=model_path_dest, model_path_src=model_path_src, test_indices=test_indices)\n",
    "\n",
    "        print(\"Prediction shape: \", pred.shape, np.sum(pred), np.sum(pred>=0.5))            \n",
    "        assert len(pred)==len(test_indices)\n",
    "        pred = np.concatenate((test_indices[:, :3], pred), axis=1)\n",
    "        print(\"Prediction shape: \", pred.shape, np.sum(pred[:, 1]), np.sum(pred[:, 1]>=0.5))\n",
    "\n",
    "        mfileu.write_file('our_free_results_personal', 'window_free_'+str(subj)+\"_\"+str(subj)+\".pkl\", pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test = 'test'\n",
    "if train_test=='test':\n",
    "    print(\"Testing.....\")    \n",
    "        \n",
    "    for subj in range(11):\n",
    "        indices = get_subject_indices_free(subj)\n",
    "        print(\"Subj, sess, indices shape: \", subj, sess, indices.shape)\n",
    "\n",
    "        gx = get_mid_gx(ds, indices, win_size=win_size)\n",
    "        indices = indices[gx<=gx_th]    \n",
    "        print(\"After gx filter indices shape: \", indices.shape)\n",
    "\n",
    "        src_subj = subj+2 if subj<5 else 100            \n",
    "        model_path_src = \"our_bite_models/subj_\"+str(src_subj)            \n",
    "        pred = train_test_model([], indices, params, )            \n",
    "        print(\"Prediction shape: \", pred.shape, np.sum(pred), np.sum(pred>=0.5))            \n",
    "        assert len(pred)==len(indices)\n",
    "\n",
    "        pred = pred.reshape((-1, 1))\n",
    "        pred = np.concatenate((ix, pred), axis=1)\n",
    "        print(\"Prediction shape: \", pred.shape, np.sum(pred[:, 1]), np.sum(pred[:, 1]>=0.5))            \n",
    "\n",
    "        mfileu.write_file('our_bite_results', 'bite_free_'+str(subj)+\".pkl\", pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
