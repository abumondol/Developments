{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************* Bite Detection ***************\n",
      "Window len:  6\n",
      "Weighted:  True\n",
      "Use free data:  True\n",
      "Num epochs:  1\n"
     ]
    }
   ],
   "source": [
    "root_path ='C:/ASM/DevData/eating/eating_detection_new/'\n",
    "window_len = 6\n",
    "weighted = False\n",
    "free = True\n",
    "epochs=1\n",
    "print(\"************* Bite Detection ***************\")\n",
    "print('Window len: ', window_len)\n",
    "print('Weighted: ', weighted)\n",
    "print('Use free data: ', free)\n",
    "print('Num epochs: ', epochs)\n",
    "#0:stev, 1:stev_uva, 2:stev_uva_free "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(data, annots, window_size):    \n",
    "    step_size = window_size//3\n",
    "    annot_count = len(annots)\n",
    "    sample_count = len(data)\n",
    "    indices = np.arange(0, sample_count-window_size, step_size)\n",
    "    window_count = len(indices)\n",
    "    windows = np.zeros((window_count, window_size, data.shape[1]))\n",
    "    labels = np.zeros((window_count, ))\n",
    "    features = np.zeros((window_count, 2))\n",
    "    cut_points = np.zeros((window_count, 5))\n",
    "    \n",
    "    lab = True if annots.shape[1]==2 else False \n",
    "    \n",
    "    annot_index = 0    \n",
    "    for i in range(window_count):\n",
    "        si = indices[i]\n",
    "        windows[i, :, :] = (data[si:si+window_size, :]+9.8)/(2*9.8)\n",
    "        \n",
    "        mid_start = si + step_size\n",
    "        mid_end = mid_start + step_size-1\n",
    "        \n",
    "        features[i, 0] = np.amin(data[mid_start:mid_end+1, 0])\n",
    "        features[i, 1] = np.sum(np.var(data[si:si+window_size, :], axis=0))\n",
    "        \n",
    "        cut_points[i, :] = [si, mid_start, mid_end, si+window_size-1, 0]        \n",
    "                \n",
    "        if lab and annot_index < annot_count and annots[annot_index, 0] < si+window_size:            \n",
    "            if annots[annot_index, 0] > mid_end:\n",
    "                labels[i] = -1\n",
    "                \n",
    "            elif mid_start <= annots[annot_index, 0] <= mid_end:\n",
    "                labels[i] = annots[annot_index, 1]\n",
    "                cut_points[i, 4] = annots[annot_index, 0]\n",
    "                \n",
    "            elif si <= annots[annot_index, 0] < mid_start:\n",
    "                labels[i] = -1                \n",
    "                annot_index = annot_index + 1\n",
    "                while annot_index < annot_count and annots[annot_index, 0] < mid_start: \n",
    "                    annot_index = annot_index + 1\n",
    "                    \n",
    "                if annot_index < annot_count and mid_start <= annots[annot_index, 0] <= mid_end:\n",
    "                    labels[i] = annots[annot_index, 1]\n",
    "                    cut_points[i, 4] = annots[annot_index, 0]\n",
    "                \n",
    "            else:\n",
    "                print(\"Annot time error\")\n",
    "                sys.exit(0)\n",
    "                \n",
    "        elif (not lab) and annot_index < annot_count and annots[annot_index, 0] < si+window_size:\n",
    "            if si > annots[annot_index, 1]:\n",
    "                annot_index += 1\n",
    "            else:\n",
    "                if annots[annot_index, 2] <3:\n",
    "                    labels[i] = 1\n",
    "                else:\n",
    "                    labels[i] = 2\n",
    "                \n",
    "    return windows, labels, features, cut_points            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows_dataset(ds, window_size):\n",
    "    windows = []\n",
    "    labels = []\n",
    "    features = []\n",
    "    cut_points = []\n",
    "    subject_session = []\n",
    "    for subject in range(len(ds)):\n",
    "        for sess in range(len(ds[subject])):\n",
    "            accel = ds[subject][sess][0]\n",
    "            annots = ds[subject][sess][1]\n",
    "            accel = accel[:, 1:]\n",
    "            accel = accel + 9.8/(2*9.8)            \n",
    "            \n",
    "            w, l, f, c = get_windows(accel, annots, window_size)\n",
    "            ss = np.zeros((len(l), 2))\n",
    "            ss[:, 0] = subject\n",
    "            ss[:, 1] = sess\n",
    "            \n",
    "            if len(windows)==0:\n",
    "                windows = w\n",
    "                labels = l\n",
    "                features = f\n",
    "                cut_points = c\n",
    "                subject_session = ss\n",
    "                \n",
    "            else:\n",
    "                windows = np.concatenate((windows, w), axis=0)\n",
    "                labels = np.concatenate((labels, l), axis=0)\n",
    "                features = np.concatenate((features, f), axis=0)\n",
    "                cut_points = np.concatenate((cut_points, c), axis=0)\n",
    "                subject_session = np.concatenate((subject_session, ss), axis=0)\n",
    "                \n",
    "    return windows, labels, features, cut_points, subject_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_annots(annots, min_distance):    \n",
    "    count = len(annots)\n",
    "    flags = np.ones((count, ))\n",
    "    \n",
    "    for i in range(1, count):\n",
    "        if annots[i, 0] - annots[i-1, 0]<=min_distance:\n",
    "            flags[i-1] = 0\n",
    "            \n",
    "    annots = annots[flags==1]\n",
    "    #print('Annnot prcess: before, after :: ', count, len(annots))\n",
    "    return annots\n",
    "            \n",
    "    \n",
    "def process_annots_dataset(ds, min_distance):\n",
    "    for subject in range(len(ds)):\n",
    "        for sess in range(len(ds[subject])):            \n",
    "            annots = ds[subject][sess][1]\n",
    "            ds[subject][sess][1] = process_annots(annots, min_distance)            \n",
    "    return ds\n",
    "    \n",
    "def process_uva_lab_data(data, min_distance):\n",
    "    #usc:0-13, uva: 2, 5, 2, 2, 4, 5, 1\n",
    "    data[14] = [data[14][0], data[15][0]]\n",
    "    data[15] = [data[16][0], data[17][0], data[18][0], data[19][0], data[20][0]]\n",
    "    data[16] = [data[21][0], data[22][0]]\n",
    "    data[17] = [data[23][0], data[24][0]]\n",
    "    data[18] = [data[25][0], data[26][0], data[27][0], data[28][0]]\n",
    "    data[19] = [data[29][0], data[30][0], data[31][0], data[32][0], data[33][0]]\n",
    "    data[20] = data[34]\n",
    "    data = data[:21]\n",
    "    data = process_annots_dataset(data, min_distance)\n",
    "    return data\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject counts total:  28\n"
     ]
    }
   ],
   "source": [
    "with open(root_path + \"data/steven_lab_data.pkl\", 'rb') as file:\n",
    "    stev_lab_data = pickle.load(file)\n",
    "with open(root_path + \"data/uva_lab_data.pkl\", 'rb') as file:\n",
    "    uva_lab_data = pickle.load(file)\n",
    "    uva_lab_data = process_uva_lab_data(uva_lab_data, 16)\n",
    "\n",
    "stev_lab_data.extend(uva_lab_data)\n",
    "lab_data = stev_lab_data\n",
    "print('Subject counts total: ', len(lab_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows, labels, features, cut_points, subject_session = get_windows_dataset(lab_data, window_len*16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154758, 96, 3) (154758,) (154758, 2)\n",
      "(36491, 96, 3) (36491,) (36491, 2)\n",
      "Class weights:  [ 1.         11.34257014 38.16550117]\n"
     ]
    }
   ],
   "source": [
    "print(windows.shape, labels.shape, subject_session.shape)\n",
    "cond = (features[:, 0]<=-3) & (features[:, 1]>=0.1) & (labels>=0)\n",
    "windows = windows[cond, :, :]\n",
    "labels = labels[cond]\n",
    "subject_session = subject_session[cond, :]\n",
    "cut_points = cut_points[cond, :]\n",
    "features = features[cond, :]\n",
    "print(windows.shape, labels.shape, subject_session.shape)\n",
    "\n",
    "w0 = np.sum(labels==0)\n",
    "w1 = np.sum(labels==1)\n",
    "w2 = np.sum(labels==2)\n",
    "class_weights = np.array([1, w0/w1, w0/w2])\n",
    "print(\"Class weights: \", class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32746.  2887.   858.]\n",
      "(36491, 3)\n"
     ]
    }
   ],
   "source": [
    "windows = windows.reshape((windows.shape[0], windows.shape[1], windows.shape[2], 1))\n",
    "labels = to_categorical(labels, 3)\n",
    "print(np.sum(labels, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, X_val, Y_val, batch_size, epochs, tensorboard_logdir, class_weights=[]):\n",
    "    print(\"Starting training... Sizes: \", X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)\n",
    "    print('Class weights: ', class_weights)\n",
    "    \n",
    "    if not os.path.exists(tensorboard_logdir):\n",
    "        os.makedirs(tensorboard_logdir)\n",
    "    \n",
    "    input_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(4, 1), padding ='same', strides=(1, 1), activation='relu', input_shape=input_shape ))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 1), strides=(2, 1)))    \n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(2, 2), padding ='same', strides=(1, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 1), strides=(2, 1)))    \n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=(2, 2), padding ='same', strides=(1, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(4, 1), strides=(2, 1)))    \n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    if len(class_weights) == 0:\n",
    "        model.compile(loss = categorical_crossentropy, optimizer= Adam())\n",
    "    else:\n",
    "        model.compile(loss = weighted_categorical_crossentropy(class_weights), optimizer= Adam())\n",
    "\n",
    "    model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              shuffle=True,\n",
    "              validation_data=(X_val, Y_val),\n",
    "              callbacks=[TensorBoard(log_dir=tensorboard_logdir)])             \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "**************************************\n",
      "Excluding subject: -1\n",
      "**************************************\n",
      "\n",
      "Starting training... Sizes:  (32841, 96, 3, 1) (32841, 3) (3650, 96, 3, 1) (3650, 3)\n",
      "weights:  []\n",
      "Train on 32841 samples, validate on 3650 samples\n",
      "Epoch 1/1\n",
      "32841/32841 [==============================] - 21s 650us/step - loss: 0.4017 - val_loss: 0.2808\n",
      "Creating directory:  C:/ASM/DevData/eating/eating_detection_new/results/window_6_free_1_weighted_1//models/\n",
      "\n",
      "\n",
      "**************************************\n",
      "Excluding subject: 0\n",
      "**************************************\n",
      "\n",
      "Starting training... Sizes:  (29053, 96, 3, 1) (29053, 3) (3229, 96, 3, 1) (3229, 3)\n",
      "weights:  []\n",
      "Train on 29053 samples, validate on 3229 samples\n",
      "Epoch 1/1\n",
      "20224/29053 [===================>..........] - ETA: 5s - loss: 0.4906"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-64c007162b99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m                               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                               \u001b[0mtensorboard_logdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensorboard_logdir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                               class_weights=weights)    \n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/models/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-c1be07adf4c6>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(X_train, Y_train, X_val, Y_val, batch_size, epochs, tensorboard_logdir, class_weights)\u001b[0m\n\u001b[0;32m     37\u001b[0m               \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m               callbacks=[TensorBoard(log_dir=tensorboard_logdir)])             \n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for exclude_subject in range(-1, len(lab_data)):\n",
    "    print('\\n\\n**************************************')\n",
    "    print('Excluding subject:', exclude_subject)\n",
    "    print('**************************************\\n')\n",
    "    \n",
    "    X = np.empty((0, windows.shape[1], windows.shape[2], windows.shape[3]))\n",
    "    Y = np.empty((0,3))    \n",
    "    cond = subject_session[:,0]!=exclude_subject\n",
    "    w = windows[cond, :, :, :]\n",
    "    l = labels[cond, :]\n",
    "    X = np.concatenate((X, w)) \n",
    "    Y = np.concatenate((Y, l))\n",
    "    \n",
    "    if not weighted:\n",
    "        class_weights = []\n",
    "    path = root_path+'results/window_'+str(window_len)+'_free_'+str(int(free))+'_weighted_'+str(int(weighted))+'/'\n",
    "    tensorboard_logdir = path +'tensorboard/subject_'+str(exclude_subject)+'/'\n",
    "    \n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, stratify=Y, test_size=0.1)\n",
    "    model = train_model(X_train, Y_train, \n",
    "                              X_val, Y_val,\n",
    "                              batch_size = 128,\n",
    "                              epochs=epochs,\n",
    "                              tensorboard_logdir=tensorboard_logdir,\n",
    "                              class_weights=class_weights)    \n",
    "    \n",
    "    model_path = path+'models/'\n",
    "    if not os.path.exists(model_path):\n",
    "        print('Creating directory: ', model_path)\n",
    "        os.makedirs(model_path)\n",
    "    \n",
    "    model.save(model_path+'subject_'+str(exclude_subject)+'.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
